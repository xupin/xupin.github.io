[{"title":"Github拉项目速度慢的解决小技巧","date":"2020-07-04T16:00:00.000Z","path":"2020/07/05/git-cloning-slow/","text":"碰上github网络抽风的时候，恰巧你又需要拉取github上托管的项目，这个时候看着2kb/s的下载速度是不是很抓狂？ 粗暴的解决方式打开ipaddress查询以下三个域名的ip，写到hosts文件中。 github.com github.global.ssl.fastly.net codeload.github.com 进阶的方式 挂代理，哦豁export ALL_PROXY=socks5://127.0.0.1:4000 究极方式 复制你要拉取的项目github地址，比如：https://github.com/v2ray/v2ray-core 打开码云，创建仓库-&gt;从 GitHub / GitLab 导入仓库 搞定！git clone https://gitee.com/xupinbest/v2ray-core","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"github","slug":"github","permalink":"http://xupin.im/tags/github/"}]},{"title":"Go语言学习笔记 - goroutine","date":"2020-06-11T16:00:00.000Z","path":"2020/06/12/goroutine/","text":"在现在大数据、高并发，到处都充斥着流量的互联网时代，能不能应对高并发俨然已经发展成一个衡量服务端架构是否合格的标准，作为程序媛我们思考如何利用语言在代码层面最优设计能应对并发的程序去并行处理任务。在程序中对于任务并行处理一般趋于使用：进程、线程，以及另外一种：协程。支持协程的语言有很多，比如：C/C++、Ruby、 Python（2.5+）、Golang等等，它们有些是本身语言支持协程，有些则是需要引入第三方包来使用。不过，我们主要来学习一下Golang这门语言（简称Go），它是如何理解以及实现协程的。 一、进程、线程和协程的前世今生都知道一台计算机的核心是CPU，它承担着所有的运算。而计算机承载的操作系统（内核）则是负责所有任务的处理和调度CPU以及资源的分配。如果用人类来比喻，大脑是CPU，思维则是操作系统（内核）。 进程最早的计算机每次只能运行一个程序，如果还有其他程序需要执行则要排队等待。后来CPU运算能力提高了，这种方式过于原始有些浪费性能，于是尝试让多个程序可以并行执行，但是这样面临一个新的问题：跑在同一个CPU中的程序都会使用计算机资源，那程序的运行状态和数据怎么保障？进程。1进程是内核资源管理分配的最小单位，每个进程都有独立的虚拟地址空间。内核中的每个程序都运行在独立进程的上下文中，上下文是由程序正常运行需要的一系列参数组成，参数包括存储器中的代码和数据，寄存器中的内容以及进程打开的文件描述符（文件句柄）等。可以把上下文通俗理解为：`环境`。 如果程序在运行过程中需要进行IO操作，IO操作阻塞了程序后面的计算，这时候CPU属于空闲状态，那内核会把CPU切换到其他进程去处理。不过当进程数量变高以后，计算机的大部分资源都被进程切换这个操作消耗掉了。为什么说进程切换操作消耗资源代价比较高？1所谓进程切换其实就是上下文切换，需要切换新的页表并加载新的虚拟地址空间、切换内核栈以及硬件上下文等。只要发生进程切换操作就得反复进入内核，加载切换一系列状态。 线程为了减少这种开销，线程应运而生。1线程是内核调度CPU执行的最小单位，线程是运行在进程上下文的逻辑流，线程是具体执行程序的单位。一个进程至少包含一个主线程（可以拥有多个子线程），但是一个线程只能存在于一个进程中。 线程切换相比进程切换开销就小了很多，线程切换只需要把寄存器刷新即可。 协程后面程序媛们发现线程这样还是有性能瓶颈（IO阻塞），无论是进程还是线程因为涉及到大量的计算机资源，所以都是由内核调度管理。能不能开发一种由代码控制的线程呢？这就是协程。 1协程是由用户控制的线程（用户态线程），协程在程序中实现自我调度，不需要像进程切换一样进入内核加载切换状态，提高了线程在IO上的性能问题（IO多路复用）。 但是协程也有个致命的问题，假如进程中的某一程序出现了阻塞操作同时被CPU中断处理（抢占式调度），那么该进程中的所有线程都会被阻塞。 后面会专门写一篇关于进程和线程以及协程的特性以及区别。（不够详细，埋坑Orz~） 二、什么是goroutine？上面简单学习了进程和线程以及协程的渊源，虽然不够详细但是我们大概知道其实在进程或者线程甚至于协程存在的性能瓶颈大部分是CPU调度问题。 1go func() // Go语言启动协程，只需要使用go关键字即可启动协程运行函数。使用go关键字创建的这个协程就叫做`goroutine` 之前说了goroutine是Go语言的协程，其实这么理解是可以的但goroutine比协程更强大。它们使用的线程模型有着本质的区别，如下： goroutine通过通道来通信，而协程通过让出执行和恢复操作来通信。 goroutine通过Go语言的调度器进行调度，而协程通过程序本身调度。 大部分语言或者第三方库提供的协程就是使用的用户态线程模型，但goroutine使用的不是传统的用户态线程模型。以下主流的线程模型： 内核级线程模型 内核级别的线程的状态切换需要内核直接处理，所以内核清楚的知道每一个KSE（Kernel Scheduling Entity）的存在（即内核线程和KSE是一对一关系），它们可以全系统内进行资源的竞争。 用户级线程模型（用户态线程，协程） 用户态级别的线程受用户控制，内核并不直接知道用户态线程的存在（为什么这么说？因为用户态线程和内核线程存在着多对一的关系，即多个用户态线程对应一个内核线程），一般用户态多线程属于同一个进程，所以它们只能在进程内进行资源竞争。 两级线程模型（混合型线程模型） 两级线程模型吸取了内核级和用户级线程的经验，两级线程模型下的线程和内核线程处于多对多的关系。一个进程内的多个线程可以分别绑定内核线程，既可以多个线程绑定多个内核线程也可以多个线程绑定一个内核线程，当某个线程内的程序产生阻塞其绑定的内核线程被CPU中断处理，进程内的其他线程可以重新与其他内核线程绑定。 goroutine使用的正是两级线程模型，但是这种多个线程跑在多个内核中，既不是用户级线程模型完全靠自身调度也不是内核级线程模型完全依赖内核调度，而是用户和内核协同调度。因为这种模型复杂性较高，所以Go语言开发了自己的runtime调度器。 三、Go runtime调度器Go runtime调度器的结构由三部分组成: G Goroutine，每个goroutine有对应的G结构体，G结构体储存goroutine的上下文信息。G并不能直接被调度，需要绑定对应的P才能被调度执行。 P Processor，为G和M进行调度的逻辑处理器，对于G来说P像是内核，而在M看来P相当于上下文。P的数量可以在程序中代码控制，如下: 1runtime.GOMAXPROCS(runtime.NumCPU()) // 该值最大为256。 M Machine，负责调度任务（可以理解为内核线程的抽象），代表着操作系统内核，是真正处理任务的服务。M的数量不是固定的，受Go runtime调度器控制。（不过该值最大为10000，可以参考：src/runtime/proc.go） 值得一提的是Go语言在最初的版本中Go runtime调度器的结构是GM模型（并非GMP模型），P服务是因为GM模型在并发上出现很大的性能损耗。有兴趣的小伙伴可以看一下Go runtime的核心开发者Dmitry Vyukov发现的问题Scalable Go Scheduler Design Doc。 简单说了一下Go runtime调度器中的GPM模型的概念，那么GPM究竟是怎么调度的呢？首先当通过go func()创建一个G对象的时候，G会被优先放入P的本地队列。为了执行G M需要绑定一个P，然后M启动内核线程并循环从P的本地队列取出G并执行。 当P发现当前绑定的M被阻塞时会转入绑定其他M（新的M可能是被创建或者从内核线程缓存中取出）。 如果M处理完了当前P的本地队列里的G后，P会尝试从全局队列里取G来执行（同样P也会周期性的检查全局队列是否有G可以执行）。如果全局队列没有可以执行的G，P会随机挑选另外一个P并从它的本地队列中取出一半G到自己的本地队列中执行。这个动作使用调度算法work-stealing（工作窃取算法）实现。 最后以上就是Go runtime调度器的运行原理（大概），后面有更深入的理解会补充进来。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"goroutine","slug":"goroutine","permalink":"http://xupin.im/tags/goroutine/"}]},{"title":"Mysql小技巧 - 查找连续编号中的缺失编号","date":"2020-04-12T16:00:00.000Z","path":"2020/04/13/mysql-trick/","text":"在和小伙伴讨论问题的时候，小伙伴突然问了我这样一个小问题，数据库中如何查找连续编号中的缺失编号？ 1.描述场景大概是这样，有一份连续数据ID：1 … 27，其中ID：6，7，14的数据丢了。结构如下：1234567891011121314151617181920212223242526272829303132CREATE TABLE `letter` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 27 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Compact;INSERT INTO `letter` VALUES (1, 'A');INSERT INTO `letter` VALUES (2, 'B');INSERT INTO `letter` VALUES (3, 'C');INSERT INTO `letter` VALUES (4, 'D');INSERT INTO `letter` VALUES (5, 'E');# INSERT INTO `letter` VALUES (6, 'F');# INSERT INTO `letter` VALUES (7, 'J');INSERT INTO `letter` VALUES (8, 'H');INSERT INTO `letter` VALUES (9, 'I');INSERT INTO `letter` VALUES (10, 'J');INSERT INTO `letter` VALUES (11, 'K');INSERT INTO `letter` VALUES (12, 'L');INSERT INTO `letter` VALUES (13, 'M');# INSERT INTO `letter` VALUES (14, 'N');INSERT INTO `letter` VALUES (15, 'O');INSERT INTO `letter` VALUES (16, 'P');INSERT INTO `letter` VALUES (17, 'Q');INSERT INTO `letter` VALUES (18, 'R');INSERT INTO `letter` VALUES (19, 'S');INSERT INTO `letter` VALUES (20, 'T');INSERT INTO `letter` VALUES (21, 'U');INSERT INTO `letter` VALUES (22, 'V');INSERT INTO `letter` VALUES (23, 'W');INSERT INTO `letter` VALUES (24, 'X');INSERT INTO `letter` VALUES (25, 'Y');INSERT INTO `letter` VALUES (26, 'Z'); 怎么把6，7，14这三条数据找出来？方法有很多种哈，今天我们说一下如何利用SQL快速查询出来。大概思路是把ID+1，然后查询ID+1这个值是否存在ID列表中，如果不存在那肯定就是缺失的。SQL如下：123456SELECT id + 1 AS id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) 但是这样会有一个小问题，就是MAX(id)+1（27）也会被查询出来，所以：123456789101112SELECT id + 1 AS id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) AND id &lt;( SELECT max( id ) FROM `letter` ) 2.优化1234567891011121314151617181920SELECT start_id, ( SELECT MIN(id)- 1 FROM `letter` WHERE id &gt; start_id ) AS end_id FROM ( SELECT id + 1 AS start_id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) AND id &lt;( SELECT max(id) FROM `letter` ) ) AS max_id ORDER BY start_id 这种情况适用于查找整数类型的连续编号，那么如果编号是string类型的呢？后面有机会再补充进来=,=。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Redis学习笔记 - 集群","date":"2020-03-26T16:00:00.000Z","path":"2020/03/27/redis-cluster/","text":"之前粗浅的学习了Redis三种集群策略的主从复制和哨兵策略，现在最后这篇来学习一下Redis Cluster也就是最后一个集群策略。 1.什么是集群？集群（Cluster），Redis2.6版本（正式版本是3.0）推出的分布式解决方案，有效解决了单Master节点写操作的压力并且分布式存储数据，大大提高了负载能力。 在Redis发布3.0正式版本前，一般使用代理中间件来实现分布式集群策略。这里不展开学习了，有兴趣的小伙伴自行研究。 特点 Cluster策略是分布式部署，节点间相互协调工作。 因为对主从复制和哨兵策略都称为集群策略，所以为了防止误解在下文中提及的集群（Cluster）策略，直接用Cluster称呼。 Cluster至少要3个Master节点，并且是无中心化设计。 客户端使用Cluster，不需要连接所有节点，只需要连接Cluster中任意一个可用节点即可。 数据的分布式存储不需要指定，Cluster会自动完成。 优点 Cluster策略拥有主从复制和哨兵策略的优点。 解决了单Master节点写操作的压力。 分布式存储数据，提高了负载能力。 支持线性扩容。 缺点 部分操作命令受限，比如mset，目前只能支持同一个插槽（slot）的key进行操作。 事务机制不支持多节点操作。 不支持多数据库，即只有db0。 2.如何配置（新旧方式） 旧方式，./redis-trib.rb create –replicas {SLAVE_NUM} {IP}:{PORT} … {IP}:{PORT} 新方式，./redis-cli –cluster create –cluster-replicas {SLAVE_NUM} {IP}:{PORT} … {IP}:{PORT} redis.conf示例12345678910111213# 是否以守护进程方式运行daemonize yes/no# 是否启用Clustercluster-enabled yes/no# 节点信息配置文件，自动生成。# FILE_NAME：配置文件名cluster-cluster-config-file &#123;FILE_NAME&#125;# 节点连接超时时间# MS：超时时间（单位：Millisecond）cluster-node-timeout &#123;MS&#125; 3.工作机制 Redis节点启动，节点根据配置cluster-enabled判断是否加入Cluster。 新节点通过cluster meet {IP} {PORT}命令和其他节点感知并建立连接，节点间会通过Gossip协议PING/PONG命令来检测状态和交换信息。 Cluster计算并且分配主节点插槽数量。这个地方注意，不是插槽数量，是每个节点的插槽数量。插槽数量是固定的：16384。 插槽分配成功之后，Cluster开始服务。 4.如何感知新节点？当给某一节点发送命令cluster meet {IP} {PORT}（新节点），该节点就会尝试与新节点建立连接，具体流程： 该节点向新节点发送MEET命令。 新节点接收到MEET命令后，回复PONG命令。 该节点接收到新节点返回的PONG命令，知道新节点成功接收了自己的MEET命令。 该节点向新节点发送PING命令。 新节点接收到该节点发送的PING命令，知道该节点已经成功接收到自己返回的PONG命令。 该节点和新节点握手完成，建立连接。 最后，该节点会将新节点的信息通过Gossip协议同步给Cluster中的其他节点，让其他节点也与新节点进行握手，建立连接。 可以通过cluster nodes命令查看集群中哪些节点已经建立连接。 5.数据插槽说到插槽（slot）不得不提一下，为了能够让数据平均分配到多个节点上而采用的数据分区算法。常见的数据分区算法：范围（Range）、哈希（Hash）、一致性哈希算法和虚拟哈希槽等。 Cluster采用的虚拟哈希槽数据分区算法，所有的key根据哈希函数映射到0 ~ 16383插槽内（公式：slot = crc16(key) &amp; 16383），之前也提到过插槽也是平均分配到每个Master节点的。 虚拟哈希槽的特点 降低了节点和数据之间的耦合性，方便线性扩容&amp;动态管理节点。 节点自己管理和插槽的对应关系。 支持查询节点、插槽和key的对应关系。 可以通俗理解为，插槽是Cluster管理数据的基本单位。 6.动态管理节点假如我们原有4个Master节点（M1 … M4），但是现在因为数据增量问题临时加一个Master节点（M5），我们需要怎么操作呢？ 启动M5节点，客户端发送MEET命令让M5节点加入到Cluster中，现在M5节点没有任何插槽所以不会接受任何读写操作。 在M5节点执行cluster setslot {SLOT} importing {SOURCE_NODE_ID}命令，让M5节点准备导入{SLOT}插槽。 在拥有这个{SLOT}插槽的源节点上面执行cluster setslot {SLOT} migrating {M5_NODE_ID}，让源节点准备好迁出插槽。 这时候如果客户端操作的key存在于{SLOT}插槽中，那么这个操作由源节点处理。如果key不存在于{SLOT}插槽中，这个操作将由M5节点操作。 现在源节点的{SLOT}插槽不会创建任何新的key，需要把源节点{SLOT}插槽中的key迁移到M5节点。执行cluster getkeysinslot {SLOT} {COUNT}命令获取{SLOT}插槽中指定{COUNT}数量的key列表。 在源节点对每个key执行migrate命令，把key迁移到M5节点。 在源节点和M5节点执行cluster setslot {SLOT} NODE {M5_NODE_ID}，完成迁移。 这就是动态增加节点的流程了，可能在新的Redis版本中增加了节点迁移工具，但是核心流程应该还是这样。 7.Hash Tag学习了数据插槽，我们知道Redis在key分配到插槽的这一操作完全是自动化的，不过当我们有需求对不同的key需要放到同一插槽中的时候，这个时候要怎么操作呢？我们只需要在key中加入{}符号即可，比如： {UID_1001}:following {UID_1001}:followers 这两个key会被分配到同一插槽，原理就是当key中存在{}符号，哈希算法只会针对{}符号内的字符串。 8.插槽为什么是16384？这个值能修改吗？首先crc16算法算出的值有16bit，2^16即65536。也就是说该算法的值在0 ~ 65535之间，那么为什么作者还是选择了16384即0 ~ 16383。 很开心，对于这个疑问Redis作者给了明确的回答。前面我们知道每个节点之间会以每秒1次的频率互相发送心跳包（PING）&amp;交换信息（信息分为消息头和消息体），之前也提了交换的信息体里面主要包含节点的信息等，那么消息头的内容呢？如下：123456789101112131415161718192021222324typedef struct &#123; char sig[4]; /* Siganture \"RCmb\" (Redis Cluster message bus). */ uint32_t totlen; /* Total length of this message */ uint16_t ver; /* Protocol version, currently set to 1. */ uint16_t port; /* TCP base port number. */ uint16_t type; /* Message type */ uint16_t count; /* Only used for some kind of messages. */ uint64_t currentEpoch; /* The epoch accordingly to the sending node. */ uint64_t configEpoch; /* The config epoch if it's a master, or the last epoch advertised by its master if it is a slave. */ uint64_t offset; /* Master replication offset if node is a master or processed replication offset if node is a slave. */ char sender[CLUSTER_NAMELEN]; /* Name of the sender node */ unsigned char myslots[CLUSTER_SLOTS/8]; char slaveof[CLUSTER_NAMELEN]; char myip[NET_IP_STR_LEN]; /* Sender IP, if not all zeroed. */ char notused1[34]; /* 34 bytes reserved for future usage. */ uint16_t cport; /* Sender TCP cluster bus port */ uint16_t flags; /* Sender node flags */ unsigned char state; /* Cluster state from the POV of the sender */ unsigned char mflags[3]; /* Message flags: CLUSTERMSG_FLAG[012]_... */ union clusterMsgData data; /* message body*/&#125; clusterMsg; 其中有个myslots字段要注意，该字段使用位图，即1bit代表1slot，如果该bit为1即说明该插槽属于这个节点。那么该字段的大小为：16384 / 8bit / 1024b = 2kb。也就是消息头不考虑其他信息的情况，单是myslots就已经有2kb大小。 那么消息体呢？之前已经提到了消息体中会包含节点信息。具体是什么样的呢？消息体每次携带最少3个节点的信息，数量约为总节点数的1/10。如果节点数量越多，消息体越大。 如果插槽数量是65535，那么该字段的大小放大为：65535 / 8bit / 1024b = 8kb。这对于每秒1次频率的心跳包来讲，带宽开销是极大的。 上面说了节点越多，消息体也就越大，如果节点超过1000个也会导致网络拥堵，因为Redis作者不建议Cluster节点的数量超过1000，那么对于1000个以下的节点来说16384个插槽也就够用了。 第三个考虑是关于位图的压缩问题，我还没有搞明白~~所以这里不展开说了，先埋个坑。 这个值能修改吗？16384插槽数量是写死在Redis源代码中的，所以是不可以更改的。 附上关于作者的回答 https://github.com/antirez/redis/issues/2576 最后Cluster在故障恢复主从切换的机制（包括：主观宕机、客观宕机、投票选举、主从切换）和哨兵策略基本一致，所以在这里就不学习Cluster关于故障恢复主从切换的相关知识了。 前面提到过Gossip协议，它主要职责就是各节点间的信息交换，常用的Gossip消息可分为： ping pong meet fail 后面会专门来学习Gossip协议的知识（-,-再次埋个坑）。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Redis学习笔记 - 哨兵","date":"2020-03-25T16:00:00.000Z","path":"2020/03/26/redis-sentinel/","text":"现在这篇来学习一下Redis Sentinel即哨兵策略的相关知识。 1.什么是哨兵？哨兵（Sentinel），Redis2.6版本（正式版本是2.8，现2.6版本已被废弃）开始提供的一种集群策略，核心思想是解决了主从复制（Replication）在Master节点故障，无法自动切换Slave节点为新Master节点的问题。 特点 哨兵策略是分布式部署，节点间相互协调工作。 哨兵集群至少要3个节点。 可以把哨兵看作是一种特殊的Redis服务。 优点 哨兵策略拥有主从复制的优点。 哨兵策略解决了Master节点故障，无法自动切换Slave节点为新Master节点的问题。 缺点 Master节点写操作的压力没有得到解决。 数据存储能力还是受到单节点限制。 2.如何配置（多种方式） 启动Redis哨兵，./redis-sentinel redis-sentinel.conf 启动Redis服务时并且启动哨兵，./redis-server redis-sentinel.conf –sentinel redis-sentinel.conf示例1234567891011121314151617181920212223242526# 哨兵监控的节点。# MASTER_NAME：自定义的Master节点名称。# IP：Master节点的地址。# PORT：Master节点的端口。# QUORUM：当Master节点故障，确认Master节点odown最少的哨兵数量。sentinel monitor &#123;MASTER_NAME&#125; &#123;IP&#125; &#123;PORT&#125; &#123;QUORUM&#125;# 哨兵验证# PASS：Master节点的auth，需要注意的时哨兵不能同时为Master节点和Slave节点设置密码，所以auth需要保持一致。sentinel auth-pass &#123;MASTER_NAME&#125; &#123;PASS&#125;# 哨兵心跳Ping最大时间，如果哨兵向Master节点发送Ping超过这个时间或者回复err，那么哨兵会主观（sdown）认为该Master节点已经处于不可用状态。# MS：心跳Ping等待响应的最大时间（单位：Millisecond）。sentinel down-after-milliseconds &#123;MASTER_NAME&#125; &#123;MS&#125;# 同时同步数据的Slave节点数量# SLAVE_NUM：当Master节点故障，Slave节点通过竞选当选新Master节点，最多允许几个Slave节点开始同步新Master的数据。sentinel parallel-syncs &#123;MASTER_NAME&#125; &#123;SLAVE_NUM&#125;# 主从节点切换所需要的最大时间。# MS：主从节点切换所需要的最大时间（单位：Millisecond）。sentinel failover-timeout &#123;MASTER_NAME&#125; &#123;MS&#125;# Master节点故障调用的脚本# SCRIPT_PATH：脚本路径sentinel notification-script &#123;MASTER_NAME&#125; &#123;SCRIPT_PATH&#125; 3.工作机制 哨兵向已知节点和哨兵发送心跳包检测状态。 Master节点无效回复，哨兵判断Master节点状态：主观宕机，客观宕机。 Master节点被确定客观宕机，进行领头（Leader）哨兵选举。 准备进行主从切换的领头哨兵获取其他哨兵的授权。 授权成功，从Slave节点中选举新Master节点。 领头哨兵把新Master节点信息同步给其他哨兵，其他哨兵把新Master节点信息同步对应Slave节点。 4.心跳包每个哨兵以每秒钟1次的频率向它已知的Master节点、Slave节点和其他哨兵发送PING命令，希望得到的有效回复如下：123PING replied with +PONG.PING replied with -LOADING error.PING replied with -MASTERDOWN error. 其它任何回复或者无回复都是无效回复。 5.哨兵和节点之间的自动发现机制通过Redis的pub/sub系统实现，每个哨兵都会向自己监控的节点对应的channel：sentinel:hello发送一条消息（消息体包含自己的{IP}、{PORT}和{RUNID}以及Master节点的完整配置），每个订阅该channel的哨兵都可以消费这条消息并且能发现到其他哨兵的存在，如果某个哨兵发现自己维护的节点配置低于新接收的节点配置，则会用新的节点配置进行覆盖。 6.主观/客观宕机如果一个Master节点在收到PING命令后没有在有效时间内（down-after-milliseconds）进行有效回复，则会被标记为主观宕机（sdown，Subjectively Down）。 哨兵会获取其他哨兵检测该节点的状态，命令：12345# IP：主观宕机的Master节点地址# PORT：主观宕机的Master节点端口# CURRENT_EPOCH：哨兵的配置纪元，用于领头哨兵选举。# RUNID：可以是*和哨兵的RunID，当值是 * 代表检测节点是否主观宕机，如果是RunID则用于领头哨兵选举。SENTINEL is-master-down-byaddr &#123;IP&#125; &#123;PORT&#125; &#123;CURRENT_EPOCH&#125; &#123;RUNID&#125; 当有足够数量（{QUORUM}）的哨兵都认为该Master节点处于主观宕机状态。则该Master节点会被标记为客观宕机（odown，Objectively Down），若没有足够数量的哨兵都认为该Master节点处于主观宕机状态，则不会被标记为客观宕机，同时如果该Master节点重新返回哨兵有效回复，该Master节点主观宕机状态会被移除。 7.领头哨兵选举因为只需要一个哨兵完成主从切换，所以需要选举一个领头哨兵。 每个哨兵都会发送SENTINEL is-master-down-byaddr命令希望成为领头哨兵。收到该命令的哨兵如果没有同意过其他哨兵的同样命令，那么同意该请求，否则拒绝。 如果某一哨兵发现同意自己请求的哨兵数量并且数量大于等于{QUORUM}，那么它将成为领头哨兵。 如果选举过程中有多个哨兵当选领头哨兵，等待一段时间后选举会重新进行。 该方法基于raft算法领头选举方法实现。 8.主从切换授权当选举出领头哨兵之后并未马上进行主从切换，领头哨兵还需要获取{MAJORITY}数量的哨兵授权。1# MAJORITY：该值不可配置，Redis自行计算。公式：majority = voters / 2 + 1 如果{QUORUM} &lt; {MAJORITY}，领头哨兵需要{MAJORITY}数量的哨兵进行授权。 如果{QUORUM} &gt;= {MAJORITY}，那么领头哨兵需要{QUORUM}数量的哨兵授权才可以。 当领头哨兵获得授权之后，正式开始主从切换流程。 9.主从切换开始主从切换（failover），首先领头哨兵会选举出一个Slave节点出来作为新Master节点，该Slave节点选举参考参数： 该节点和Master节点断开的时长，如果一个Slave节点与Master节点断开连接时间已经超过down-after-milliseconds参数的10倍，再加上Master宕机的时长，该Slave节点就会被认为不适合选举为新的Master节点。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state Slave节点的优先级（slave-priority），slave-priority越低优先级越高。 复制数据偏移量（复制数据最完整） RunID（最小的） 10.配置同步在领头哨兵完成主从切换之后，会在本地生成最新的Master配置然后通过pub/sub消息机制同步给其他哨兵，其他哨兵则更新对应的Master节点配置。 那么其他哨兵怎么知道这份配置是最新的呢？ 领头哨兵准备执行主从切换前，会从要切换成新Master节点的Slave节点取得一个configuration epoch，可以理解为配置版本号。如果领头哨兵主从切换失败了，那么其他哨兵会等待failover-timeout时间然后接替继续执行切换，每次接替都会重新获取一个configuration epoch，作为新的配置版本号。如果领头哨兵切换成功，那么其他哨兵会根据自己的配置版本号来更新对应Slave节点的Master节点配置。 最后为什么说哨兵最少要3个节点，举个例子： 如果是2个节点，{QUORUM}值为1，此时其中一台服务器出现客观宕机。领头哨兵需要进行主从切换，在进行主从切换前需要获取{MAJORITY}数量的哨兵同意，该{MAJORITY}参数最小的值是：2，此时领头哨兵无法进行主从切换。 https://redis.io/topics/sentinel","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Redis学习笔记 - 主从复制","date":"2020-03-24T16:00:00.000Z","path":"2020/03/25/redis-replication/","text":"Redis单点配置的情况下如果服务出现故障宕机，那么服务也就处于不可用状态，假如是生产环境那么带来的后果会有很严重，所以出现了高可用方案：集群策略。 1.三种集群策略Redis提供了三种集群策略，它们分别是： 主从复制（Replication） 哨兵（Sentinel） 集群（Cluster） 这三种策略会逐一学习，本篇主要学习策略之一主从复制（Replication）。 2.主从复制的概念在主从复制（Replication）策略中，服务分为两类：Master节点、Slave节点。 特点 Master节点可以拥有多个Slave节点，但Slave节点只能服务一个Master节点。 数据复制方向只能是Master节点-&gt;Slave节点。 优点 实现了多机热数据备份，提高了面对宕机数据恢复的灾备能力。 在主从复制的基础上实现读写分离提高服务吞吐量，即：Master节点提供写服务，Slave节点提供读服务。 如果Master节点宕机可以快速切换使用Slave节点提供服务。 缺点 Master节点故障，无法自动切换Slave节点为新Master节点的问题。 Master节点写操作的压力没有得到解决。 数据存储能力还是受到单节点限制。 3.如何配置（多种方式） 在Slave节点服务器redis.conf增加配置行，slaveof {MASTER_IP} {MASTER_PORT}。 启动redis-server时，./redis-server slaveof {MASTER_IP} {MASTER_PORT}。 在redis-cli命令行界面输入，slaveof {MASTER_IP} {MASTER_PORT}。 4.工作机制 Slave节点执行slaveof命令，保存Master节点信息。 节点内部的定时任务发现主节点信息，开始尝试Socket连接主节点。 连接建立成功，Slave节点发送ping命令，期望得到pong命令响应，否则发起重连。 如果主节点有设置auth，那么进行auth验证，成功继续，失败终止。（非必须，根据Master节点是否配置auth决定） Slave节点同步Master节点全量数据集。（该操作是Master节点向Slave节点发送数据哟） Master节点持续把写命令同步Slave节点。 5.同步命令Redis主从复制数据有两个命令，sync和psync，sync是Redis2.8版本之前的同步方法，psync是Redis2.8版本以后优化sync新设计同步方法。在这会着重学习psync，也会捎带说一下为什么sync会被优化。 首先，psync需要3个参数支持： Master节点和Slave节点复制数据的偏移量。 1Master节点和Slave节点复制数据的偏移量，主要作用是通过对比复制偏移量，来判断Master节点和Slave节点数据是否一致。 Master节点复制积压缓冲区。 1psync的特性之一，用于增量数据复制和补救丢失的复制数据。 Master节点的RunID（Replication ID）。 1Redis服务启动的时，都会生成一个40位的唯一RunID。 Master节点和Slave节点复制数据的偏移量：每个参与复制数据的节点都会维护一份复制偏移量，Master节点在处理完写命令后，会把命令的字节长度进行累加，Slave节点每秒钟会向Master节点上报自己的复制偏移量，因此Master节点也会记录Slave节点的偏移量。Master节点持续把写命令同步Slave节点，Slave节点成功接收到之后也会累加自身的偏移量。查看偏移量：123&gt;info replicationmaster_repl_offset:&#123;NUM&#125; # Master节点偏移量slave_repl_offset:&#123;NUM&#125; # Slave节点偏移量 复制积压缓冲区：复制积压缓冲区是一个保存在Master节点拥有固定长度的队列，该队列先进先出，大小受repl-backlog-size参数控制（默认：1MB），查看缓冲区：12&gt;info replicationrepl_backlog_size:&#123;NUM&#125;（byte） Master节点的RunID（Replication ID）：该ID主要是用来识别Redis服务节点，因为如果使用IP+PORT方式，假如Master节点重启之后修改了RDB/AOF备份文件，此时Slave节点再基于原来的复制偏移量进行复制数据是不可靠的。查看RunID：12&gt;info serverrun_id:8d252f66c3ef89bd60a060cf8dc5cfe3d511c5e4 psync命令使用方式1psync &#123;RUNID&#125; &#123;OFFSET&#125; 6.增量/全量复制知道了命令如何使用，那么当Slave节点发送psync命令给Master节点之后会发生什么？流程分为全量复制和增量复制两种。 全量复制，如果Slave节点发送的命令是：psync ? -1 Master节点知道Slave节点要全量复制数据，返回命令则是：+fullresync {RUNID} {OFFSET}，同时Master节点会执行RDB备份并且使用复制积压缓冲区来记录此后所有的写命令。Master节点 RDB备份完成之后向Slave节点发送备份文件，同时继续缓冲写命令，在备份文件发送完毕后Master节点会向Slave节点发送缓冲区的写命令。Slave节点在收到Master节点发送的备份文件之后，会丢弃所有的旧数据，开始载入备份文件并且开始执行Master节点发送缓冲区的写命令。 值得一提的是，在Slave节点加载备份文件的时候数据处于不可靠阶段，此时可以通过参数slave-server-stale-data（yes、no）配置是否响应请求，yes响应，no则抛出“SYNC with master in progress”。 如果备份从创建到传输完毕消耗时间大于repl-timeout参数的值，Slave节点将会放弃接收备份文件并且清理已经下载的临时文件。 增量复制，Master节点会根据{RUNID}和{OFFSET}决定返回结果。 Master节点首先会检查{RUNID}是否与自身一致，如果不一致将会执行全量数据复制。如果一致会根据{OFFSET}参数在缓冲区查找，如果数据偏移量之后的数据存在缓冲区，返回命令：+continue，表示可以增量复制数据。如果返回命令+err，表示Master节点版本过低不支持psync命令，将会使用sync进行全量复制数据。 最后最后学习一下为什么sync会被优化？ 使用sync命令，在网络或者其他不可抗力因素导致Master节点和Slave节点断开连接，需要重新进行一次全量数据复制，Slave节点数据恢复成本极高。 https://redis.io/topics/replication http://try.redis.io","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Mysql学习笔记 - 分片和分区","date":"2020-03-22T16:00:00.000Z","path":"2020/03/23/mysql-sharding-partition/","text":"之前在复习Mysql主从知识的同时，小伙伴考了我一个问题，分片和分区有什么差别 … 乍一听可能只是名词上的不同，但其实它们俩的确不是同一个东西。 1.什么是分片（Sharding）？我们举个例子，一张文章表。结构如下：123456789CREATE TABLE `mark`.`article` ( `id` int NOT NULL AUTO_INCREMENT, `type` varchar(20) NOT NULL COMMENT '类型', `title` varchar(255) NOT NULL COMMENT '标题', `content` longtext NOT NULL COMMENT '内容', `author` varchar(20) NOT NULL COMMENT '作者', `date` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`)); 需求如下： 用户打开app会直接展示文章列表（类型，标题，作者，日期） 点击文章查看详情 那么这张表在数据量比较小的初期应对访问是没什么问题的。但是随着数据量日益膨胀，查询效率会越来越低，因为里面content字段非常巨大。 这时候要怎么优化呢？把content字段拆出去，因为访问最多的请求并不需要content，怎么拆呢？结构如下： 1234567891011121314151617# 文章表CREATE TABLE `mark`.`article` ( `id` int NOT NULL AUTO_INCREMENT, `type` varchar(20) NOT NULL COMMENT '类型', `title` varchar(255) NOT NULL COMMENT '标题', `c_id` int NOT NULL COMMENT '文章ID', `author` varchar(20) NOT NULL COMMENT '作者', `created_at` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`));# 文章内容表CREATE TABLE `mark`.`article_content` ( `id` int NOT NULL AUTO_INCREMENT, `content` longtext NOT NULL COMMENT '内容', `created_at` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`)); 2.垂直（纵向）切分上述这种情况就是分片的一种，垂直切分也被称为纵向切分，这种分片的方式不仅可以跨表也可以跨库。 优点 拆分规则简单。 数据模块清晰。 容易维护，容易定位问题。 缺点 如果分片表跨库，那么在SQL层无法进行连表查询，只能在程序层处理。 事务处理复杂度变高。 后期表结构的扩展性受限。 3.水平（横向）切分说到了垂直切分，那就不得不提一下水平切分也被称之为横向切分。同样举个例子，一张用户日志表。结构如下：12345CREATE TABLE `mark`.`user_log` ( `u_id` int NOT NULL COMMENT '用户ID', `content` longtext NOT NULL COMMENT '日志内容', `created_at` datetime NOT NULL COMMENT '创建日期'); 需求如下： 根据用户ID快速检索日志信息 情况一样，在数据量比较小的时候功能响应速度应该还可以，但是随着数据量增长的比较厉害。查询效率会呈断崖式下降。 那么这时候要怎么优化呢？我们按照用户ID去拆分表，即每个用户存储的日志固定在一张表。结构如下：12345CREATE TABLE `mark`.`user_log_&#123;NUM&#125;` ( `u_id` int NOT NULL COMMENT '用户ID', `content` longtext NOT NULL COMMENT '日志内容', `created_at` datetime NOT NULL COMMENT '创建日期'); 为了省事，后面{NUM}是个数字哈，每个用户进来我们会利用摘要算法（比如：crc32）取个固定数值，然后固定把对应的数据存储到相应的表内。 优点 不会出现跨库无法连表查询的情况。 事务处理相对简单。 很难出现扩展性受限的问题。 缺点 数据分布不平均，可能一张表10W行，另外一张100W行。 难维护，定位问题需要前置算法查询。 后期数据迁移比较麻烦。 4.什么是分区（Partition）？分片简单说了说，那么接下来要学习一下分区。分区和分片比较明显的一点是：分片多是利用程序配合来实现，分区则是数据库（不仅仅是Mysql，其它数据库也有）提供的机制。 首先，分区分为4种模式： Range List Hash Key 5.RangeRange（范围）大概的思想是将数据进行分段，比如按照日期将内购表进行拆分，分为不同的年份。 1234567891011CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY RANGE( YEAR(created_at) ) ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (1995), PARTITION p2 VALUES LESS THAN (2000), PARTITION p3 VALUES LESS THAN (2005), PARTITION p4 VALUES LESS THAN (2010), PARTITION p5 VALUES LESS THAN (2015));# 也可以按照金额来进行分区。# PARTITION BY RANGE( money ) (); 写入以下数据1234567891011INSERT INTO purchase(app_id,name,money,currency,created_at) VALUES(600001,'desk organiser',6.0000,'CNY','2003-10-15'),(600001,'alarm clock',12.0000,'CNY','1997-11-05'),(600002,'chair',30.0000,'CNY','2009-03-10'),(600002,'bookcase',68.0000,'CNY','1989-01-10'),(600002,'exercise bike',128.0000,'CNY','2014-05-09'),(600003,'sofa',258.0000,'CNY','1987-06-05'),(600003,'espresso maker',648.0000,'CNY','2011-11-22'),(600004,'aquarium',99.0000,'USD','1992-08-04'),(600005,'study desk',129.0000,'USD','2006-09-16'),(600006,'lava lamp',299.0000,'USD','1998-12-25'); 然后我们观察一下，可能通过肉眼看没什么变化，执行这条查看分区状态的SQL1234567891011121314151617SELECT partition_name part, partition_expression expr, table_rows FROM information_schema.PARTITIONS WHERE table_schema = SCHEMA () AND table_name = '&#123;TABLE&#125;';# 结果如下part | expr | descr | table_rowsp0 | YEAR(created_at) | 1990 | 2p1 | YEAR(created_at) | 1995 | 1p2 | YEAR(created_at) | 2000 | 2p3 | YEAR(created_at) | 2005 | 1p4 | YEAR(created_at) | 2010 | 2p5 | YEAR(created_at) | 2015 | 2 很明显，数据分别按照规则写进了分区表p0 ~ p5。 6.ListList（列表）模式，多用于对于指定字段的数值进行明确的数据拆分。 123456789CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY LIST( app_id ) ( PARTITION p0 VALUES IN (600001), PARTITION p1 VALUES IN (600002), PARTITION p2 VALUES IN (600003), PARTITION p3 VALUES IN (600004), PARTITION p4 VALUES IN (600005), PARTITION p5 VALUES IN (600006)); 写入同样的数据，之后执行查看分区状态的SQL。1234567part | expr | descr | table_rowsp0 | app_id | 600001 | 2p1 | app_id | 600002 | 3p2 | app_id | 600003 | 2p3 | app_id | 600004 | 1p4 | app_id | 600005 | 1p5 | app_id | 600006 | 1 7.HashHash（哈希）模式通过对一个或多个字段进行Hash计算，通过这个Hash值来进行分区（是不是感觉很像分片，其实分片的思想就是根据分区衍生而来）。 123CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY HASH( YEAR(created_at) )PARTITIONS 4; # 为了区别，设4个分区。 然后写入同样的数据，之后执行查看分区状态的SQL。12345part| expr | descr | table_rowsp0 | YEAR(created_at) | NULL | 1p1 | YEAR(created_at) | NULL | 3p2 | YEAR(created_at) | NULL | 3p3 | YEAR(created_at) | NULL | 3 8.KeyKey（键）模式和Hash模式极其相似，可能有一点区别就是，Hash模式下是用户自定义规则进行Hash计算，而Key模式是Mysql使用自己的函数进行Hash计算。 123CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY KEY( created_at )PARTITIONS 3; # 为了区别，设3个分区。 重复步骤，写入同样的数据，之后执行查看分区状态的SQL。1234part| expr | descr | table_rowsp0 | `created_at` | NULL | 2p1 | `created_at` | NULL | 3p2 | `created_at` | NULL | 5 Key模式和Hash模式在expr上面有了直观的不同表现，Hash模式是：YEAR(created_at)，而Key模式是：created_at。这就是我们前面提到的Hash计算方式的区别。 9.最后其实Mysql分区还有第5种模式，叫做Composite（复合）模式，比如：Range - Key123456789101112CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY RANGE( YEAR( created_at ) ) SUBPARTITION BY KEY( created_at )SUBPARTITIONS 3( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (1995), PARTITION p2 VALUES LESS THAN (2000), PARTITION p3 VALUES LESS THAN (2005), PARTITION p4 VALUES LESS THAN (2010), PARTITION p5 VALUES LESS THAN (2015)); 这里就不展开Composite（复合）模式的学习了，最后通过以上的例子，可以看出分区也是有垂直分区和水平分区的说法的。 https://dev.mysql.com/doc/mysql-partitioning-excerpt/5.7/en/partitioning-management.html","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 刷脏机制","date":"2020-03-20T16:00:00.000Z","path":"2020/03/21/mysql-checkpoint/","text":"之前学习了LRU算法和Mysql缓冲池使用的LRU变体算法，其中有个共同点就是当LRU链表写满以后如果再有新数据进来会淘汰尾部的数据，那么Mysql淘汰这些尾部数据的时候是否会进行什么操作呢？这就是我们在最后提到了一个参数Modified db pages，即脏页。 1.什么是脏页？ 脏页，这个名词很抽象从字面意思去看可能很不解，脏页是当内存中的数据页和磁盘中的数据页内容不一致时，这个数据页称之为脏页。因为从操作系统的角度来讲，自己读入的数据被外部所修改等于被污染，所以叫脏页。 当内存中的数据页和磁盘中的数据页数据一致，叫干净页。 2.脏页什么时候会刷新？ 缓冲池（buffer pool）空间不足，也就是LRU链表写满时，新数据进来时淘汰掉的尾部数据脏页。 redo log不可用时，需要强制将脏页列表中的一些数据页刷入磁盘。 Mysql在服务器负载较小时，会主动进行刷脏操作。 Mysql服务正常关闭，会刷新所有脏页。 3.什么是redo log？InnoDB中两块非常重要的日志，一个是undo log，另外一个就是我们接下来要学习的redo log。前者用来保证事务的原子性以及InnoDB的MVCC（Mutil-Version Concurrency Control），后者用来保证事务的持久性。 那么什么时候写redo log呢？当数据库对数据做修改的时候，需要把数据页从磁盘读到缓冲池中，然后在缓冲池中进行修改。但是InnoDB采用的是WAL（Write Ahead Log）策略来防止数据丢失，也就是事务提交时，先写redo log才会再去修改内存数据页。 redo log的文件名默认以ib_logfile{NUM}，存储在my.cnf中datadir目录下面，受以下两个参数控制 innodb_log_file_size，日志大小 innodb_log_files_in_group，日志个数，默认是2个。 所以redo log的大小等于innodb_log_files_in_group*innodb_log_file_size。 既然redo log会产生，那么什么时候会被覆盖呢？redo log被设计成可循环使用，当日志文件写满时那些已经被刷入磁盘中的数据就可以被覆盖啦。 WAL（Write Ahead Log），是关系数据库系统中用于提供原子性和持久性（ACID属性中的两个）的一系列技术，ARIES是WAL系列技术常用的算法，在文件系统中WAL通常称为journaling。WAL的主要思想是将元数据的实时变更操作写入日志文件中，然后在系统负载较小时再把日志刷入磁盘。主要是为了减少磁盘的IO操作，此处就不展开学习了。先Mark一下 4.CheckpointCheckpoint（检查点），在数据库中一般是用来把redo log脏页刷入磁盘的一个操作，通过LSN保存记录，作用是当发生宕机等crash情况时，再次启动时会查询Checkpoint，在该Checkpoint之后发生的事务修改恢复到磁盘。通俗来解释，就像我们玩一些游戏每过不久就会存一次档，然后如果游戏客户端不幸crash重新进入最近的一次存档即可，同理。 Checkpoint存在的目的： 缩短恢复数据时间。 缓冲池写满时，淘汰脏页刷入磁盘。 redo log写满时，进行刷脏操作。 那么怎么查看我们的检查点呢？可以使用命令show engine innodb status来查看：12345678910111213&gt;show engine innodb status\\G;---LOG---Log sequence number 1597945Log flushed up to 1597945Last Checkpoint at 1597945Max Checkpoint age 7782360Checkpoint age target 7539162Modified age 0Checkpoint age 00 pending log writes, 0 pending chkp writes8 log i/o's done, 0.42 log i/o's/second 12345Log sequence number | 当前系统LSN最大值，新的日志LSN将在此基础上生成（LSN+新日志的大小）。Log flushed up to | 当前已经写入日志文件的LSN。Last Checkpoint at | 当前已经写入Checkpoint的LSN。Max Checkpoint age | Percona的XtraDB参数，此处不过多解释。Checkpoint age target | 同上。 LSN（Log Sequence Number），LSN是日志空间中每条日志的结束点，用字节偏移量来表示。每个数据页有LSN，redo log也有LSN，Checkpoint亦有LSN。该LSN记录当前数据页最后一次修改的LSN号，用于在恢复数据时对比重做日志LSN号决定是否对该数据页进行恢复数据。可以通俗理解为，存档编号。 5.Checkpoint什么时候会触发？InnoDB存储引擎有两种Checkpoint，分别是： Sharp Checkpoint Fuzzy Checkpoint Sharp Checkpoint发生在数据库服务关闭时，将所有脏页刷入磁盘，此时innodb_fast_shutdown参数的值为1（innodb_fast_shutdown参数的值：0、1、2），这是默认机制。但是考虑到如果数据库在使用时也执行这种机制，数据库的性能会受到影响，所以Fuzzy Checkpoint刷新部分脏页的这种机制产生了。 Fuzzy Checkpoint刷新部分脏页，也分为以下几种方式： Master Thread Checkpoint FLUSH_LRU_LIST Checkpoint Dirty Page too much Checkpoint Async/Sync Flush Checkpoint Master Thread Checkpoint 主线程以每秒或者每十秒从缓冲池的脏页列表（Flush List）刷新一定比例的数据页回磁盘，这个操作过程是异步的不会阻塞线程。 FLUSH_LRU_LIST Checkpoint InnoDB需要保证LRU链表中有足够空闲页可以使用，在InnoDB1.1.x版本前，如果LRU链表写满有新的数据进来如果淘汰尾部脏页，会触发Checkpoint机制强制进行刷脏操作。该操作是阻塞线程的，所以在InnoDB1.2.x版本开始，这个操作放到Page Cleaner Thread来处理，每次刷新LRU链表脏页的数量受innodb_lru_scan_depth参数控制（默认：1024）。 Dirty Page too much Checkpoint 当LRU链表中脏页数量过多时（比例），InnoDB为了保证缓冲池中有足够多的空闲页可以使用，会强制触发Checkpoint机制进行刷脏操作。此值受innodb_max_dirty_pages_pct参数控制（默认：75%）。 Async/Sync Flush Checkpoint 为了保证redo log循环使用的可重用性，在redo log不可用时会强制触发Checkpoint刷脏操作。在InnoDB1.2.x版本以前，Async Flush Checkpoint会阻塞当前查询线程，Sync Flush Checkpoint会阻塞所有查询线程。InnoDB1.2.X之后放到单独的Page Cleaner Thread来处理。 6.最后关于Async/Sync Flush Checkpoint刷脏方式的原理有些复杂，这里先Mark一下，暂时不展开学习了。 前面说到的Percona XtraDB https://www.percona.com/doc/percona-server/8.0/scalability/innodb_io.html","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Laravel + Dropzone实现文件分片上传","date":"2020-03-18T16:00:00.000Z","path":"2020/03/19/laravel-chunk-upload/","text":"1.什么是分片上传？ 如果我们上传的文件是一个很大的文件，那么上传的时间应该会比较久，再加上网络不稳定各种因素的影响，很容易导致传输中断。 服务端一般都会设置固定大小的接收BUFF，往往上传文件的体积是该值的几何倍数。 分片上传的原理大致可以描述为，把一个较大文件分成若干份的分片一个一个传输，服务端在接收到最后一个分片后进行合并资源。 2.前端前端使用Dropzone控件，DropzoneJS是一个开源库，提供带有图像预览的拖放文件上传并且有分割文件分片上传的机制，当然该控件不仅仅可以上传图片，经测试视频，Word等文件一样可以。 Dropzone官网提供了一个simple.html的上传demo（js、css文件路径有改动过），内容如下：1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;meta charset=\"utf-8\"&gt;&lt;title&gt;Dropzone simple example&lt;/title&gt;&lt;!-- DO NOT SIMPLY COPY THOSE LINES. Download the JS and CSS files from the latest release (https://github.com/enyo/dropzone/releases/latest), and host them yourself!--&gt;&lt;script src=\"./dropzone.js\"&gt;&lt;/script&gt;&lt;link rel=\"stylesheet\" href=\"./dropzone.css\"&gt;&lt;p&gt; This is the most minimal example of Dropzone. The upload in this example doesn't work, because there is no actual server to handle the file upload.&lt;/p&gt;&lt;!-- Change /upload-target to your upload address --&gt;&lt;form action=\"api/upload\" class=\"dropzone\"&gt;&lt;/form&gt; Dropzone官方提供的demo默认是没有开启分片上传的，需要修改dropzone.js123456789101112/** * Whether you want files to be uploaded in chunks to your server. This can't be * used in combination with `uploadMultiple`. * * See [chunksUploaded](#config-chunksUploaded) for the callback to finalise an upload. */chunking: false, // 修改为true,即开启分片/** * If `chunking` is `true`, then this defines the chunk size in bytes. */chunkSize: 2000000, // 默认分片大小，单位：Byte 3.后端 框架：Laravel 6.8扩展：laravel-chunk-upload代码如下：1234567891011121314151617181920212223242526 // create the file receiver$receiver = new FileReceiver(\"file\", $request, HandlerFactory::classFromRequest($request));// check if the upload is success, throw exception or return response you needif ($receiver-&gt;isUploaded() === false) &#123; throw new UploadMissingFileException();&#125;// receive the file$save = $receiver-&gt;receive();// check if the upload has finished (in chunk mode it will send smaller files)if ($save-&gt;isFinished()) &#123; // save the file and return any response you need, current example uses `move` function. If you are // not using move, you need to manually delete the file by unlink($save-&gt;getFile()-&gt;getPathname()) return $this-&gt;saveFile($save-&gt;getFile());&#125;// we are in chunk mode, lets send the current progress/** @var AbstractHandler $handler */$handler = $save-&gt;handler();return response()-&gt;json([ \"done\" =&gt; $handler-&gt;getPercentageDone(), 'status' =&gt; true]); 4.Demo https://github.com/xupin/chunk-upload-example","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"laravel","slug":"laravel","permalink":"http://xupin.im/tags/laravel/"},{"name":"dropzone","slug":"dropzone","permalink":"http://xupin.im/tags/dropzone/"}]},{"title":"Mysql学习笔记 - LRU算法","date":"2020-03-17T16:00:00.000Z","path":"2020/03/18/mysql-lru/","text":"1.什么是LRU？LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“最近使用的页面数据会在未来一段时期内仍然被使用,已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用”。 2.LRU的实现LRU算法最常见的实现是使用链表来保存数据，该链表是双向链表，然后利用先进先出的特性，最新写入的数据会最快被获取。 访问不存在的数据时，缓存数据则会写入链表的头部，链表写满时会淘汰掉尾部的缓存数据。 当缓存数据被访问时，则将该缓存数据向链表头部移动。 优点：面对频繁访问的热点数据，查询效率高 缺点：如果一次查询扫描全表，那么LRU列表会被污染 3.Mysql LRU算法有何不同？Mysql（InnoDB）的缓冲池（buffer pool）使用了LRU算法的变体，将链表分为三个部分：young、midpoint、old。链表比例(5、3)分配给young、old。其中young占据5/8，old占据3/8，此比例受参数innodb_old_blocks_pct控制。 young是链表中最近访问过的新子表。 old是链表中最近访问的旧子表。 midpoint是介于新子表 &amp; 旧子表的边界中间位置（也可以理解为旧子表的头部）。 数据库刚启动LRU链表为空时，此时会检查Free List中是否有空闲的数据页，如果有则从Free List中删除并且在LRU链表中写入相同的数据页。 访问不存在的数据页时，数据页不会直接写入链表的头部，而是写入中间位置，如果链表满了则会从旧子表尾部淘汰数据页。 当数据页被访问时，则判断访问数据页的时间是否大于设定innodb_old_blocks_time（默认：1000ms），如果大于则向链表头部移动，如果小于其位置不变。 改进优点：能够防止单次大量的全表扫描污染整个LRU链表。 4.Mysql LRU相关查询命令123456789101112131415161718192021222324252627282930&gt;show engine innodb status\\G;----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 137756672; in additional pool allocated 0Total memory allocated by read views 88Internal hash tables (constant factor + variable factor) Adaptive hash index 2217584 (2213368 + 4216) Page hash 139112 (buffer pool 0 only) Dictionary cache 593780 (554768 + 39012) File system 83536 (82672 + 864) Lock system 333248 (332872 + 376) Recovery system 0 (0 + 0)Dictionary memory allocated 39012Buffer pool size 8191Buffer pool size, bytes 134201344Free buffers 8048Database pages 143Old database pages 0Modified db pages 0Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 0, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 143, created 0, written 00.00 reads/s, 0.00 creates/s, 0.00 writes/sNo buffer pool page gets since the last printoutPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 143, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0] 123456Buffer pool size | innodb_buffer_pool的大小Free buffers | 当前Free List中数据页数量Database pages | LRU链表中数据页数量Old database pages | LRU链表中旧子表数据页数量Modified db pages | LRU链表中脏页数量Pages made young 0, not young 0 | 数据页从old移至young的行为：page made young,数据页因为innodb_old_blocks_time导致没有从old移至young的行为：page not young。后面的数值是该行为发生的次数。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - Int长度的问题","date":"2020-03-15T11:30:00.000Z","path":"2020/03/15/mysql-int-length/","text":"创建数据表的时候我们总是要考虑存储数据的字段该用哪种类型、多少长度比较合适，不过在Mysql中如果字段类型是Int，此时长度是不生效的。 1.Int类型的长度问题12345CREATE TABLE `test` ( `id` int(3) NOT NULL, `name` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci 有这样一张表，id是Int类型长度3的字段，那么理论上是不能存储长度超过3的值，比如：1INSERT INTO `test`(`id`, `name`) VALUES (1000000, &apos;name&apos;); 不过结果是意外的，这条数据可以正常写入。 2.Int类型的长度为什么不生效翻了一下Mysql的数据类型文档，发现这么一句话： 对于整数类型，M表示最大显示宽度。 对于浮点和定点类型，M是可以存储的总位数（精度）。 对于字符串类型，M是最大长度。 M的最大允许值取决于数据类型。 所以这个Int类型的“长度”其实叫宽度，也可以理解为显示长度。 3.如果存储的数据长度低于显示宽度会怎样？1INSERT INTO `test`(`id`, `name`) VALUES (1, &apos;name&apos;); 当然是可以写入的，但是好像并没有什么特别的不一样，不过我们再执行一条SQL1ALTER TABLE `test` MODIFY COLUMN `id` int(3) UNSIGNED ZEROFILL; 这次数据产生变化了，变成了这样123id | name001 | name1000000 | name 4.为什么要设计显示宽度？我也没有得出明确的答案，和小伙伴讨论这个问题~ta觉得是为了方便排序，你觉得呢？","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql int length","slug":"mysql-int-length","permalink":"http://xupin.im/tags/mysql-int-length/"}]},{"title":"Golang 实现定时任务调度","date":"2020-01-19T11:30:00.000Z","path":"2020/01/19/go-console/","text":"每一个任务都需要编写一个Crontab命令，这是件很麻烦且很不友好的事情。 任务调度器允许你以代码的形式定义调度命令，并且服务器上只需要一个Crontab命令即可, 任务调度又是我们俗称的 “计划任务” 1.工欲善其事，必先利其器Github一番搜索，发现Golang有个cron包（robfig/cron）大概满足需求，于是学习一下。 2.介绍看了一遍文档，cron包支持的已经很全面了~~~不用自己造轮子了。 表达式 兼容Linux的crontab表达式（支持分钟级别）。 日志 可以很详细的记录调度的任务状态 时区 支持任务级别的时区配置 预定义计划 支持在未来指定的时间去运行 线程安全问题 该cron lib管理任务队列的slice没有做并发安全考虑，可能会出现任务竟抢行为。 3.实例1234567891011121314151617181920212223242526package mainimport ( \"fmt\" \"time\" \"github.com/robfig/cron\")func main() &#123; c := cron.New() c.AddFunc(\"* * * * * *\", test) c.Start() timer := time.NewTimer(time.Second * 10) for &#123; select &#123; case &lt;-timer.C: timer.Reset(time.Second * 10) &#125; &#125;&#125;func test() &#123; fmt.Println(\"I'm a test script!\")&#125; 12345$ go run .I'm a test script!!!I'm a test script!!!I'm a test script!!!... 4.简单封装 main.go 123456789101112131415package mainimport \"cron-example/console\"func main() &#123; quitChan := make(chan bool, 1) // console go func() &#123; console.Default() &#125;() &lt;-quitChan&#125; console.go 12345678910111213141516171819202122232425262728293031package consoleimport ( \"log\" \"time\" \"cron-example/console/commands\" \"github.com/robfig/cron\")func schedule(c *cron.Cron) &#123; c.AddFunc(\"* * * * * *\", commands.Test)&#125;func Default() &#123; log.Println(\"Starting...\") c := cron.New() c.Start() schedule(c) timer := time.NewTimer(time.Second * 10) for &#123; select &#123; case &lt;-timer.C: timer.Reset(time.Second * 10) &#125; &#125;&#125; https://github.com/xupin/go-cron-example","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"PHP踩坑小记 - aliyundb SQLSetStatement is NOT supported","date":"2020-01-07T11:30:00.000Z","path":"2020/01/07/php-aliyundb/","text":"PHP项目在数据库迁移使用ADB（aliyundb）时发现这个问题，具体错误描述是：1SQLSTATE[HY000]: General error: 1815 [15022, 2020010711034801025210201503151413416] statement type: class com.alibaba.fastsql.sql.ast.statement.SQLSetStatement is NOT supported! (SQL: select * from `users`) 在排查该问题的过程中，尝试了各种方式，降版本、使用原生语句等，甚至查找ADB手册和Issue还是无法解决。 1.原因因为ADB兼容Mysql但是有些机制还是略有不同，阅读源码后发现比如：ADB就不能很好的支持本地预处理语句，这也是该异常的原因。 2.解决方案PHP PDO链接的属性值PDO::ATTR_EMULATE_PREPARES需要设置为true1PDO::ATTR_EMULATE_PREPARES =&gt; true","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"php","slug":"php","permalink":"http://xupin.im/tags/php/"}]},{"title":"高性能Golang WEB框架Gin - 初探","date":"2019-12-31T04:30:00.000Z","path":"2019/12/31/go-gin/","text":"Gin 是一个用 Go (Golang) 编写的 HTTP web 框架。 它是一个类似于 martini 但拥有更好性能的 API 框架, 由于 httprouter，速度提高了近 40 倍。 1.有哪些优点 较高的性能（Golang WEB框架比对数据） 简单易用的中间件 使用了性能高可扩展的HTTP路由httprouter 社区长期有着很高的活跃度 Github星星多（Orz） …等 2.安装/更新安装1$ go get github.com/gin-gonic/gin 更新1$ go get -u github.com/gin-gonic/gin 3.Hello World1234567891011121314151617181920package mainimport ( \"github.com/gin-gonic/gin\")func main() &#123; // 框架 g := gin.Default() // 路由 g.GET(\"/t\", test) // 服务端口 g.Run(\":4000\")&#125;func test(c *gin.Context) &#123; c.String(200, \"Hallo!\")&#125; 运行1$ go run main.go 访问 http://127.0.0.1:4000/t ，页面输出Hallo!。 4.路由Gin支持的路由方式和大部分主流框架基本一致。12345678910111213func main() &#123; g := gin.Default() g.GET(\"/someGet\", getting) g.POST(\"/somePost\", posting) g.PUT(\"/somePut\", putting) g.DELETE(\"/someDelete\", deleting) g.PATCH(\"/somePatch\", patching) g.HEAD(\"/someHead\", head) g.OPTIONS(\"/someOptions\", options) g.Run(\":4000\")&#125; Gin同样也支持路由参数，不过不支持路由正则表达式。1234567891011121314151617181920func main() &#123; g := gin.Default() // 此 handler 只能匹配 /user/&#123;PARAM&#125; g.GET(\"/user/:name\", func(c *gin.Context) &#123; name := c.Param(\"name\") c.String(http.StatusOK, \"Hello %s\", name) &#125;) // 此 handler 会匹配 /user/&#123;PARAM&#125;/ 和 /user/&#123;PARAM&#125;/&#123;ACTION_2&#125; // 如果访问 /user/&#123;PARAM&#125;，在没有 \"/user/:name\" 路由的情况下，会重定向至 /user/&#123;PARAM&#125;/ 匹配当前路由。 g.GET(\"/user/:name/*action\", func(c *gin.Context) &#123; name := c.Param(\"name\") action := c.Param(\"action\") message := name + \" is \" + action c.String(http.StatusOK, message) &#125;) g.Run(\":4000\")&#125; 5.路由组Gin在路由分组上的做法和其他框架也都是一致的，再次体现了Gin简单易用极容易上手的优点。1234567891011121314151617func main() &#123; g := gin.Default() userGroup := g.Group(\"/user\") &#123; userGroup.POST(\"/login\", ctrls.Login) userGroup.POST(\"/logout\", ctrls.Logout) &#125; reportGroup := g.Group(\"/report\") &#123; reportGroup.POST(\"/revenue\", ctrls.RevenueReport) reportGroup.POST(\"/cost\", ctrls.CostReport) &#125; g.Run(\":4000\")&#125; 6.中间件Gin的中间件分为：全局，路由组，路由级别。123456789101112131415func main() &#123; g := gin.Default() // 全局中间件 g.Use(middleware) // 路由组中间件 routerGroup := g.Group(\"/user\", middleware) &#123; // 路由中间件 routerGroup.GET(\"/login\", middleware, ctrls.Login) &#125; g.Run(\":4000\")&#125; 7.参数Gin如何获取请求参数？下面的简单例子：123456789101112131415161718192021222324func main() &#123; g := gin.Default() // URL参数 // URL：/user?email=mark@im.com g.GET(\"/user\", func(c *gin.Context) &#123; // 使用 DefaultQuery 或者 Query email := c.DefaultQuery(\"email\", \"default value\") email = c.Query(\"email\") c.String(http.StatusOK, \"Hallo, %s\", email) &#125;) // POST参数 g.POST(\"/user/login\", func(c *gin.Context) &#123; // 使用 DefaultPostForm 或者 PostForm email := c.PostForm(\"email\") email = c.DefaultPostForm(\"email\", \"default value\") c.String(http.StatusOK, \"Hallo, %s\", email) &#125;) g.Run(\":4000\")&#125; 最后Gin支持的操作还有很多，比如：数据绑定、数据验证、上传文件、静态资源嵌入等。 Gin没有提供的组件也有很多，比如：ORM、Console、日志滚动分割等。 仁者见仁智者见智，Gin专注做HTTP WEB框架的核心，更多扩展需要开发人员自己去选择，组件化的设计无疑是好的。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"10分钟学会使用Tmux","date":"2019-12-24T04:30:00.000Z","path":"2019/12/24/tmux/","text":"Tmux是一个终端多路复用器：它使从单个屏幕创建，访问和控制多个终端成为可能。Tmux可能会与屏幕分离，并继续在后台运行，然后再重新连接。 1.Tmux的结构 Session Window Panel 即，Tmux允许访问多个会话，每个会话可以拥有多个窗口，每个窗口还可以进行划分窗格。 2.安装运行Tmux安装12345678# centos$ yum install tmux# ubuntu$ apt install tmux# mac$ brew install tmux 运行1$ tmux 效果图，如下： 底面状态栏的左侧是会话窗口相关信息，右侧是系统相关信息。 退出 1$ exit // 或者Ctrl + D 3.Tmux怎么分屏呢？首先，在Tmux中所有的快捷键都是以Ctrl + B开头的（默认是这样）。 横向划分窗格：Ctrl + B %竖向划分窗格：Ctrl + B “（英文） 窗格分好了，怎么切换窗格呢？ Ctrl + B Up/Down/Left/Right（键盘方向键）亦或自定义键。12345vim ~/.tmux.confbind h select-pane -Lbind j select-pane -Dbind k select-pane -Ubind l select-pane -R 示例图，如下： 最后本小白文仅仅简单教会入门使用Tmux，Tmux支持的操作远远不止这么简单。 比如快捷键： 新建窗口：Ctrl + B c 切换下一个窗口：Ctrl + B n 切换上一个窗口：Ctrl + B p 比如命令： 新建会话：tmux new -s 会话列表：tmux ls / tmux list-session 关于Tmux更多内容可以去官方文档了解一下 https://github.com/tmux/tmux#documentation","tags":[{"name":"tools","slug":"tools","permalink":"http://xupin.im/tags/tools/"},{"name":"tmux","slug":"tmux","permalink":"http://xupin.im/tags/tmux/"}]},{"title":"Elasticsearch实现字段分组","date":"2019-12-17T11:30:00.000Z","path":"2019/12/17/elasticsearch-group/","text":"Elasticsearch是Elastic Stack核心的分布式搜索和分析引擎。 1.背景因为业务调整采用了ES作为数据库，所以需要了解ES对于这一块的设计如何实现类似Mysql中Group By的查询的效果。 2.实现方式ES实现Group By有两种方式：TermsAgg、CompositeAgg，它们也具有不同程度的优缺点。 TermsAgg的使用方式非常粗暴，直接进行桶嵌套即可，如下: TermsAgg请求DSL语句12345678910111213141516171819&#123; \"aggregations\": &#123; \"group_app\": &#123; \"terms\": &#123; \"field\": \"app\", \"size\": 1000000 &#125;, \"aggregations\": &#123; \"group_campaign_id\": &#123; \"terms\": &#123; \"field\": \"campaign_id\", \"size\": 1000000 &#125; &#125; &#125; &#125; &#125;, \"size\": 0&#125; TermsAgg执行结果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; \"took\": 142, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 175033, \"max_score\": 0.0, \"hits\": [] &#125;, \"aggregations\": &#123; \"group_app\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"com.aa.bb.cc\", \"doc_count\": 59929, \"group_campaign_id\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"Campaign_118\", \"doc_count\": 56466 &#125;, &#123; \"key\": \"Campaign_119\", \"doc_count\": 1937 &#125; ] &#125; &#125;, &#123; \"key\": \"com.dd.ee.ff\", \"doc_count\": 23231, \"group_campaign_id\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"Campaign_120\", \"doc_count\": 16692 &#125;, &#123; \"key\": \"Campaign_121\", \"doc_count\": 5336 &#125; ] &#125; &#125; ] &#125; &#125;&#125; CompositeAgg的使用方式略微有一些不同，下面： CompositeAgg请求DSL语句1234567891011121314151617181920212223242526&#123; \"aggregations\": &#123; \"group_by\": &#123; \"composite\": &#123; \"sources\": [ &#123; \"app\": &#123; \"terms\": &#123; \"field\": \"app\" &#125; &#125; &#125;, &#123; \"campaign_id\": &#123; \"terms\": &#123; \"field\": \"campaign_id\" &#125; &#125; &#125; ], \"size\": 1000 &#125; &#125; &#125;, \"size\": 0&#125; CompositeAgg执行结果123456789101112131415161718192021222324252627282930313233343536373839&#123; \"took\": 14, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 175033, \"max_score\": 0.0, \"hits\": [] &#125;, \"aggregations\": &#123; \"group_by\": &#123; \"after_key\": &#123; \"app\": \"com.dd.ee.ff\", \"campaign_id\": \"Campaign_120\" &#125;, \"buckets\": [ &#123; \"key\": &#123; \"app\": \"com.aa.bb.cc\", \"campaign_id\": \"Campaign_119\" &#125;, \"doc_count\": 182 &#125;, &#123; \"key\": &#123; \"app\": \"com.dd.ee.ff\", \"campaign_id\": \"Campaign_120\" &#125;, \"doc_count\": 40 &#125; ] &#125; &#125;&#125; 3.优缺点TermsAgg 优点：使用简单、没有数据量限制。 缺点：数据结构层次深、不支持分页。 CompositeAgg 优点：使用简单、数据结构清晰可读，支持分页。 缺点：有数据量限制。 最后至于ES这两种分组方式的性能方面还没有研究，后续学习笔记会更新性能上面的差异。 https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-composite-aggregation.html // 附上ES官方文档","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://xupin.im/tags/elasticsearch/"}]},{"title":"Nginx正向代理和反向代理","date":"2019-12-17T04:30:00.000Z","path":"2019/12/17/nginx-proxy/","text":"代理通常用于在几台服务器之间分配负载，无缝显示来自不同网站的内容或通过除HTTP之外的协议将处理请求传递给应用服务器。 1.代理的常用场景及优点正向代理 gfw科学上网。 客户端访问鉴权。 反向代理 负载均衡。 保证内网的安全，阻止web攻击。 2.正向代理和反向代理有什么不同比较通俗的来解释：正向代理代理的是客户端，服务端不知道实际发起请求的客户端、反向代理代理的是服务端，客户端不知道实际接收请求的服务端。 一张图表示（来源：知乎wplulala）","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"nginx","slug":"nginx","permalink":"http://xupin.im/tags/nginx/"}]},{"title":"Mysql学习笔记 - 索引","date":"2019-12-16T11:30:00.000Z","path":"2019/12/16/mysql-index/","text":"索引用于快速查找具有特定列值的行。没有索引，MySQL必须从第一行开始，然后通读整个表以找到相关的行。桌子越大，花费越多。如果表中有相关​​列的索引，MySQL可以快速确定要在数据文件中间查找的位置，而不必查看所有数据。这比顺序读取每一行要快得多。InnoDB和MyISAM只支持BTREE，因此默认均是BTree，MEMORY和HEAP支持Hash和Btree，如无明确声明，则默认索引均是Hash（包括主键）。 1.Mysql有哪些常用的索引 主键索引：数据列不允许重复，不允许为null，一个表只能有一个主键。 唯一索引：数据列不允许重复，允许为null，一个表允许多个列创建唯一索引。 普通索引：基本的索引类型，没有唯一性的限制，允许为null。 全文索引：全文索引是目前实现大数据搜索的关键技术。 2.如何分析SQL语句执行性能以下是两种分析SQL性能的常用方式，explain、show profiles/show profile。 explain + SQL语句，获取SQL分析数据 select_type：对应SQL语句的查询复杂度。 table：正在访问的表。 partitions：数据所在的分区。 type：表示是否用上索引，以及索引是如何使用的，此字段决定索引的性能。ALL&lt;TYPE&lt;RANGE&lt;REF&lt;CONST possible_keys：查询条件存在的索引。 key：触发的索引。 key_len：索引字段的长度。 ref：索引访问，返回所有匹配某个单值的行。 rows：执行查询必须检查的行数，在InnoDB中此值不精确。 filtered：条件过滤出的行数的百分比。 extra：查询分析结果的额外信息，很重要 e.g. Using index、Using where … show profiles 获得当前会话中执行的SQL语句，字段为：Query_ID, Duration, Query，show profile all for query {Query_ID} 3.Mysql索引的建立原则Mysql的索引遵循最左原则，在创建多列索引时，要根据业务需求，where条件中使用最频繁的一列放在最左边。12345678910111213index(a,b,c)where a = v // 使用awhere a = v and b = v // 使用a,bwhere a = v and b = v and c = v // 使用a,b,cwhere b = v // 未使用索引where a = v and c = v // 使用awhere b = v and c = v // 未使用索引where a &gt; v // 是否使用索引，取决于查询结果集，如果全表扫描速度比索引速度快，那么不使用索引。where a like \"v%\" // 使用awhere a like \"%v%\" // 不适用索引 4.Mysql中索引建立常见问题 为经常需要排序、分组和联合操作的字段建立索引 为常作为查询条件的字段建立索引 索引列值保证唯一性 索引建立的数量不要过多 索引列不要使用函数或者表达式 行锁依赖索引的建立 普通索引的数据重复率过高会导致索引失效 最左原则","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 事务隔离级别","date":"2019-12-05T04:30:00.000Z","path":"2019/12/05/mysql-transaction/","text":"什么是事务？事务（Transaction）一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)，具有4个特性：原子性、一致性、隔离性、持久性，简称ACID。 那么Mysql的事务隔离级别又是什么呢？Mysql的事务隔离级别分别是4种：未提交读（Read Uncommitted）、已提交读（Read Committed）、Repeatable Read（可重复读）、Serializable（可串行化）。可以简单理解为Mysql事务的4种执行标准。 一、ACID特性 原子性（Atomicity）：一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。 一致性（Consistency）：事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 隔离性（Isolation）：一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durability）：指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 二、Mysql事务隔离级别 未提交读（Read Uncommitted），指当前事务可以读取到其他事务还未提交的数据变化，最容易带来的问题是脏读（Dirty Read），很少应用到生产环境。 已提交读（Read Committed），大部分数据库默认隔离级别但不包括Mysql，指当前事务可以读取其他事务已提交的数据变化，具有隔离性的基本标准，但是在出现交叉事务的并发操作场景中会发生两次读取的数据结果集不一致的问题，即不可重复读（Nonrepeatable Read）。 Repeatable Read（可重复读），Mysql数据库的默认隔离级别，这里顺便解释一下为什么Mysql的隔离级别不是Read Committed，因为在Mysql5.0之前日志格式只有statement这一种，这种格式导致主从复制一致性很难得到保证。这种隔离级别可以解决Nonrepeatable Read的问题，但如果也出现在交叉事务的并发操作场景会出现幻读（Phantom Read）的现象，这一行为状态是当前事务不能及时有效的读取其他事务的数据变化。 Serializable（可串行化），Mysql事务隔离最高级别，望文生义大概意思是事务的执行是有序（串行化顺序）的，不存在事务交叉执行的场景从而解决了Phantom Read的问题，但是在高并发的业务场景下请求会出现阻塞、超时、锁竟抢的问题，系统的可用性也随之降低。 收尾 https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html // Mysql事务官方文档","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"通俗解释什么是悲观锁/乐观锁","date":"2019-12-04T11:30:00.000Z","path":"2019/12/04/pessimistic-optimistic/","text":"什么是锁？在多进程（线程）编程中为了数据的一致性、有效性，如果一资源被某进程（线程）上锁，那么在释放锁之前其他进程（线程）无法进行操作或者等待获取（上）锁。 一、悲观锁悲观锁（Pessimistic locking），顾名思义对待任何事务都持悲观态度，做任何事情都会认为会有竞抢行为~所以在进行任何操作之前都要做好万全准备（上锁）才会继续执行后续的操作。通常是利用系统提供的锁机制来实现。比如：12Mysql中的上锁命令：for update、lock in share mode等Redis中的一些操作命令：setnx、getset等（原子操作） 二、乐观锁乐观锁（Optimistic locking），和悲观锁相反对待任何事务都持乐观态度，只会在最后即将执行操作的时刻前才会进行验证。通常是通过程序配合来实现。比如：1update `orders` set hash = md5(now()) where uid = 1 and hash = &#123;lastHash&#125;; 三、优缺点悲观锁优点很鲜明，因为每次执行的操作都是独占的，数据的一致性、有效性、安全性较高。但缺点同样突出，每次操作都会产生上锁的开销，在并发请求比较密集的情况下容易阻塞或者驳回请求，甚至是造成死锁，也大大降低了系统的性能。 乐观锁优点是省去了锁的开销，能较高的提高系统吞吐量，缺点是如果出现在并发高且竟抢（冲突）行为比较多的场景下数据的一致性很难保证。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Redis学习笔记 - 持久化方式","date":"2019-12-03T07:30:00.000Z","path":"2019/12/03/redis-aof-rdb/","text":"Redis是一个开放源代码（BSD许可）的内存中数据结构存储，用作数据库，缓存和消息代理。 一、Redis数据存储的方式有两种12单纯的缓存模式, 整个数据的生命周期随着Redis Server的停止而消失。persistence模式, 数据会持续备份到磁盘文件。 二、Redis如何实现持久化存储？Redis提供了两种方式。12RDB(Redis DataBase)AOF(Append-only file) 三、RDBRDB的工作原理有点像运维脚本自动化定时备份数据一样，当内存中的数据到达配置的阈值就会执行DUMP操作备份数据到临时文件,备份成功结束后重命名为dump.rdb文件。123优点：fork子进程来进行备份,父进程不会进行io操作。恢复数据时的速度很快。缺点：这种方式是每隔一次才会进行备份,假如下一次备份前down机会丢失部分数据。而且如果备份的数据比较大，fork的子进程将会比较耗时，这段时间内会导致父进程阻塞。补充：因为这种每隔一段时间去备份一次的方式，类似快照。所以又叫 Snapshot。 四、AOFAOF的工作原理是讲写操作（数据）格式化追加到日志尾部，该日志文件保存了所有的历史操作数据，这一点非常类似Mysql中的bin.log。12优点：AOF这种方式可以保证较高的数据完整性，可以设置不同的策略，比如不保存，每一秒钟保存一次，或者每执行一个命令保存一次。AOF的默认策略为每一秒钟保存一次, 就算出现down机最多也就丢失1秒钟的数据, AOF备份数据因为是在后台线程执行fork子进程, 所以不会阻塞主线程。缺点：对于同样规模的数据备份的文件体积AOF要大于RDB。在备份速度上面也会慢于RDB，恢复速度同样也没有RDB快。 五、AOF备份触发机制1231. Redis服务端接收到客户端bgrewriteaof指令请求，如果当前没有在进行备份那么立即进行备份，否则等待备份完毕之后再执行备份。2. Redis conf配置了auto-aof-rewrite-percentage和auto-aof-rewrite-min-size参数，并且当前AOF文件大小server.aof_current_size大于auto-aof-rewrite-min-size，且AOF文件大小的增长率大于auto-aof-rewrite-percentage时触发备份。3. 使用config set appendonly yes命令时，调用startAppendOnly()函数触发备份。 六、那究竟使用哪种方式？12RDB 会有丢失数据风险，备份文件体积小，数据备份/恢复速度快。AOF 数据完整性更加安全，但是频繁备份需要过多的io操作性能会受到影响，备份文件体积较大，数据恢复速度慢。 首先，我们明白AOF, RDB的优缺点之后可以概括出: AOF适合热备，RDB适合冷备，Redis4.0以后允许使用aof-use-rdb-preamble配置项打开RDB-AOF混合持久化。 七、RDB/AOF混合使用的之后备份文件的变化当RDB和AOF同时开启之后121. Redis默认会优先加载AOF的配置文件。2. AOF备份文件的内容格式发生改变，备份文件前半段是RDB格式的全量数据后半段是Redis命令格式的增量数据。 八、AOF文件内容格式AOF文件内容格式是Redis通讯协议RESP（REdis Serialization Protocol）格式的命令文本存储，在此不展开学习后续会专门学习Redis的RESP协议。 最后附上Redis对于持久化方式说明的文档 https://redis.io/topics/persistence","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"使用Guetzli压缩图片","date":"2017-06-12T03:00:00.000Z","path":"2017/06/12/guetzli/","text":"Guetzli is a JPEG encoder that aims for excellent compression density at high visual quality. Guetzli-generated images are typically 20-30% smaller than images of equivalent quality generated by libjpeg. Guetzli generates only sequential (nonprogressive) JPEGs due to faster decompression speeds they offer. 1.安装libpng依赖库1$ yum install libpng-devel git 2.获取Guetzli项目 https://github.com/google/guetzli 123456789101112131415$ cd /tmp$ git clone https://github.com/google/guetzli$ cd guetzli$ make$ ./bin/Release/guetzli // 这里我就不加软连接,为了方便使用你们可以加入软连接Guetzli JPEG compressor. Usage: guetzli [flags] input_filename output_filename // 这里是使用方法Flags: --verbose - Print a verbose trace of all attempts to standard output. --quality Q - Visual quality to aim for, expressed as a JPEG quality value. Default value is 95. --memlimit M - Memory limit in MB. Guetzli will fail if unable to stay under the limit. Default limit is 6000 MB. --nomemlimit - Do not limit memory usage. 3.压缩前后对比压缩前(920KB) 压缩后(748KB),20%","tags":[{"name":"guetzli","slug":"guetzli","permalink":"http://xupin.im/tags/guetzli/"},{"name":"tools","slug":"tools","permalink":"http://xupin.im/tags/tools/"}]},{"title":"部署像Trello一样的看板工具 - Wekan","date":"2017-06-09T04:00:00.000Z","path":"2017/06/09/wekan/","text":"Wekan is an completely Open Source and Free software collaborative kanban board application with MIT license. 1.安装MongoDB123456789$ vi /etc/yum.repos.d/mongodb-org-3.2.repo $ [mongodb-org-3.2]$ name=MongoDB Repository$ baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/3.2/x86_64/$ gpgcheck=1$ enabled=1$ gpgkey=https://www.mongodb.org/static/pgp/server-3.2.asc$ yum install mongodb-org$ service mongod start // 启动MongoDB 2.安装NodeJS http://nodejs.org/dist/latest/ // pkg列表 我下载的是 http://nodejs.org/dist/latest/node-v8.1.0.tar.xz 1234567891011$ yum install gcc gcc-c++ wget$ cd /tmp$ wget http://nodejs.org/dist/latest/node-v8.1.0.tar.xz$ xz -d node-v8.1.0.tar.xz$ tar -vxf node-v8.1.0.tar$ cd node-v8.1.0$ vi README.md // To build 有告诉需要的环境,以及安装步骤$ ./configure$ make &amp;&amp; make install$ node -v$ v8.1.0 3.获取Wekan项目 https://github.com/wekan/wekan/releases/ // 版本列表 我下载的是 v0.23 1234567891011$ cd /tmp$ wget https://github.com/wekan/wekan/releases/download/v0.23/wekan-0.23.tar.gz$ tar -zxvf wekan wekan-0.23.tar.gz$ mkdir wekan$ mv ./bundle ./wekan $ cd wekan/bundle$ export MONGO_URL='mongodb://127.0.0.1:27017/wekan' $ export ROOT_URL='https://example.com' $ export MAIL_URL='smtp://user:pass@mailserver.example.com:25/' $ export PORT=8080$ node main.js // 如果没有报错,访问http://host:8080就可以使用Wekan啦 4.一些错误问题 ##There is an issue with node-fibers## npm install fibers","tags":[{"name":"tools","slug":"tools","permalink":"http://xupin.im/tags/tools/"},{"name":"wekan","slug":"wekan","permalink":"http://xupin.im/tags/wekan/"}]},{"title":"PHP 图片转换Webp格式","date":"2017-03-21T16:00:00.000Z","path":"2017/03/22/php-webp/","text":"WebP是Google推出的一种图片格式，它基于VP8编码，可对图像大幅压缩。与JPEG相同，WebP也是一种有损压缩，但在画质相同的情况下，WebP格式比JPEG图像小40%。 1.利与弊 WebP 与 JPEG 图像相比，这种格式可对图像大幅压缩。从而显著优化页面加载时间和带宽使用情况。WebP 目前支持桌面上的 Chrome 和 Opera 浏览器。手机支持仅限于原生的 Android 浏览器和 Android 系统上的 Chrome 浏览器,后面会介绍关于如何处理这个限制的方法。 （Google 已和正在部署的 WebP 的产品） 2.后端PHP代码如何转换？12345678910&lt;?php// 文件名$fileName = './res.png';// 图片源$res = imagecreatefrompng($fileName); // 如果是jpg(imagecreatefromjpg),其他格式自行查阅手册// 输出.webp文件imagewebp($res, './res.webp'); (非常简单!) 3.前端JS代码如何判断浏览器支持?1234567891011121314151617181920212223242526272829// 代码片段摘自网站,亲测可用function checkWebp(callback) &#123; var img = new Image(); img.onload = function () &#123; var result = (img.width &gt; 0) &amp;&amp; (img.height &gt; 0); callback(result); &#125;; img.onerror = function () &#123; callback(false); &#125;; img.src = 'data:image/webp;base64,UklGRiIAAABXRUJQVlA4IBYAAAAwAQCdASoBAAEADsD+JaQAA3AAAAAA';&#125;function showImage(useWebp)&#123; var imgs = Array.from(document.querySelectorAll('img')); imgs.forEach(function(i)&#123; var src = i.attributes['data-src'].value; if (useWebp)&#123; src = src.replace(/\\.jpg$/, '.webp'); &#125; i.src = src; &#125;);&#125;checkWebp(showImage); 4.实例前后对比转换前(378KB) 转换后(8KB)","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"webp","slug":"webp","permalink":"http://xupin.im/tags/webp/"}]},{"title":"PHP + Google(FCM)安卓设备消息推送/群发","date":"2017-02-28T09:10:00.000Z","path":"2017/02/28/google-msg-push/","text":"Firebase 是一个移动平台，可以帮助您快速开发高品质应用，扩大用户群，并赚取更多收益。Firebase 由多种互补功能组成，您可以自行组合和匹配这些功能以满足自己的需求。 1.获取Google推送的KEY(Google应用后台) AIzaSyCm5Vufc1FAtWE1kupGhVDRb0ThkCoWC7d // 这个是我的 2.PHP推送代码12345678910111213141516171819202122232425262728293031// 初始化环境$url = 'https://fcm.googleapis.com/fcm/send';$ch = curl_init($url);curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);curl_setopt($ch, CURLOPT_RETURNTRANSFER,1);curl_setopt($ch, CURLOPT_POST, 1);$header = array( 'Content-Type: application/json', 'Authorization: key=AIzaSyCm5Vufc1FAtWE1kupGhVDRb0ThkCoWC7d' // 这里是刚才的KEY);curl_setopt($ch, CURLOPT_HTTPHEADER, $header);// 构建消息体,json编码$content = json_encode(['notification'=&gt;[ 'title' =&gt; '这里是标题', 'body' =&gt; '这里是内容', 'icon' =&gt; '这里是后台设置的iconName'],'to'=&gt;'设备标识(Google SDK可以获取)','registration_ids'=&gt;['设备标识','设备标识'] // 如果要推送多台设备,可以使用此字段,最多支持1000台设备]);curl_setopt($ch, CURLOPT_POSTFIELDS, $content);$ret = json_decode(curl_exec($ch), true);$ret['success']; // 推送成功数$ret['failure']; // 推送失败数 3.返回值示例1234567891011121314151617&#123; \"multicast_id\": 7669649331143639654, \"success\": 3, \"failure\": 0, \"canonical_ids\": 0, \"results\": [ &#123; \"message_id\": \"0:1488360060090224%6490a2d16490a2d1\" &#125;, &#123; \"message_id\": \"0:1488360060095871%6490a2d16490a2d1\" &#125;, &#123; \"message_id\": \"0:1488360060090765%6490a2d16490a2d1\" &#125; ]&#125; 4.Google(FCM) 文档 https://firebase.google.com/docs","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"google fcm","slug":"google-fcm","permalink":"http://xupin.im/tags/google-fcm/"}]},{"title":"通用缩写表","date":"2017-02-27T16:00:00.000Z","path":"2017/02/28/short-names/","text":"1、本缩写表是《编码命名规范》的附录。2、本缩写表中列出的都是通用性缩写，不提供标准缩写，如：Win9x、COM 等。3、使用本缩写表里的缩写时，请对其进行必要的注释说明。4、除少数情况以外，大部分缩写与大小写无关。 缩写 全称 addr Address adm Administrator app Application arg Argument asm assemble asyn asynchronization avg average DB Database bk back bmp Bitmap btn Button buf Buffer calc Calculate char Character chg Change clk Click clr color cmd Command cmp Compare col Column coord coordinates cpy copy ctl /ctrl Control cur Current cyl Cylinder dbg Debug dbl Double dec Decrease def default del Delete dest /dst Destination dev Device dict dictionary diff different dir directory disp Display div Divide 缩写 全称 dlg Dialog doc Document drv Driver dyna Dynamic env Environment err error ex/ext Extend exec execute flg flag frm Frame func /fn Function grp group horz Horizontal idx /ndx Index img Image impl Implement inc Increase info Information init Initial/Initialize/Initialization ins Insert inst Instance INT /intr Interrupt len Length lib Library lnk Link log logical lst List max maximum mem Memory mgr /man Manage/Manager mid middle min minimum msg Message mul Multiply num Number obj Object ofs Offset org Origin/Original param Parameter pic picture pkg package pnt /pt Point pos Position pre /prev previous 缩写 全称 prg program prn Print proc Process/Procedure prop Properties psw Password ptr Pointer pub Public rc rect ref Reference reg Register req request res Resource ret return rgn region scr screen sec Second seg Segment sel Select src Source std Standard stg Storage stm Stream str String sub Subtract sum summation svr Server sync Synchronization sys System tbl Table temp /tmp Temporary tran /trans translate/transation/transparent tst Test txt text unk Unknown upd Update upg Upgrade util Utility var Variable ver Version vert Vertical vir Virus wnd Window","tags":[{"name":"short names","slug":"short-names","permalink":"http://xupin.im/tags/short-names/"}]},{"title":"Nginx recv() failed (104 Connection reset by peer)","date":"2017-02-23T10:05:00.000Z","path":"2017/02/23/nginx-recv-104/","text":"Error: recv() failed (104: Connection reset by peer) while reading response header from upstream 1.日志信息123456789// www.xxxx.com 公司域名,故此屏蔽2017/02/18 12:03:58 [error] 7519#7519: *33 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:03:59 [error] 7519#7519: *35 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:04:00 [error] 7519#7519: *37 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:04:02 [error] 7519#7519: *39 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:04:07 [error] 7519#7519: *9 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"GET /Api/GetRegSvrInfo&amp;pid=1&amp;ver=3003 HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201:81\"2017/02/18 12:04:08 [error] 7519#7519: *42 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:04:09 [error] 7519#7519: *44 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\"2017/02/18 12:04:10 [error] 7519#7519: *46 recv() failed (104: Connection reset by peer) while reading response header from upstream, client: 192.168.1.207, server: www.xxxx.com, request: \"POST /Api/GetRegSvrInfo HTTP/1.1\", upstream: \"fastcgi://127.0.0.1:9000\", host: \"192.168.1.201\" 2.异常原因 ups请求9000端口之后重置 // 我遇见的问题 3.解决方案1$ iptables -L -n // 检查是否有其他进程占用9000端口 9000端口没有被占用 12$ vi /etc/sysconfig/iptables-A INPUT -i lo -j ACCEPT // 本地回环(lo一般指127.0.0.1),检查是否有这条配置 lo防火墙策略未配置 123$ iptables -A INPUT -i lo -j ACCEPT // 没有配置,执行这个指令$ iptables save // 保存$ iptables restart // 重启生效 4.依然未解决12$ vi /etc/php-fpm.d/www.conf // 这是我的PHP-FPM配置文件request_terminate_timeout = 0/600; // 这样解决一般是PHP某些脚本执行时间过长超时 顽固分子,就是不行!!! 1$ iptables stop // 关闭防火墙之后(慎用,生产环境谨慎操作),试试是否可以访问 如果可以访问,麻烦运维哥哥帮忙看一下防火墙策略o(╯□╰)o!!! 5.其他问题欢迎邮箱我一起讨论解决","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"nginx","slug":"nginx","permalink":"http://xupin.im/tags/nginx/"}]},{"title":"PHP防止XSS攻击、脚本访问Cookie","date":"2017-02-23T08:30:00.000Z","path":"2017/02/23/cookie-httponly/","text":"httponly是微软对cookie做的扩展,主要是解决用户的cookie可能被盗用的问题。 1.Cookie漏洞1234567891011121314151617181920212223242526272829303132333435363738Missing `httpOnly` Cookie Attribute 105925 http (80/tcp) High Vulnerability Detection Result:The cookies:Set-Cookie: PHPSESSID=esjpr7dkb64rgn1kjhr1r45j05; path=/ are missing the httpOnly attribute.Summary:The application is missing the 'httpOnly' cookie attributeSolution:Set the 'httpOnly' attribute for any session cookie.Vulnerability Detection Method:Check all cookies sent by the application for a missing 'httpOnly' attributeInsight:The flaw is due to a cookie is not using the 'httpOnly' attribute. Thisallows a cookie to be accessed by JavaScript which could lead to session hijacking attacks.CVSS Base Vector:AV:N/AC:L/Au:N/C:P/I:N/A:NAffected Software/OS:Application with session handling in cookies.References:https://www.owasp.org/index.php/HttpOnlyhttps://www.owasp.org/index.php/Testing_for_cookies_attributes_(OTG-SESS-002) 2.漏洞表现 跨站点脚本攻击 // 这种攻击手段具体表现,很多博主都写过,在此不表了 3.解决方案1$ vi /etc/php.ini // PHP 5.2版本及以上 session.cookie_httponly = true/1 1234567// PHP 5.2版本及以上ini_set('session.cookie_httponly', 1); // 注册php.ini配置session_set_cookie_params(0, NULL, NULL, NULL, TRUE);setcookie('cookieName', 'cookieValue', NULL, NULL, NULL, NULL, TRUE);setrawcookie('cookieName', 'cookieValue', NULL, NULL, NULL, NULL, TRUE);// PHP 5.1版本及以下header('Set-Cookie: hidden=value; httpOnly'); 4.网站安全扫描工具 Nikto、WebScarab等","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"xss","slug":"xss","permalink":"http://xupin.im/tags/xss/"}]},{"title":"Nginx负载均衡配置","date":"2017-02-23T03:00:00.000Z","path":"2017/02/23/nginx-ups/","text":"nginx [engine x] is an HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server, originally written by Igor Sysoev. 1.服务器列表 // 我们有如下两台服务器,都部署了Nginx 192.168.1.1 // host指向这台服务器 192.168.1.2 2.Nginx配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 打开192.168.1.1这台服务器的nginx.conf,编辑后保存http &#123; . . . // 无关紧要的部分忽略掉 upstream lee&#123; // 这里的ups name自定义,我这里使用lee // 这里除了80端口以外,其他端口都可以使用 server 127.0.0.1:8080; server 192.168.1.2:8080; &#125; server&#123; listen 80; // ups监听80 server_name www.xxx.com xxx.comn; // 这个可选,一般是你指向这台服务器的Host location / &#123; proxy_pass http://lee; // 这里是上面自定的ups name proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125; server &#123; listen 8080; server_name 127.0.0.1; root /var/www/html; // 这里是你的Nginx根目录 index index.php index.html index.htm; location /status &#123; stub_status on; &#125; location ~ \\.php &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include /etc/nginx/fastcgi_params; &#125; location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|js|css)$ &#123; expires 30d; &#125; &#125;&#125;// 打开192.168.1.2这台服务器的nginx.conf,编辑后保存http &#123; . . . // 无关紧要的部分忽略掉 server &#123; listen 8080; server_name 192.168.1.2; root /var/www/html; // 这里是你的Nginx根目录 index index.php index.html index.htm; location /status &#123; stub_status on; &#125; location ~ \\.php &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include /etc/nginx/fastcgi_params; &#125; location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|js|css)$ &#123; expires 30d; &#125; &#125;&#125; 3.重新加载Nginx配置1234$ nginx -tc /etc/nginx/nginx.conf // 你nginx.conf所在的目录,检查配置是否存在问题nginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful$ nginx -s reload // 不需要停服,直接重载配置 4.Hi 007,任务完成12// 编辑个脚本测试一下,echo 'Hi 007';","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"nginx","slug":"nginx","permalink":"http://xupin.im/tags/nginx/"}]},{"title":"PHP + Apple苹果设备消息推送(APNS)","date":"2017-02-22T09:30:00.000Z","path":"2017/02/22/apple-msg-push/","text":"1.合成证书(需要两个文件.cer,.p12,苹果开发者后台获取) ios.cer,ios.p12 // 我这里的文件名 123456$ openssl x509 -in ios.cer -inform der -out ios_cer.pem$ openssl pkcs12 -nocerts -out ios_p12.pem -in ios.p12Enter Import Password: // 这里输入Apple后台文件导出时的密码MAC verified OKEnter PEM pass phrase: // 这里输入新的pem密码(用于代码中),我输入的是123456789$ cat ios_cer.pem ios_p12.pem &gt; key.pem 2.PHP推送代码12345678910111213141516171819202122232425// 初始化环境$ctx = stream_context_create();stream_context_set_option($ctx, 'ssl', 'local_cert', 'assets/key/key.pem'); // key文件的路径stream_context_set_option($ctx, 'ssl', 'passphrase', '123456789'); // 生成pem输入的密码// 建立APNS连接$fp = stream_socket_client( 'ssl://gateway.push.apple.com:2195', $err, $errstr, 60, STREAM_CLIENT_CONNECT|STREAM_CLIENT_PERSISTENT, $ctx);// 构建消息体,json编码$payload = json_encode( array( 'aps'=&gt;array('alert' =&gt; '这里是内容', 'sound' =&gt; '这里是提示声音')));// 转换二进制$msg = chr(0) . pack('n', 32) . pack('H*', '设备标识(Apple SDK可以获取)') . pack('n', strlen($payload)) . $payload;// 发送消息$result = fwrite($fp, $msg, strlen($msg));if(!$result)&#123; // 推送失败&#125;else&#123; // 推送成功&#125;fclose($fp); 3.APNS文档 https://developer.apple.com/notifications/","tags":[{"name":"apns","slug":"apns","permalink":"http://xupin.im/tags/apns/"},{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"}]},{"title":"Nginx + PHP7 + Mysql(MariaDB) + Memcached + Redis的环境搭建(可选)","date":"2017-02-22T07:30:00.000Z","path":"2017/02/22/php7/","text":"PHP is a popular general-purpose scripting language that is especially suited to web development. Fast, flexible and pragmatic, PHP powers everything from your blog to the most popular websites in the world. 1.更新yum源,安装扩展包123$ yum update$ yum install epel-release$ yum install git 2.添加Nginx源1$ vi /etc/yum.repos.d/nginx.repo [nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1 3.安装开发库(可选)1$ yum install ntp make openssl openssl-devel pcre pcre-devel libpng libpng-devel libjpeg-6b libjpeg-devel-6b freetype freetype-devel gd gd-devel zlib zlib-devel gcc gcc-c++ libXpm libXpm-devel ncurses ncurses-devel libmcrypt libmcrypt-devel libxml2 libxml2-devel imake autoconf automake screen sysstat compat-libstdc++-33 curl curl-devel 4.安装Nginx123$ yum install nginx$ chkconfig --levels 235 nginx on // 设2、3、5级别开机启动$ service nginx start 5.安装Mysql(MariaDB)123$ yum install mariadb mariadb-server mariadb-devel$ chkconfig --levels 235 mariadb on$ service mariadb start 6.安装PHP-FPM1234$ rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm$ yum install php70w lighttpd-fastcgi php70w-cli php70w-gd php70w-imap php70w-ldap php70w-odbc php70w-pear php70w-xml php70w-xmlrpc php70w-mbstring php70w-mcrypt php70w-mssql php70w-snmp php70w-soap php70w-tidy php70w-common php70w-devel php70w-fpm php70w-mysql // 安装php和所需组件使PHP支持MySQL、FastCGI模式$ chkconfig --levels 235 php-fpm on$ service php-fpm start 7.安装Memcached(可选)123456789101112131415$ yum install –enablerepo=rpmforge memcached libevent libevent-devel$ wget https://launchpad.net/libmemcached/1.0/1.0.18/+download/libmemcached-1.0.18.tar.gz$ tar -zxf libmemcached-1.0.18.tar.gz$ cd libmemcached-1.0.18/$ ./configure$ make &amp;&amp; make install$ git clone https://github.com/php-memcached-dev/php-memcached.git$ cd php-memcached/$ git checkout php7$ phpize$ ./configure --disable-memcached-sasl$ make &amp;&amp; make install$ chkconfig --level 2345 memcached on$ service memcached start$ vim /etc/php.ini // 末行增加 [Memcached]extension=memcached.so 8.安装Redis(可选)12345678910$ yum install redis$ git clone https://github.com/phpredis/phpredis.git$ cd phpredis$ git checkout php7$ phpize$ ./configure$ make &amp;&amp; make install$ chkconfig --level 2345 redis on$ service redis start$ vim /etc/php.ini // 末行增加 [Redis]extension=redis.so 9.重启PHP-FPM1$ service php-fpm restart 完成","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"php","slug":"php","permalink":"http://xupin.im/tags/php/"}]}]