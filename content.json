[{"title":"数据结构与算法 - AST","date":"2022-12-12T16:00:00.000Z","path":"2022/12/13/data-structures-algorithms-ast/","text":"AST（Abstract syntax tree）抽象语法树，用于表示编程语言源代码的一种抽象语法的结构，树的每个节点都对应源代码中的一个结构。 前言计算机如何生成AST结构？一般来说顺序如下： 词法分析 扫描源代码，生成标记（token） 语法分析 解析tokens，构造AST结构 分析过程： AST结构： 应用场景AST应用场景比较广泛，比如： Typescript编译Javascript文件 格式化插件 代码混淆/压缩 类型检查/推导 等等…诸多应用场景。 词法分析如何读入源代码，扫描、解析以及生成AST？下面尝试用一个可以计算+、-、*、/公式的小程序来学习和实验。 假设，给定的源码是1+2+3，应该怎样去扫描？扫描原理很简单，通俗的说就是按字符读取记录为Token。 123456789101112131415// 首先定义存储标记的结构type Token struct &#123; Str string&#125;// 扫描源码s := \"1+2+3\"tokens := []Token&#123;&#125;for _, c := range s &#123; tokens = append(tokens, Token&#123; Str: string(c), &#125;)&#125;fmt.Printf(\"%+v\\n\", tokens)// [&#123;Str:1&#125; &#123;Str:+&#125; &#123;Str:2&#125; &#123;Str:+&#125; &#123;Str:3&#125;] 以上，扫描工作就完成了！ 但词法分析一般没有这么简单，比如数字1和操作符+也不应该是同一种类型。所以我们需要改造词法分析器！在生成Token的同时记录类型，方便进一步处理。这里我们使用状态机的设计思路： 读入一个字符 判断字符类型，数字 or 操作符&nbsp;&nbsp;&nbsp;&nbsp;1. 是数字，记录当前字符位置，继续读取下一个字符，直至非数字或者字符边界，记录完整的数值。&nbsp;&nbsp;&nbsp;&nbsp;2. 不是数字，直接记录操作符。 返回token 详细扫描逻辑，如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103type Lexer struct &#123; Formula string Char byte Pos int&#125;type Token struct &#123; Str string Type int Start int End int&#125;func (r *Lexer) Lex() ([]*Token, error) &#123; tokens := make([]*Token, 0) r.Char = r.Formula[0] for r.Pos &lt; len(r.Formula) &#123; token := r.Scan() if token.Type == enums.ILLEGAL &#123; return []*Token&#123;&#125;, fmt.Errorf(\"'%s' is not supported\", token.Str) &#125; tokens = append(tokens, token) &#125; return tokens, nil&#125;func (r *Lexer) Scan() *Token &#123; var token *Token pos := r.Pos switch r.Char &#123; case '0', '1', '2', '3', '4', '5', '6', '7', '8', '9': for r.IsDigit() &#123; if !r.NextChar() &#123; break &#125; &#125; token = &amp;Token&#123; Str: string(r.Formula[pos:r.Pos]), Type: enums.NUMBER, Start: pos, End: r.Pos, &#125; case '+': token = &amp;Token&#123; Str: string(r.Char), Type: enums.ADD, Start: pos, End: pos, &#125; r.NextChar() case '-': token = &amp;Token&#123; Str: string(r.Char), Type: enums.SUB, Start: pos, End: pos, &#125; r.NextChar() case '*': token = &amp;Token&#123; Str: string(r.Char), Type: enums.MUL, Start: pos, End: pos, &#125; r.NextChar() if r.Char == '*' &#123; token = &amp;Token&#123; Str: \"**\", Type: enums.XOR, Start: pos, End: pos, &#125; r.NextChar() &#125; case '/': token = &amp;Token&#123; Str: string(r.Char), Type: enums.QUO, Start: pos, End: pos, &#125; r.NextChar() default: token = &amp;Token&#123; Str: string(r.Char), Type: enums.ILLEGAL, Start: pos, End: r.Pos, &#125; &#125; return token&#125; 现在，我们调用词法分析器解析1+2+312345678910111213141516lexer := lexer.Lexer&#123; Formula: \"1+2+3\",&#125;tokens, err := lexer.Lex()if err != nil &#123; fmt.Println(err) return&#125;for _, token := range tokens &#123; fmt.Printf(\"%+v \\n\", token)&#125;// &amp;&#123;Str:1 Type:9 Start:0 End:1&#125;// &amp;&#123;Str:+ Type:10 Start:1 End:1&#125;// &amp;&#123;Str:2 Type:9 Start:2 End:3&#125;// &amp;&#123;Str:+ Type:10 Start:3 End:3&#125;// &amp;&#123;Str:3 Type:9 Start:4 End:5&#125; 语法分析生成tokens是第一步，这一串token又要怎样去解析呢？ 已知+-*/均属于二元运算，二元运算三元素：运算符、左变量、右变量。所以我们需要的结构大概是这样：Number{1} Op{+} Number{2} Op{+} Number{3}，用二叉树来表示： 12345678910111213141516171819// 首先定义一个语法分析器type Parser struct &#123; Tokens []*lexer.Token // tokens CurToken *lexer.Token // 当前token Index int // 下标 Err error // error&#125;// 数值结构type Number struct &#123; Val int&#125;// 二元运算结构type Stmt struct &#123; Type int Left Node Right Node&#125; 处理逻辑如下： 顺序取出一个token。 ParseExpr()判定token类型，返回对应结构，下标+1（处理下一个token，应当是运算符token）。&nbsp;&nbsp;&nbsp;&nbsp;1. 如果是数值类型，则直接返回Number{}结构。&nbsp;&nbsp;&nbsp;&nbsp;2. 如果是运算符类型，则递归处理返回Stmt{}结构。 ParseRight()处理右侧变量，把左侧变量传入函数。 根据运算符判断变量优先级问题，如果当前运算符小于传入变量优先级，则直接返回传入变量，处理结束。 记录当前运算符（类型），下标+1，处理当前操作符右侧变量（步骤2）。 再次判断传入变量优先级是否低于当前运算符。&nbsp;&nbsp;&nbsp;&nbsp;1. 是，则把右侧变量（步骤4）当成左侧变量传入递归处理。&nbsp;&nbsp;&nbsp;&nbsp;2. 不是，则构造二元运算表达式Stmt{Type: 运算符类型, Left: 左侧变量, Right: 右侧变量}。 当前循环结束，回到步骤4。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990func (r *Parser) Parse() (Node, error) &#123; if len(r.Tokens) == 0 &#123; return nil, errors.New(\"the token list is empty\") &#125; if r.CurToken == nil &#123; r.CurToken = r.Tokens[0] &#125; return r.Compile(), r.Err&#125;func (r *Parser) Compile() Node &#123; left := r.ParseExpr() right := r.ParseRight(1, left) return right&#125;func (r *Parser) ParseExpr() Node &#123; switch r.CurToken.Type &#123; case enums.NUMBER: return r.ParseNumber() case enums.ADD: return r.ParseNumber() case enums.SUB: if t := r.NextToken(); t.Type == enums.EOF &#123; r.Err = errors.New(\"expects to be number, eof given\") return nil &#125; return &amp;Stmt&#123; Type: enums.SUB, Left: &amp;Number&#123;&#125;, Right: r.ParseExpr(), &#125; case enums.MUL: return r.ParseNumber() case enums.QUO: return r.ParseNumber() default: r.Err = fmt.Errorf(\"expects to be number, '%s' given\", r.CurToken.Str) return nil &#125;&#125;func (r *Parser) ParseRight(precedence int, left Node) Node &#123; for &#123; curPrec := r.Precedence() if curPrec &lt; precedence &#123; return left &#125; tokenType := r.CurToken.Type r.NextToken() right := r.ParseExpr() if right == nil &#123; return nil &#125; if curPrec &lt; r.Precedence() &#123; right = r.ParseRight(curPrec, right) if right == nil &#123; return nil &#125; &#125; left = &amp;Stmt&#123; Type: tokenType, Left: left, Right: right, &#125; &#125;&#125;func (r *Parser) ParseNumber() *Number &#123; f, err := strconv.ParseFloat(r.CurToken.Str, 64) if err != nil &#123; return &amp;Number&#123;&#125; &#125; node := &amp;Number&#123; Val: f, &#125; r.NextToken() return node&#125;func (r *Parser) Precedence() int &#123; switch r.CurToken.Type &#123; case enums.ADD, enums.SUB: return 1 case enums.MUL, enums.QUO: return 2 default: return 0 &#125;&#125; 看看效果：123456789101112131415161718lexer := lexer.Lexer&#123; Formula: \"1+2-3*4\",&#125;tokens, err := lexer.Lex()if err != nil &#123; fmt.Println(err) return&#125;p := &amp;parser.Parser&#123; Tokens: tokens,&#125;ast, err := p.Parse()if err != nil &#123; fmt.Println(err) return&#125;fmt.Printf(\"%+v \\n\", ast)// &#123;Type: 10, Left: &#123;Type: 10, Left: &#123;Type: 9, Val: 1&#125;, Right: &#123;Type: 9, Val: 2&#125;&#125;, Right: &#123;Type: 9, Val: 3&#125;&#125; 最后有了AST结构，我们就可以开始进行计算啦～这样一个支持+-*/的小程序就完成了！计算逻辑相对比较简单，这里直接贴代码:1234567891011121314151617181920212223242526// 数值类型func (r *Number) Evaluate() float64 &#123; return r.Val&#125;// 二元运算类型func (r *Stmt) Evaluate() float64 &#123; left := r.Left.Evaluate() right := r.Right.Evaluate() switch r.Type &#123; case enums.ADD: return left + right case enums.SUB: return left - right case enums.MUL: return left * right case enums.QUO: if right == 0 &#123; fmt.Printf(\"expr[%g/%g]exception, division by zero \\n\", left, right) return 0 &#125; return left / right default: return 0 &#125;&#125; 附上完整代码：math/main.go","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - Tire Tree","date":"2022-12-10T16:00:00.000Z","path":"2022/12/11/data-structures-algorithms-tire/","text":"Tire树又称单词查找树、前缀树，是一种哈希树变种的树形结构。核心思想是空间换时间，利用字符串的公共前缀来提高查询效率，常常被应用字符统计、检索等场景，比如搜索引擎的词频统计和提示等 原理前缀树的存储原理很简单，即合并字符串相同的字符前缀进行存储。 比如，存储dog、cat、doing三个字符串的树结构： 需要注意：前缀树不是二叉树，而是多叉树。 前缀树的特征： 根结点不存储任何内容。 每个节点只存储1个字符。 根节点到任意子节点是为存储的字符串。 写入和查询操作的时间复杂度均为O(N)。 如何实现一般可以通过以下两种方式来实现， 单向链表（本文采用方式） 二维数组 存储 12345678910111213// 定义根结点type Tire struct &#123; Root *Node Size int // 总共存储多少个字符&#125;// 定义存储字符的节点type Node struct &#123; Char string // 当前节点存储的字符 Count int // 出现过多少次 IsWord bool // 是否是完整的单词（字符串结尾） Next map[rune]*Node // 存储下一个字符的节点&#125; 从结构可以看出，其实前缀树就是把字符串按字符顺序存储。 1234567891011121314151617181920212223func (r *Tire) Append(s string) &#123; if r.Find(s, true) &#123; return &#125; node := r.Root for _, c := range s &#123; v, ok := node.Next[c] // 是否存在该字符，如果不存在则进行存储，反之取下一个字符 if !ok &#123; v = &amp;Node&#123; Char: string(c), IsWord: false, Count: 1, Next: make(map[rune]*Node, 0), &#125; node.Next[c] = v r.Size += 1 &#125; else &#123; v.Count += 1 &#125; node = v &#125; node.IsWord = true // 最后一个字符把IsWord标记为true&#125; 查找 1234567891011func (r *Tire) Find(s string, isFullMatch bool) bool &#123; node := r.Root for _, c := range s &#123; v, ok := node.Next[c] // 是否存在该字符，如果不存在则直接返回false，反之继续查找下一个字符 if !ok &#123; return false &#125; node = v &#125; return (!isFullMatch &amp;&amp; !node.IsWord) || node.IsWord&#125; 删除 删除是前缀树中稍复杂一点的操作，如果字符被其他字符串使用，不能直接删除。 12345678910111213141516171819202122func (r *Tire) Remove(s string) bool &#123; if !r.Find(s, true) &#123; return false &#125; node := r.Root prev := r.Root for i, c := range s &#123; node = node.Next[c] node.Count -= 1 if node.Count == 0 &#123; r.Size -= (len(s) - i) break &#125; prev = node &#125; if node.Count == 0 &#123; delete(prev.Next, rune(node.Char[0])) &#125; else &#123; node.IsWord = false &#125; return true&#125; 最后附上完整代码：tire/main.go","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - AOI","date":"2022-10-07T16:00:00.000Z","path":"2022/10/08/data-structures-algorithms-aoi/","text":"AOI（Area Of Interest）翻译过来称为“感兴趣的区域”，用于计算玩家与玩家（或其他Entity）之间彼此进入、离开、移动视野的算法。通俗的解释比如，玩家离开某个地图时，计算出需要通知的其他玩家。 AOI模块是多人联机游戏服务器中很重要的功能模块之一，AOI模块的“好”与“坏”会很大程度上影响服务器的运行效率。 思路当一个玩家进入场景后，首先要计算出场景中的其他所有玩家并放进对象集合（又称观察者集合），之后该玩家的进入、移动、离开或其他AOI行为都将会一一通知该集合内的玩家，并且每个玩家都需要维护这样一个对象集合。 Watchers 观察者集合，即能够看到我的其他玩家或实体。 需要注意的是，该对象集合内的玩家列表会随着玩家的AOI行为发生变化，在玩家离开场景后清空对象集合。 全场景同步即每个玩家都能看到进入该场景的所有玩家，那么所有进入该场景的玩家发生AOI行为，AOI场景管理器都会通知其他玩家。比如，我从地图0,0移动至地图10,10这样的行为。 进入 123456玩家[pp]进入地图 0,0 玩家[wl]进入地图 2,20 通知玩家[pp]进入视野玩家[sd]进入地图 0,4 通知玩家[pp]进入视野 通知玩家[wl]进入视野 移动 123玩家[sd]进行移动 0,4 -&gt; 0,5 通知玩家[pp]进行移动 通知玩家[wl]进行移动 离开 123玩家[sd]离开地图 通知玩家[pp]离开地图 通知玩家[wl]离开地图 但如果地图场景大、玩家数量多的场景下问题就凸显出来，服务器通信量将会十分巨大（玩家与玩家之间）。比如，在一款叫《绝地求生》的游戏中最大的地图是8km*8km，假如两个玩家相距6km，这两个玩家彼此之间还需要通信吗？ 限制玩家视野为了提高服务器运行效率，所以需要对玩家的可视范围（视野）进行一定限制，从而减少通信量。即每个玩家都只能看到视野内的其他玩家，当发生行为时也只会同步信息给这些玩家。 比如，当每个玩家都只有5视野单位时： 进入 1234玩家[pp]进入地图 0,0 玩家[wl]进入地图 2,20玩家[sd]进入地图 0,4 通知玩家[pp]进入视野 移动 123玩家[sd]进行移动 0,4 -&gt; 1,20 通知玩家[pp]离开视野 通知玩家[wl]进入视野 离开 12玩家[sd]离开地图 通知玩家[wl]离开地图 对玩家进行了视野限制后，消息量会大幅下降。但因为会对场景内所有玩家进行暴力查找，所以检索效率并不高。 网格化把玩家视野也进行网格化，类似的优化算法有：九宫格、灯塔、十字链表、六边形等 九宫格 把地图网格化后，限制玩家的视野范围为9个格子。如图： 一般设计是玩家的手机屏幕显示为4/9格子，但真实视野为1 ～ 9网格，所以服务器只需要同步消息给玩家可视范围内的9个网格其他玩家。 灯塔 灯塔是在九宫格的基础上进行优化，在把地图网格化的基础上划分区域（比如4个网格组成一个区域），每个区域有一个管理者：灯塔，这个管理者知晓当前区域的全部玩家。如图： 每个灯塔维护两个对象集合：Watchers（观察者集合）、Markers（被观察者集合）。 进入 根据玩家坐标计算出灯塔，把玩家加入到灯塔的Markers。 通知该灯塔所有Watchers，有玩家进入场景。 找出该玩家视野内所有灯塔，把玩家加入到灯塔的Watchers，同时把这些Watchers和玩家互相加进对方的视野列表。 移动 灯塔无变化，仅通知移动行为给玩家视野列表内的玩家。 灯塔有变化，对灯塔列表差集进行交集、并集运算。 并集，从离开灯塔的Watchers、Markers移除该玩家，新进入灯塔的Watchers、Markers添加该玩家。 交集，仅通知移动行为。 对移动前后灯塔的Watchers进行交集、并集运算。 并集，从离开的Watchers互相从对方的视野列表内移除，新进入的Watchers互相加进对方的视野列表。 离开 根据玩家坐标计算出灯塔，把玩家从该灯塔的Markers中移除。 通知该灯塔所有Watchers，有玩家离开场景。 找出该玩家视野内所有灯塔，把玩家从这些灯塔的Watchers中移除，同时把该玩家和视野列表内的所有玩家互相把对方从视野列表内移除。 十字链表及云风大神提出的六边形场景适用性和实现细节有待商榷，暂时不讨论 示例一般设计上灯塔坐标是需要转换的，类似于Redis Cluster的slot机制，通过转换玩家坐标来判断灯塔归属。 123456789101112131415func (r *Aoi) transPos(x, y uint) (uint, uint) &#123; x = uint(math.Floor(float64(x) / float64(r.TowerWidth))) y = uint(math.Floor(float64(y) / float64(r.TowerHeight))) return x, y&#125;func (r *Aoi) getTower(x, y uint) *Tower &#123; x, y = r.transPos(x, y) tower, ok := r.Towers[x][y] if !ok &#123; fmt.Printf(\"灯塔[异常]不存在的灯塔: %d,%d \\n\", x, y) return nil &#125; return tower&#125; 完整代码: grid/main.go tower/main.go","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - BitMap","date":"2022-08-23T16:00:00.000Z","path":"2022/08/24/data-structures-algorithms-bitmap/","text":"BitMap（位图）是一种以bit方式存储数据的数据结构，以此来提高空间利用率。 定义BitMap，通过字面意思理解利用bit映射数据的存储结构。所以它应该有Key、Value，Key是bit、Value是要存储的具体的值。核心思想是通过bit来记录数据是否存储：0不存在、1存在。 定义一个长度为8的bit数组，从0 ～ 7分别映射对应的整型数值。 当整数4存储时，则改变对应bit的状态（值）。 这样做带来的好处，同样一个整数在数组中存储需要4byte*8bit=32bit空间，在BitMap中只需要1bit空间。 实现逻辑定义一个可以存储0 ～ 32的BitMap123bits := make([]byte, (32&gt;&gt;3)+1) // 32 &gt;&gt; 3 等价于 floor(32 / 8)// fmt.Printf(\"%08b\\n\", bits)// [00000000 00000000 00000000 00000000 00000000] 把31存储进BitMap，利用按位或操作改变bit值123bits[31&gt;&gt;3] |= 1 &lt;&lt; (31 % 8) // 拆解逻辑// bits[3] = 00000000 | 10000000 遇见数据去重，判断31是不是已存在123bits[31&gt;&gt;3]&amp;1&lt;&lt;(31%8) == 0 // 拆解逻辑// 10000000 &amp; 10000000 从BitMap删除指定数据1234bits[31&gt;&gt;3] &amp;^= 1 &lt;&lt; (31 % 8) // 拆解逻辑// 10000000 &amp; 10000000 = 10000000// 10000000 ^ 10000000 = 00000000 完整代码：bitmap/main.go 位操作BitMap通过位操作去实现，所以温习以下操作符： 按位与：&amp; 123// 将操作符左右两侧数值的bit进行对应 &amp;&amp; 运算4 &amp; 5 = 400000100 &amp; 00000101 = 00000100 按位或：| 123// 将操作符左右两侧数值的bit进行对应 || 运算4 | 5 = 500000100 | 00000101 = 00000101 按位异或：^ 123// 将操作符左右两侧数值的bit进行逻辑预算，规则如下：1 ^ 1 = 0、1 ^ 0 = 1、0 ^ 1 = 1、0 ^ 0 = 04 ^ 5 = 100000100 ^ 00000101 = 00000001 按位取反：^ 其他语言一般是 ~，Go语言是 ^ 123// 将操作符右侧数值的bit进行取反，0 -&gt; 1、1 -&gt; 0^4 = -5-(00000100 + 00000001) = -0000101 = 10000101 左移：&lt;&lt; 123// 将操作符左侧数值的bit按照右侧数值进行左移（溢出位则舍弃）4 &lt;&lt; 5 = 128[00000]100 &lt;&lt; 5 = 100[00000] // 左侧舍弃的位在右侧进行补位 右移：&gt;&gt;和左移同理，只不过方向相反","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - JPS","date":"2022-08-11T16:00:00.000Z","path":"2022/08/12/data-structures-algorithms-jps/","text":"JPS寻路算法是基于A*计算过程中查找节点冗余、效率较低等问题优化的一种算法。 背景A*是比较常见的寻路算法，但存在一个较为明显的问题，当移动到新的节点总是会把相邻节点（4、8）加入到待查找列表。这样带来两个问题 在多条移动路径成本相同时，冗余查找。 待查找节点过多，内存使用率过高。 JPSJPS全称“Jump search point”，又称“跳点搜索”、“拐点搜索”，由两位澳大利亚教授2011年提出（论文）。 JPS寻路算法保留A*的主体逻辑，不一样的是JPS只会把感兴趣的节点加入到待查找列表，怎样计算出感兴趣的节点呢？以下2个概念眼熟一下 强迫邻居 跳点（拐点） 通过有没有强迫邻居判断当前节点是否跳点，也就是感兴趣的节点。 强迫邻居强迫邻居的定义： 节点 x 的8个邻居中有障碍，且 x 的父节点 p 经过x 到达 n 的距离代价比不经过 x 到达的 n 的任意路径的距离代价小，则称 n 是 x 的强迫邻居。 允许对角移动的情况下，检索方向强迫邻居节点的方式分为3种：对角、垂直、水平。 对角移动 垂直移动 水平移动 绿色是父节点，红色是障碍物，x是当前节点，n是强迫邻居。 通俗的解释就是，在指定移动方向上“强迫”当前节点改变移动方向的节点就是当前节点的强迫邻居，同时当前节点也是跳点，跳点和强迫邻居都相邻障碍节点。 跳点（拐点）知道如果有强迫邻居改变当前节点移动方向，那么当前节点就被视为跳点，这是判断跳点的条件之一。判断当前节点是否跳点的任一条件： 当前节点是起点或终点。 当前节点拥有强迫邻居。 如果移动方向是对角移动，当前节点的垂直、水平方向上的节点满足条件1或条件2，当前节点一样被视为跳点。 条件3可能有些难理解，示意图: 黄色网格1、2、3都是跳点，网格1因为满足“当前节点的垂直、水平方向上的节点满足条件1或条件2，当前节点一样被视为跳点”条件（网格2是跳点），所以也是跳点。 寻路思路 把起点放进待查找节点列表。 从待查找节点列表里面取出一个，判断该节点是否是终点，如果不是则查找该节点的相邻节点。 遍历相邻节点，判断节点是否已查找过，否则对相邻节点进行跳点查找并对跳点进行估价，同时把当前节点设为跳点到父节点，如果跳点是终点则返回，否则把跳点加入到待查找节点列表。 对待查找节点列表按 f 值进行排序，取最小值。重复2、3、4步骤，直至找到终点坐标（最终的跳点）。 完整的寻路过程： 以上寻路过程，JPS和A*寻路过程的区别在于跳点查找：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 跳点函数// 判断当前点是否满足跳点条件func (r *Jps) jump(node, parent *Node) *Node &#123; // 是终点，直接返回 if r.isEnd(node) &#123; return node &#125; x, y := node.X, node.Y dx, dy := r.direction(node, parent) // 对角移动 if dx != 0 &amp;&amp; dy != 0 &#123; // [左|右]不能走 &amp;&amp; [左上|左下|右上|右下]能走 if !r.isWalkable(x-dx, y) &amp;&amp; r.isWalkable(x-dx, y+dy) &#123; return node &#125; // [上|下]不能走 &amp;&amp; [左上|右上|左下|右下]能走 if !r.isWalkable(x, y-dy) &amp;&amp; r.isWalkable(x+dx, y-dy) &#123; return node &#125; // 递归查找方向[上|下]继续查找 if r.isWalkable(x+dx, y) &amp;&amp; r.jump(r.nodes[x+dx][y], node) != nil &#123; return node &#125; // 递归查找方向[左|右]继续查找 if r.isWalkable(x, y+dy) &amp;&amp; r.jump(r.nodes[x][y+dy], node) != nil &#123; return node &#125; &#125; else if dx == 0 &#123; // 垂直移动 // 右不能走 &amp;&amp; [右下|右上]能走 if !r.isWalkable(x+1, y) &amp;&amp; r.isWalkable(x+1, y+dy) &#123; return node &#125; // 左不能走 &amp;&amp; [左上|左下]能走 if !r.isWalkable(x-1, y) &amp;&amp; r.isWalkable(x-1, y+dy) &#123; return node &#125; &#125; else &#123; // 水平移动 // 下不能走 &amp;&amp; [左下|右下]能走 if !r.isWalkable(x, y+1) &amp;&amp; r.isWalkable(x+dx, y+1) &#123; return node &#125; // 上不能走 &amp;&amp; [左上|右上]能走 if !r.isWalkable(x, y-1) &amp;&amp; r.isWalkable(x+dx, y-1) &#123; return node &#125; &#125; // 递归查找方向[左上|左下|右上|右下]继续查找 if r.isWalkable(x+dx, y+dy) &#123; if next := r.jump(r.nodes[x+dx][y+dy], node); next != nil &#123; return next &#125; &#125; // 无强迫邻居（当前节点不是跳点）或到达边界 return nil&#125; 同时，为了减少查找不必要的节点，查找相邻节点的函数也有优化12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// 查找相邻节点位置func (r *Jps) findNeighbors(node *Node) []*Node &#123; neighbors := make([]*Node, 0) // 第一次移动 if node.Parent == nil &#123; for _, v := range r.neighborPos &#123; x, y := node.X+v[0], node.Y+v[1] // 检测节点是否非法 if !r.isWalkable(x, y) &#123; continue &#125; neighbors = append(neighbors, r.nodes[x][y]) &#125; &#125; else &#123; // 计算当前节点位于父节点的方向：水平、垂直和对角方向 x, y := node.X, node.Y dx, dy := r.direction(node, node.Parent) // 移动方向上的下一个 if r.isWalkable(x+dx, y+dy) &#123; neighbors = append(neighbors, r.nodes[x+dx][y+dy]) &#125; // 对角移动 if dx != 0 &amp;&amp; dy != 0 &#123; // [左|右]能走 if r.isWalkable(x+dx, y) &#123; neighbors = append(neighbors, r.nodes[x+dx][y]) &#125; // [上|下]能走 if r.isWalkable(x, y+dy) &#123; neighbors = append(neighbors, r.nodes[x][y+dy]) &#125; // [左|右]不能走 &amp;&amp; [左上|左下|右上|右下]能走 if !r.isWalkable(x-dx, y) &amp;&amp; r.isWalkable(x-dx, y+dy) &#123; neighbors = append(neighbors, r.nodes[x-dx][y+dy]) &#125; // [上|下]不能走 &amp;&amp; [左上|右上|左下|右下]能走 if !r.isWalkable(x, y-dy) &amp;&amp; r.isWalkable(x+dx, y-dy) &#123; neighbors = append(neighbors, r.nodes[x+dx][y-dy]) &#125; &#125; else if dx == 0 &#123; // 垂直移动 // 右不能走 &amp;&amp; [右下|右上]能走 if !r.isWalkable(x+1, y) &amp;&amp; r.isWalkable(x+1, y+dy) &#123; neighbors = append(neighbors, r.nodes[x+1][y+dy]) &#125; // 左不能走 &amp;&amp; [左上|左下]能走 if !r.isWalkable(x-1, y) &amp;&amp; r.isWalkable(x-1, y+dy) &#123; neighbors = append(neighbors, r.nodes[x-1][y+dy]) &#125; &#125; else &#123; // 水平移动 // 下不能走 &amp;&amp; [左下|右下]能走 if !r.isWalkable(x, y+1) &amp;&amp; r.isWalkable(x+dx, y+1) &#123; neighbors = append(neighbors, r.nodes[x+dx][y+1]) &#125; // 上不能走 &amp;&amp; [左上|右上]能走 if !r.isWalkable(x, y-1) &amp;&amp; r.isWalkable(x+dx, y-1) &#123; neighbors = append(neighbors, r.nodes[x+dx][y-1]) &#125; &#125; &#125; return neighbors&#125; 完整代码：jps/main.go 关于其他优化点有很多值得优化的细节， 因为是网格寻路，客户端角色走路会出现拐直角的表现（平滑路径，比如：弗洛伊德算法）。 待查找节点列表可以用二叉堆实现，插入和弹出效率都很高。 地图节点使用数组存储，检索效率较高。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - AStar(A*)","date":"2022-07-31T16:00:00.000Z","path":"2022/08/01/data-structures-algorithms-astar/","text":"寻路算法是游戏比较重要的组成部分之一，尤其在国内游戏很常见的自动寻路系统，比如操纵角色从A点到B点的移动。计算最短路径有很多不同的算法，比如：Dijkstra、AStar（A*）、NavMesh（多边形算法）、RVO动态避障等等 A*寻路算法寻路算法的移动路径核心有2点：最短、可移动。 所以首先需要确定起点坐标的相邻节点（4方向、8方向），然后通过计算最靠近终点坐标的相邻坐标（不考虑障碍物）确认检索方向，再去计算“相邻坐标”移动的下一个坐标（距离终点坐标最近 &amp; 可移动），重复计算这个过程直至找到终点坐标，这就是A*算法 A*算法是一类基于网格的寻路算法，也就是把地图看作一个由网格组成的矩阵，每个坐标就是一个网格。 一张3x3地图 启发函数从起点移动到终点坐标可能会产生多条移动路径，所以还需要计算出哪条移动路径最短，不然越走越远就演变成《论述如何帮助玩家刷微信步数》的尴尬情况。 在网格地图中肯定是经过越少的网格路径就越短，但我们不可能把每条可能的路径都走一遍再选出哪条路径最短，所以需要进行评估接下来走哪个网格最近。 假设我们把网格直线（水平、垂直）移动成本设为1、对角移动成本设为：1.4。然后把移动路径上的坐标的成本进行相加取出最小值即可能是当前最佳移动路径。 这也叫启发式算法，即优先检索可能是成本最低的网格。 估价公式我们不可能把每条可能的路径都走一遍，所以通过估价公式：f = g + h，来评估接下来走哪一个网格。 g 从起点坐标移动到当前坐标的移动成本（网格数） h 从当前坐标移动到终点坐标的移动成本 f 当前坐标的移动成本 通过这样一个网格移动成本的评估公式，可以大概计算出从起点网格移动至终点网格的路径中，下一步走哪一个网格可能路径最短。 移动成本怎么计算移动成本？使用距离算法作为估价函数对网格进行估价。比如以下的距离算法（不仅限于）： 欧氏距离 曼哈顿距离 切比雪夫距离 45度角计算（Octile） 不同的距离算法在不同的场景上表现不一，比如说在静态网格地图场景中 欧氏距离 优点： 计算直线距离，移动路径最短。 缺点： 计算过程中伴随着平方与开根号运算，并且需要使用浮点数，性能差。 因为是网格所以基本不能按照直线进行移动，精确度不够。 曼哈顿距离 优点： 计算过程不存在浮点数，性能较高。 缺点： 只可以垂直或者水平移动。 所以在对4方向、8方向又或者6方向上的算法选择是不一样的。比如4方向移动上下左右使用曼哈顿距离来进行估价就完全足够，但如果是8方向移动就不可以（可以对角移动）。 所以，如果是8方向移动的场景通常会采用曼哈顿+对角计算的距离算法。 寻路思路示例：3x3 &amp; 无障碍的地图 绿色是起点，蓝色是终点 把起点放进待查找节点列表。 从待查找节点列表里面取出一个，判断该节点是否是终点，如果不是则查找该节点的相邻节点，找到3个节点分别是右、右下、下。 遍历相邻节点，判断节点是否已查找过，否则对3个相邻节点的进行估价（因为使用曼哈顿算法所以把浮点数放大10倍），并把绿色节点设为3个相邻节点的父节点，如果某一相邻节点是终点则返回，否则把相邻节点加入到待查找节点列表。 1234561. 右 起点距离该节点水平移动一个网格，所以g = 10。该节点到终点对角移动一个网格，所以h = 14。最终该节点的移动成本：14（f = g + h ）。2. 右下 起点距离该节点对角移动一个网格，所以g = 14。该节点到终点垂直移动一个网格，所以h = 10。最终该节点的移动成本：14（f = g + h ）。3. 下 起点距离该节点垂直移动一个网格，所以g = 10。该节点到终点水平移动两个网格，所以h = 20。最终该节点的移动成本：30（f = g + h ）。 对待查找节点列表按 f 值进行排序，取最小值。重复2、3、4步骤，直至找到终点坐标。 代码实现（附上完整代码）首先最基础的是把地图进行网格化。123456789101112131415/*地图从左上角开始，水平x 垂直y yx 0,0 1,0 2,0 0,1 1,1 2,1 0,2 1,2 2,2*/// 0是可移动节点、1是障碍节点mapData := [][]int&#123; &#123;0, 0, 1, 1, 0, 0, 0, 0&#125;, &#123;0, 0, 0, 0, 1, 0, 0, 0&#125;, &#123;0, 0, 0, 1, 1, 0, 0, 0&#125;, &#123;0, 0, 0, 0, 1, 0, 0, 0&#125;, &#123;0, 0, 0, 0, 0, 0, 0, 0&#125;,&#125; 每个节点（网格）对应一个坐标，因为估价需要同时每个节点还存储着f、g、h值。12345678910111213141516// 节点type Node struct &#123; // 坐标 X int Y int // 成本 F int G int H int // 父节点 Parent *Node // 类型 Type int // 状态 State int&#125; 有了节点，那么还需要定义每个节点有哪些邻居（这里是8方向）1234567891011// 如果不允许对角移动，去除对角坐标neighborPos = [][]int&#123; &#123;0, -1&#125;, // 上 &#123;1, -1&#125;, // 右上 &#123;1, 0&#125;, // 右 &#123;1, 1&#125;, // 右下 &#123;0, 1&#125;, // 下 &#123;-1, 1&#125;, // 左下 &#123;-1, 0&#125;, // 左 &#123;-1, -1&#125;, // 左上&#125; 并不是所有节点都需要查找，所以还需要定义待查找节点列表、已查找节点列表（已查找过的节点会被忽略）。1var openList, closeList []*Node 最后，查找节点的逻辑（这里偷懒，直接贴上封装过的逻辑）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071type AStar struct &#123; // 启发算法 Heuristic func(node, end *Node) int // 地图大小 Rows int // y Cols int // x // 地图节点 nodes [][]*Node start *Node end *Node // 开放、关闭列表 openList []*Node closeList []*Node // 相邻节点坐标 neighborPos [][]int&#125;func (r *AStar) FindPath(start, end *Node) *Node &#123; r.start = r.nodes[start.X][start.Y] r.end = r.nodes[end.X][end.Y] // 如果起止点是障碍物 if !r.start.isWalkable() || !r.end.isWalkable() &#123; fmt.Println(\"障碍物不可移动\") return nil &#125; // 先把开始节点放进开放列表 r.openListAppend(start) for len(r.openList) &gt; 0 &#123; node := r.openListPop() // 判断当前节点是否是终点 if r.isEnd(node) &#123; return node &#125; // 找开放列表的第一个节点的相邻节点 neighbors := r.findNeighbors(node) for _, neighbor := range neighbors &#123; // 是否在关闭列表 if neighbor.isClosed() &#123; continue &#125; // 开始节点移动至当前节点的成本 // 相邻节点的坐标x,y // 开始节点移动至相邻节点的成本 g, x, y := node.G, neighbor.X, neighbor.Y // 判断移动方式是水平（或垂直）、对角，计算成本 if x == node.X || y == node.Y &#123; g += COST_STRAIGHT &#125; else &#123; g += COST_DIAGONAL &#125; if !neighbor.isOpened() || g &lt; neighbor.G &#123; neighbor.G = g neighbor.H = r.Heuristic(neighbor, end) neighbor.F = neighbor.G + neighbor.H neighbor.Parent = node // 优化逻辑，相邻节点是否是终点 // if r.isEnd(neighbor) &#123; // return neighbor // &#125; if !neighbor.isOpened() &#123; r.openListAppend(neighbor) &#125; &#125; &#125; // 当前节点放进关闭列表 r.closeListAppend(node) // 更新开放列表顺序 r.openListSort() &#125; return nil&#125; 为什么要给节点定义父节点？ 这里使用链表记录移动路径，即通过终点节点不断迭代父节点打印出完整路径。 完整代码：astar/main.go","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"数据结构与算法 - Linked list","date":"2022-07-18T16:00:00.000Z","path":"2022/07/19/data-structures-algorithms-linked-list/","text":"链表是一种不需要连续空间存储的线性数据结构，通过指针把数据节点链接起来。数据节点的逻辑顺序取决于链表指针链接的先后顺序，同时，链表也有：单链表、双链表、循环链表等多类。 定义链表是由一系列节点组成，除此之外链表还有几个基本概念： 数据节点 一般由两块组成：数据域、指针域。 首元节点 链表的第一个数据节点。 头节点 非必要节点，放在第一个数据节点前。一般用于存储链表的信息（比如说链表长度等）和保证数据节点操作的统一性。 头指针 指向第一个节点的指针，如果有头节点则指向头节点，否则指向首元节点。 单链表最简单的一种链表，只支持一个遍历方向：链头 -&gt; 链尾。两部分组成：当前节点的数据、指向后置节点的指针（尾节点指针指向nil）。 例子：12345678910111213141516171819202122232425262728293031323334353637type Node struct &#123; V string Next *Node&#125;func main() &#123; // 头节点 head := &amp;Node&#123; V: \"head node\", &#125; // 节点1 node := &amp;Node&#123; V: \"node1\", &#125; head.Next = node // 节点2 node2 := &amp;Node&#123; V: \"node2\", &#125; node.Next = node2 // 节点3 node3 := &amp;Node&#123; V: \"node3\", &#125; node2.Next = node3 // 头指针 -&gt; 头节点 linkedList := head for linkedList.Next != nil &#123; // 打印下一个节点值（这块就体现出头节点的好处，首元节点和其他数据节点的操作一致） fmt.Println(linkedList.Next.V) linkedList = linkedList.Next &#125;&#125; 以上是单链表的简单实现，其中添加节点的操作也被称为：尾插法，封装一下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445type Node struct &#123; V string Next *Node&#125;type List struct &#123; Head *Node&#125;func main() &#123; list := &amp;List&#123;&#125; n1 := &amp;Node&#123;V: \"node1\"&#125; n2 := &amp;Node&#123;V: \"node2\"&#125; n3 := &amp;Node&#123;V: \"node3\"&#125; list.Append(n1) list.Append(n2) list.Append(n3) list.Traverse() // 输出结果：node1-&gt;node2-&gt;node3-&gt;nil&#125;// 尾插法func (l *List) Append(node *Node) &#123; if l.Head == nil &#123; l.Head = node &#125; else &#123; now := l.Head for now.Next != nil &#123; now = now.Next &#125; now.Next = node &#125;&#125;// 遍历func (l *List) Traverse() &#123; now := l.Head for now != nil &#123; if now.Next == nil &#123; print(now.V, \"-&gt;\", \"nil\") &#125; else &#123; print(now.V, \"-&gt;\") &#125; now = now.Next &#125;&#125; 如果有数据节点想要插队，怎么办？头插法，思路就是：把头节点作为新增节点的后置节点，新增节点作为新的头节点。 12345678func (l *List) Insert(node *Node) &#123; if l.Head == nil &#123; l.Head = node &#125; else &#123; node.Next = l.Head l.Head = node &#125;&#125; 还想把链表反转，数据节点逆序呢？有很多种方式，比如：利用双指针迭代、递归等等 迭代，思路和头插法一致。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func (l *List) Reverse() &#123; if l.Head == nil &#123; return &#125; // 声明一个头节点 var head *Node // 开始遍历链表， // 第一次，head：node1 -&gt; nil // 第一次，now：node2 -&gt; node3 -&gt; nil // 第二次，head：node2 -&gt; node1 -&gt; nil // 第二次，now：node3 -&gt; nil // 第三次，head：node3 -&gt; node2 -&gt; node1 -&gt; nil // 第三次，now：nil now := l.Head for now != nil &#123; // 记录当前节点的后置节点 next := now.Next // 当前节点的后置节点变为头节点，当前节点变为新的头节点。这个操作和插入节点的逻辑一致，把头节点作为当前节点的后置节点，当前节点作为新的头节点。 if head == nil &#123; now.Next = nil &#125; else &#123; now.Next = head &#125; head = now // 指针后移 now = next // 语法糖版本 // now, now.Next, head = now.Next, head, now // 语法糖版本，拆解如下 // now = now.Next // now.Next = head // head = now // printNode(head) // printNode(now) &#125; l.Head = head&#125;func printNode(node *Node) &#123; if node == nil &#123; print(\"nil\") &#125; now := node for now != nil &#123; if now.Next == nil &#123; print(now.V, \"-&gt;\", \"nil\") &#125; else &#123; print(now.V, \"-&gt;\") &#125; now = now.Next &#125; print(\"\\n\")&#125; 删除操作:1234567891011121314151617func (l *List) Delete(node *Node) &#123; if l.Head == nil &#123; return &#125; // 如果是尾节点，则需要找出删除节点的前置节点 if node.Next == nil &#123; prev := l.Head for prev.Next != node &#123; prev = prev.Next &#125; prev.Next = nil &#125; else &#123; // 如果不是尾节点，把删除节点的后置节点数据直接覆盖删除节点。 *node = *node.Next &#125; return&#125; 单链表的实现比较简单，可以很轻松的查询到指定节点的后置节点，但如果想要查询前置节点则需要遍历一遍才可以，所以这就需要双链表了 双链表双链表通俗的讲，就是每个数据节点既可以找到它前一个节点也找的到它后一个节点。三部分组成：前值节点的指针，节点数据，后置节点的指针（尾节点指针指向nil）。 直接上代码:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061type Node struct &#123; V string Prev *Node Next *Node&#125;type List struct &#123; Head *Node Tail *Node&#125;func main() &#123; list := &amp;List&#123;&#125; n1 := &amp;Node&#123;V: \"node1\"&#125; n2 := &amp;Node&#123;V: \"node2\"&#125; n3 := &amp;Node&#123;V: \"node3\"&#125; list.Append(n1) list.Append(n2) list.Append(n3) list.Traverse() // 输出结果：node1&lt;-&gt;node2&lt;-&gt;node3-&gt;nil&#125;func (l *List) Append(node *Node) &#123; if l.Head == nil &#123; l.Head = node l.Tail = node &#125; else &#123; // 尾节点 prev := l.Tail // 尾节点设为当前节点的前置节点 node.Prev = prev // 当前节点设为尾节点的后置节点 prev.Next = node // 当前节点变为尾节点 l.Tail = node &#125;&#125;func (l *List) Delete(node *Node) &#123; if l.Head == nil &#123; return &#125; // 删除节点的前置节点 prev := node.Prev // 删除节点的后置节点设为前置节点的后置节点 prev.Next = node.Next // 删除节点的前置节点设为后置节点的前置节点 node.Next.Prev = node.Prev&#125;func (l *List) Traverse() &#123; now := l.Head for now != nil &#123; if now.Next == nil &#123; print(now.V, \"-&gt;\", \"nil\") &#125; else &#123; print(now.V, \"&lt;-&gt;\") &#125; now = now.Next &#125;&#125; 双链表较单链表的不同比较明显，双链表可以很轻松的查找到指定节点的前置和后置节点，遍历方向也可以是从头到尾或者从尾到头。同时，双链表节点操作比较复杂，占用空间也更大。 循环链表顾名思义，循环链表是一个环状的链表。循环链表又分为：单向循环链表、双向循环链表，实现上就是把尾节点和头节点进行链接。 单向循环链表：12345678910111213141516171819202122232425262728293031323334353637383940414243444546type Node struct &#123; V string Next *Node&#125;type List struct &#123; Head *Node&#125;func main() &#123; list := &amp;List&#123;&#125; n1 := &amp;Node&#123;V: \"node1\"&#125; n2 := &amp;Node&#123;V: \"node2\"&#125; n3 := &amp;Node&#123;V: \"node3\"&#125; list.Append(n1) list.Append(n2) list.Append(n3) list.Traverse() // 输出结果：node1-&gt;node2-&gt;node3-&gt;node1&#125;func (l *List) Append(node *Node) &#123; if l.Head == nil &#123; l.Head = node node.Next = l.Head &#125; else &#123; now := l.Head for now.Next != l.Head &#123; now = now.Next &#125; now.Next = node node.Next = l.Head &#125;&#125;func (l *List) Traverse() &#123; now := l.Head for &#123; if now.Next == l.Head &#123; print(now.V, \"-&gt;\", l.Head.V) break &#125; else &#123; print(now.V, \"-&gt;\") &#125; now = now.Next &#125;&#125; 使用场景链表是最基础且很常用的一种数据结构，可以解决很多算法问题，也可以用于构建其他的数据结构。比如说解决约瑟夫环问题和实现堆栈、队列等。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"data structures","slug":"data-structures","permalink":"http://xupin.im/tags/data-structures/"},{"name":"algorithms","slug":"algorithms","permalink":"http://xupin.im/tags/algorithms/"}]},{"title":"Go学习笔记 - 泛型","date":"2022-07-13T16:00:00.000Z","path":"2022/07/14/go-generic/","text":"2009年Go语言首次发布后，支持泛型一直以来呼声最高的功能之一。十年磨一剑！Go官方终于在2022年03月15日发布go1.18 stable正式支持泛型 什么是泛型泛型的核心是把类型参数化，通俗的来说就是允许使用时才指定类型的一种设计。 假设有个两变量相加的需求，且这俩变量可能是int或者string，简单粗暴就直接定义两个函数分别支持int、string123456789// 传统写法func AddInt(arg1, arg2 int) int &#123; return arg1 + arg2&#125;func AddString(arg1, arg2 string) string &#123; return arg1 + arg2&#125;AddInt(1, 2)AddString(\"a\", \"b\") 那如果使用泛型来定义呢，它的写法大概是这样的 1234567// 泛型写法func Add[type] (var1, var2) type &#123; return var1 + var2&#125;// 使用时再指定数据类型Add[int](1, 2)Add[string](\"a\",\"b\") 好处显而易见，至少可以少拷贝一个函数。当然这不足以体现出泛型的优点，毕竟这种通用函数通过interface一样可以实现。 123456789101112func Add(var1, var2 interface&#123;&#125;) interface&#123;&#125; &#123; r := reflect.TypeOf(var1) switch r.Kind() &#123; case reflect.Int: return var1.(int) + var2.(int) case reflect.String: return var1.(string) + var2.(string) &#125; return nil&#125;Add(1, 2)Add(\"a\",\"b\") 既然通过interface也可以实现，那么为什么泛型还备受推崇呢？ 便利性 &amp; 效率 使用interface实现则需要使用断言或者反射来处理类型问题，同时Go语言中反射性能十分低下（见下图）。 安全性 interface极易引发类型异常问题，编译器在编译过程中不能发现类型问题。 Go的泛型在Go语言中如何使用泛型，先了解一下泛型的3个概念： 泛型参数 泛型参数类型约束 泛型参数列表 Go泛型的代码风格和其他语言基本类似，比如定义一个只支持int和string两值相加的泛型函数：123func Add[T int | string](var1, var2 T) T &#123; return var1 + var2&#125; 以上函数， T 泛型参数，可以是任意字母且不区分大小写（但通常使用大写字母）。 int | string 泛型参数类型约束，用于声明和约束泛型参数T支持的数据类型。比如 1234Add[T float64] // 只支持float64类型Add[T any] // 支持任何类型，any等价于interface&#123;&#125;Add[T comparable] // golang新增的内置接口，任何可进行比较的类型（!=、==、&amp;&amp;、||）Add[T ~int] // 任何底层类型为int的类型，~是新增的操作符。 [] 中括号内是泛型参数列表，支持任意个泛型参数，使用逗号分隔。 1Add[T int64, T2 float64](var1, var2 T, var3 T2) 如果泛型仅仅能在函数上使用好像有点“鸡肋”。它还可以这样去使用，1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// 泛型结构体type Resp[T any] struct &#123; Code int Msg string Data T&#125;resp := &amp;Resp[string]&#123; Code: 200, Msg: \"OK\", Data: \"\",&#125;// 泛型管道type MyChan[T int | bool] chan Tch := make(MyChan[bool], 1)ch &lt;- true&lt;-ch// 泛型Maptype MyMap[T int | string, T1 any] map[T]T1m := MyMap[string, string]&#123; \"key\": \"val\",&#125;// 泛型接口type Personer[T any] interface &#123; Say(age T)&#125;type Male struct&#123;&#125;func (g *Male) Say(age int) &#123; fmt.Println(age)&#125;var m Personer[int] = &amp;Male&#123;&#125;m.Say(12)type Female struct&#123;&#125;func (f *Female) Say(age string) &#123; fmt.Println(age)&#125;var f Personer[string] = &amp;Female&#123;&#125;f.Say(\"secret\")// 泛型类型type MyType interface &#123; int | string | float64&#125;func Add[T MyType](var1, var2 T) &#123; return var1 + var2 &#125; 以上只是简单列举，还支持一些列“套娃”行为。值的注意的是，Go新增的操作符~。123456789type MyType interface &#123; ~int | string | float64&#125;type MyInt intfunc Add[T MyType](var1, var2 T) T &#123; return var1 + var2&#125;var a, b MyInt = 1, 2Add(a, b) 使用操作符~，意味不仅支持该操作符后的基本类型，同时还支持任意底层类型是该类型的数据。 以上内容参考来源：https://tip.golang.org/doc/go1.18 什么时候使用泛型当你要为不同数据类型做着同样的处理逻辑时，可以考虑使用泛型。泛型是把双刃剑，既可以提高编码效率、提升代码美学，同样也可以让代码可读性变的很差。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"Go学习笔记 - init函数","date":"2022-03-07T16:00:00.000Z","path":"2022/03/08/go-init/","text":"定义Golang中的特殊函数，会先于main函数执行且不能被调用。该函数无入参、返回值，同时支持选择性定义。也可以选择定义多个init函数，但建议只定义一个。123456789101112131415161718package mainimport \"fmt\"func init() &#123; fmt.Println(\"init1\")&#125;func init() &#123; fmt.Println(\"init2\")&#125;func main() &#123; fmt.Println(\"main\")&#125;// init1// init3// main sync.Onceinit函数实现了sync.Once锁，只会被执行一次。 my/my.go 1234567891011package myimport \"fmt\"func init() &#123; fmt.Println(\"a.init\")&#125;func Test() &#123; fmt.Println(\"a.Test\")&#125; main.go 12345678910111213package mainimport ( \"test/my\")func main() &#123; my.Test() my.Test()&#125;// a.init// a.Test// a.Test 初始化顺序在package内部变量赋值 &gt; init() &gt; main()。 123456789101112131415161718192021package mainimport \"fmt\"var f string = sayHello()func init() &#123; fmt.Println(\"init\")&#125;func main() &#123; fmt.Println(\"main\")&#125;func sayHello() string &#123; fmt.Println(\"sayHello\") return \"hi\"&#125;// sayHello// init// main 多个package调用时的初始化顺序，则取决于依赖顺序。 my.go 123456789101112131415package myimport ( \"fmt\" \"test/your\")func init() &#123; fmt.Println(\"a.init\")&#125;func Test() &#123; fmt.Println(\"a.Test\") your.Test()&#125; your.go 1234567891011package yourimport \"fmt\"func init() &#123; fmt.Println(\"your.init\")&#125;func Test() &#123; fmt.Println(\"your.Test\")&#125; main.go 12345678910111213package mainimport ( \"test/my\")func main() &#123; my.Test()&#125;// your.init// a.init// a.Test// your.Test 疑问当package内定义多个init函数，它的执行顺序？","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"ClickHouse学习笔记 - MergeTree存储结构","date":"2022-01-25T16:00:00.000Z","path":"2022/01/26/clickhouse-mergetree/","text":"ClickHouse是这两年备受瞩目的开源列式数据库，主要应用场景是OLAP（在线数据分析处理）场景。 背景之前因为遇见一些小问题，所以想通过存储结构和索引方式两个方面搞清楚它为什么可以这么快。 ClickHouse是OLAP型的列式数据库，官方称比较行式数据库查询效率提升至少100倍，DB Benchmark显示ClickHouse比Mysql快800倍！ 什么是OLAP型列式数据库OLAP业务场景一般具有以下特征： 多数是查询请求，但请求不密集。 数据写入不频繁，或者单次大批量写入数据。 尽量不更新或少更新历史数据。 数据表列比较多（宽表），但每列存储数据较小。 对数据一致性要求较低，不依赖事务。 查询请求目标数据表比较独立，或者除目标表外其他需要关联的表的数据量相对很少。 查询结果集数据量远远小于源数据表，至少服务器内存不会溢出。 简单的查询，毫秒级别的响应。 单次查询高吞吐量。 列式和行式数据库的区别存储方式 行式 Row WatchID JavaEnable Title GoodEvent EventTime #0 89354350662 1 Investor Relations 1 2016-05-18 05:19:20 #1 90329509958 0 Contact us 1 2016-05-18 08:10:20 #2 89953706054 1 Mission 1 2016-05-18 07:38:00 #N … … … … … 列式 Row: #0 #1 #2 #N WatchID: 89354350662 90329509958 89953706054 … JavaEnable: 1 0 1 … Title: Investor Relations Contact us Mission … GoodEvent: 1 1 1 … EventTime: 2016-05-18 05:19:20 2016-05-18 08:10:20 2016-05-18 07:38:00 … 检索方式（通过两张图感受一下） 行式： 列式： 列式数据库的优势 查询请求时只读取需要的列，而不是像行式数据库需要读取符合条件的所有block（即使是行式数据库也不是按行读取数据而是块，有些数据库也称页）。需要读取数据量的减少，意味着IO操作会更快。 同一列的数据类型一致，可以使用最适用的压缩算法（更高的压缩比），节省存储空间。 ClickHouse的优缺点 优点 支持SQL语法，不像ES的DSL那样晦涩难懂。 可以保证数据一致性（最终）。 压榨性能极限，多核心处理使用机器一切可用资源。 高效的磁盘IO，数据都是有序存储。 查询不需要遵守索引最左原则。 批量写入速度快。 缺点 部分SQL语法不支持，但无伤大雅。 更新和删除操作支持不好。 不支持事务。 不支持高并发，官方默认QPS：100（可以通过配置修改） 简单的列举了优缺点，这只是冰山一角。实际上ClickHouse值得深入探讨的太多太多。 MergeTree的特性ClickHouse是真正的列式存储数据库，不同于内存计算引擎（比如SparkSQL、Presto），它有自己的数据存储机制和索引方式。 1234567891011121314CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2) ENGINE = MergeTree()ORDER BY expr[PARTITION BY expr][PRIMARY KEY expr][SAMPLE BY expr][TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'], ...][SETTINGS name=value, ...] ORDER BY 排序字段，数据存储时按照排序字段（排序字段可以是一个字段，也可以是元组）进行有序存储。 PARTITION BY ClickHouse支持按列值进行数据分区，也支持以表达式进行分区（比如toYear(date)），分区字段值不可以过多，否则会报错（比如按照user_id分区）。主要作用是分区裁剪，过滤非必要的查询数据。 PRIMARY KEY ClickHouse的MergeTree引擎可以定义主键，但不同于行式数据库。主键不做唯一约束，可以是多个字段。默认不定义主键，会将排序键作为主键。如果定义主键（排序键过大，为了控制主键文件大小），则尽量保证是排序键的前部分键。 SAMPLE BY ClickHouse提供的抽样表达式机制，查询请求时通过采样子句进行数据采样功能。建表时需要声明采样表达式（通过哪个字段采样），该字段必须是UInt类型且必须出现在主键或排序键中（如果不是UInt类型可以声明，但使用会报错）。 1234567create table example( user_id UInt64, name String)ORDER BY intHash32(user_id)SAMPLE BY intHash32(user_id) 查询请求时，通过sample关键字进行伪随机数据采样。123select * from example sample 1/10 # 数据的10%select * from example sample 10 # 10行数据select * from example sample 1/10 offset 1/2 # 从数据表50%部分中取出10%的数据 可以通过字段_sample_factor查询采样系数，该字段是虚拟的。 TTL ClickHouse原生支持的数据生命周期管理的机制，在MergeTree中可以为列、表2个级别设置TTL，当时间到期后触发相应级别的数据删除操作。无论是哪种级别的TTL，字段必须为Datetime或Date类型，因为依赖INTERVAL设置生命周期。 123456create table example( date Date, login_ip String TTL date + INTERVAL 1 DAY)TTL date + INTERVAL 1 MONTH # [DELETE,TO VOLUME,TO DISK] 默认值: DELETE 当列级TTL触发时，会把对应值修改为列类型的默认值，如果该列数据全部过期则会删除该列。 当表级TTL触发时，会删除所有符合过期条件的数据行。 当同时设置列级、表级TTL，以先到期的为准。 SETTINGS 控制 MergeTree 行为的额外参数，可选项： 1index_granularity # 索引粒度。索引中相邻的『标记』间的数据行数。默认值8192 。 其他更多参数，可以查阅官方文档SETTINGS部分。 存储结构使用MergeTree引擎，一张数据表会分成N个数据分区，每个数据分区对应一个文件目录。可以通过SQL查询到分区信息：1SELECT * FROM system.parts WHERE `table` = '&#123;table&#125;' -- path字段是数据分区的存储位置 或者在ClickHouse数据存储目录查看1234567891011121314151617181920212223242526272829$ cd /data/clickhouse/data$ tree -d.├── default│ ├── test_a -&gt; /data/clickhouse/store/adf/adf66eb8-4f34-4a9c-adf6-6eb84f346a9c/│ └── test_b -&gt; /data/clickhouse/store/ed9/ed96975a-4048-4846-ad96-975a40484846/└── system ├── asynchronous_metric_log -&gt; /data/clickhouse/store/507/5075436a-cb6c-4fac-9075-436acb6ccfac/ ├── metric_log -&gt; /data/clickhouse/store/86c/86cd831c-1bd4-4df2-86cd-831c1bd43df2/ ├── query_log -&gt; /data/clickhouse/store/986/9860fbbb-e965-476f-9860-fbbbe965776f/ ├── query_thread_log -&gt; /data/clickhouse/store/c1b/c1b39615-68ee-4fb7-81b3-961568eeafb7/ └── trace_log -&gt; /data/clickhouse/store/b6d/b6d34f2c-1580-411b-b6d3-4f2c1580911b/$ cd /data/clickhouse/store/adf/adf66eb8-4f34-4a9c-adf6-6eb84f346a9c/$ tree.├── 20220101_1_3_0│ ├── checksums.txt│ ├── columns.txt │ ├── &#123;column&#125;.bin│ ├── &#123;column&#125;.mrk│ ├── count.txt│ ├── primary.idx│ ├── partition.dat│ ├── minmax_&#123;column&#125;.idx│ ├── skp_idx_&#123;column&#125;.idx│ └── skp_idx_&#123;column&#125;.mrk├── detached├── format_version.txt└── mutation_&#123;key&#125;.txt 20220101_1_3_0 数据分区的名称，由4部分组成。 名称：20220101 最小数据块编号：1 最大数据块编号：3 数据块级别：0（即在由块组成的合并树中，该块在树中的深度） checksums.txt 校验文件，使用二进制存储数据目录的文件信息，保证安全性和完整性。 columns.txt 明文存储列信息（列名，列类型） {column}.bin 核心数据文件（二进制存储），分区字段part_type为Wide时会为每列创建对应的数据文件，如果是Compact则所有列存储在一个数据文件。该值受配置min_bytes_for_wide_part、min_rows_for_wide_part影响。该文件的内容以数据Block存储（也称页），数据是否压缩受配置max_compress_block_size、min_compress_block_size影响。默认压缩格式：lz4。Block数据块分为两部分：头信息和数据。头信息固定占9字节：压缩方式1字节、压缩后大小4字节、压缩前大小4字节。 {column}.mrk 列字段标记文件（二进制存储），文件中记录{column}.bin文件中数据Block的偏移量。 count.txt 该数据分区的数据行数。 primary.idx 主键索引文件（二进制存储），每index_granularity行取主键值作为稀疏索引。 partition.dat 当前数据分区的编号。 minmax_{column}.idx 列索引文件，存储列的最大值和最小值。 skpidx{column}.idx 跳数索引（又称二级索引）文件，受allow_experimental_data_skipping_indices参数控制，默认不开启。 skpidx{column}.mrk 跳数索引标记文件。 detached 执行ALTER TABLE table_name DETACH PARTITION|PART partition_expr命令后，会将对应的数据分离至该目录并“遗忘”。 format_version.txt 记录存储的格式。 mutation_{key}.txt 不同于行式数据库，ClickHouse对于mutation操作（Update、Delete）是异步执行的，同时会生成mutation_{key}.txt文件记录操作和生成新的数据目录20220101_1_3_0_{key}。可以通过SQL查询: 1SELECT * FROM system.mutations WHERE `table` = '&#123;table&#125;' 因为使用类LSM架构，该类操作尽量少执行（埋个坑，后面再去研究）","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"clickhouse","slug":"clickhouse","permalink":"http://xupin.im/tags/clickhouse/"}]},{"title":"函数 - 闭包函数、匿名函数及lambda“函数”有什么关系","date":"2022-01-18T16:00:00.000Z","path":"2022/01/19/func-closure-lambda-diff/","text":"经常在技术博客中看到匿名函数、闭包函数和lambda表达式这样名词，但一直傻傻分不清他们之间究竟有什么区别，所以想一探究竟。 函数定义一个函数的定义，一般会包含：函数名、函数的参数、函数的主体（代码块）、函数的返回类型/值。123func hello(name string) string &#123; return \"Hello, \" + name&#125; 闭包函数通过字面意思理解，即定义在内部的函数。12345678910111213141516171819202122232425package mainimport \"fmt\"func main() &#123; t := test() t() t() t()&#125;func test() func() &#123; x := 0 fmt.Println(\"test()\", x, &amp;x) return func() &#123; x += 1 fmt.Println(\"closure()\", x, &amp;x) &#125;&#125;// 执行程序输出以下内容// test() 0 0xc000016098// closure() 1 0xc000016098// closure() 2 0xc000016098// closure() 3 0xc000016098 执行test()函数返回闭包函数。 执行闭包函数发现可以使用x变量，但不用声明x变量。 再次执行闭包函数仍旧可以使用x变量，说明闭包函数自身在“维护”x变量。 通过内存地址可以看出闭包函数和test()函数的x变量是同一个变量。 是不是可以理解为：函数创建时获取所需外部状态，即使外部状态关闭，函数中的状态还会存在。这个过程就是“闭包”，或许闭包函数解决的是变量作用域的问题？ 引用JavaScript MDN对闭包函数的一句解释： A closure is the combination of a function and the lexical environment within which that function was declared. 匿名函数匿名函数，即“无名”函数。1234567891011package mainimport \"fmt\"func main() &#123; lam := func(x, y, z int) int &#123; return x + y + z &#125; x := lam(1, 2, 3) fmt.Println(x)&#125; 匿名函数可以直接定义在代码块中。匿名函数可以减少函数暴露，防止被外部代码调用执行（避免数据污染）。 lambda“函数”lambda“函数”，严格来说应该叫lambda表达式。它是一个可以接受N个参数但只返回单个表达式值的“函数”，可以理解为它是匿名函数的一种代码风格或者语法糖。 lambda会使代码看起来比较简洁，很适用于这个函数“只用一次”的场景。比如：123456789def square(x): return x * xprint(\"square()\", list(map(square, [1, 2, 3, 4, 5])))print(\"lambda\", list(map(lambda x: x * x, [1, 2, 3, 4, 5])))# 输出# square() [1, 4, 9, 16, 25]# lambda [1, 4, 9, 16, 25] lambda表达式带来代码简洁的同时也有副作用，lambda可读性较差，不适用于很长很复杂的逻辑，所以不要为了写lambda而写lambda。123456789func = lambda x, y, z: x + y + zx = func(1, 2, 3)print(x)# 代码被自动转换def func(x, y, z): return x + y + zx = func(1, 2, 3)print(x) 在Visual Studio Code中编辑保存这段代码会被自动转换，在PyCharm中则会直接给出警告PEP 8: E731 do not assign a lambda expression, use a defautopep8 至于为什么这种书写方式称之为lambda表达式，则是有原因的： Alonzo Church在30年代发明的Lambda Calculus，你可以在其中做的一件事就是λ运算。 最后猜猜以下这段代码执行后会输出什么？123456789101112131415161718192021package mainimport ( \"fmt\" \"sync\")func main() &#123; var wg sync.WaitGroup a := []int&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125; for _, v := range a &#123; i := v fmt.Printf(\"for v:%d i:%d\\n\", v, i) wg.Add(1) go func() &#123; defer wg.Done() fmt.Printf(\"func v:%d i:%d\\n\", v, i) &#125;() &#125; wg.Wait()&#125;","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"lambda","slug":"lambda","permalink":"http://xupin.im/tags/lambda/"},{"name":"closure","slug":"closure","permalink":"http://xupin.im/tags/closure/"},{"name":"function","slug":"function","permalink":"http://xupin.im/tags/function/"}]},{"title":"ClickHouse学习笔记 - 数据引擎","date":"2022-01-15T16:00:00.000Z","path":"2022/01/16/clickhouse-engines/","text":"ClickHouse是个由老毛子国内市场使用率第一的搜索引擎Yandex开源的OLAP型列式数据库。 背景之前因为业务接触到ClickHouse，但因为没有系统的了解过导致遇见个很小的问题、花了大量的时间、用了很挫的方式来解决问题，痛定思痛的情况下决心系统的逐步学习一下。ClickHouse无论是存储还是使用方式都不同程度的异于我们常用的Mysql，PostgreSql，Mssql等行式数据库，但它支持绝大部分的SQL语法。 数据引擎ClickHouse（基于v21.7）的引擎分为数据库引擎和表引擎。 数据库引擎 MySQL 顾名思义，和Mysql数据库有关系。该引擎允许连接远程Mysql数据库并且执行Sql交换数据。但是仅支持部分操作（读写）。 MaterializeMySQL v20.8新增的引擎，这是一个处在实验阶段的引擎，通过名称可以看出来也是和Mysql相关的引擎，该引擎通过binlog日志进行全量（第一次）和增量实时物化（可以理解为同步？）Mysql数据，原本Mysql的DDL和DML操作都可以通过ClickHouse执行。 Lazy 在最后一次操作该表之后，仅在固定秒数内将表保留在运存中，该秒数通过expiration_time_in_seconds参数控制。 Atomic 新版本默认的数据库引擎，支持对表非阻塞的drop、rename操作以及原子操作交换两张表。 PostgreSQL 该引擎的特性和MySQL引擎大致相同，只不过是对PostgreSql的支持。 MaterializedPostgreSQL 与MaterializeMySQL一样，是实验阶段的引擎。作为PostgreSql数据表的副本进行后台同步。 Replicated 基于Atomic引擎，通过将DDL操作日志写入ZK并在数据库的所有副本上执行的元数据复制（复制数据库）。可以同时运行和更新多个复制的数据库。但是同一个复制的数据库不能有多个副本。 表引擎（表引擎比数据库引擎稍微复杂一些，表引擎又分为四大系列） MergeTree系列 Log系列 Integrations系列 Special类 MergeTree系列MergeTree系列的表引擎比较常用，它支持主键、数据分区、数据副本和数据采样等特性，同时支持重要的alter操作语句。 ReplacingMergeTree 清洗数据，去除重复数据。当数据重复时数据去重策略有两种：通过ReplacingMergeTree({version})建立version字段则保留version字段最大值，否则则保留最后一行。 SummingMergeTree 按照预先定义的聚合条件汇总数据 AggregatingMergeTree SummingMergeTree引擎的进阶版。能够在合并分区时，按照预先定义的聚合条件汇总数据。 CollapsingMergeTree 支持行级数据的更新和删除，通过指定字段CollapsingMergeTree({sign})标记数据行的状态（状态值：1，-1）。分区合并时，同一个数据分区内sign值为1的数据行会和下一行sign值为-1的抵消删除。如果并发写入时则会导致乱序问题，即sign值1和-1的数据行并不相临的问题。 VersionedCollapsingMergeTree CollapsingMergeTree引擎的进阶版。通过VersionedCollapsingMergeTree({sign},{version})新增version字段解决乱序问题。 GraphiteMergeTree 存储时序数据库Graphite的数据。 Log系列Log引擎面向的应用场景一般是：小数据量、多写少查、单次查询数据较大。 StripLog 支持多线程并发读取文件（data.bin），数据写入时会将所有数据列存储在同一个文件中。 TinyLog 数据写入时每个数据列存储在不同的文件（{column}.bin），但不支持多线程并发读取文件。 Log 支持多线程并发读取文件，写入时每个数据列单独存储文件。 Integrations系列该类引擎提供多种方式与外部系统集成（外部表），支持包括但不仅限于： ODBC JDBC Mysql MongoDB HDFS S3 Kafka EmbeddedRocksDB RabbitMQ PostgreSQL SQLite Hive Special类这个分类下的引擎适用于定制化的业务场景。 内存类 Memory 将数据存储在内存中，但不提供持久化操作，重启数据就跑丢了。 Set 适用于集合类型的查询操作，比如：in。 Join 望文生义，为了连表查询。 Buffer 通俗的说像是数据脏页设置一个刷新阈值，达到即刷新到磁盘中。 中间件类 Distributed 自动分片、自动写入和查询数据。 Dictionary 自动为数据字典创建数据表。 Merge 合并多个相同表结构的查询结果。 其他 Null 写入的数据会被丢弃 … 暂时不懂什么用途，官方的描述是适用于视图展示。 Url 配合RESTful接口，操作会转化为：insert =&gt; post、select =&gt; get。 Live View 实验阶段的引擎，对事件监听进行实时数据计算。适用场景比如：证券网站。 总结简单的浏览了ClickHouse不同的数据引擎，接下来会学习存储结构和索引方式（搞清楚为什么查询这么快）。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"clickhouse","slug":"clickhouse","permalink":"http://xupin.im/tags/clickhouse/"}]},{"title":"ClickHouse学习笔记 - 数组拆分再分组","date":"2022-01-13T16:00:00.000Z","path":"2022/01/14/clickhouse-array-split-group/","text":"ClickHouse是个由老毛子国内市场使用率第一的搜索引擎Yandex开源的OLAP型列式数据库。 背景最近，数据分析项目要做一个广告素材推广成效分析功能。需求评审下来大概是通过给广告素材打标签进行分类，然后按标签纬度进行分析数据。 数据库选型该项目之前的数据是存放在Elasticsearch，但总是感觉ES的DSL语句过于繁琐不利于阅读且学习成本较高，内部针对该问题讨论之后选用了在大数据处理上同样出色的ClickHouse作为新功能的数据仓库（试点功能233）。 数据结构因为是列式数据库，所以不需要遵守数据库三大范式。 伪表结构：1234567891011121314CREATE TABLE creative_daily_summary( `date` Date, `creative_id` String, `tags` Array(String), `revenue` Float64, `cost` Float64, `impressions` Int64, `clicks` Int64, `installs` Int64)ENGINE = MergeTreeORDER BY dateSETTINGS index_granularity = 8192 这里因为考虑数据冗余的问题，标签存储为数组类型（所以也就出现了这篇笔记）。 存储一些数据：12345INSERT INTO creative_daily_summary (`date`,creative_id,tags,revenue,cost,impressions,clicks,installs) VALUES ('2022-01-07','1721',['代入/代入角色/屏幕外角色','玩法/装修/修理','玩法/模拟经营/建筑升级','玩法/小游戏/情景选择(废除)','元素/其他/人手','元素/其他/表情包','元素/场景/农场'],0.0,2.41,2204,0,0);INSERT INTO creative_daily_summary (`date`,creative_id,tags,revenue,cost,impressions,clicks,installs) VALUES ('2022-01-07','1721',['代入/代入角色/屏幕外角色','玩法/装修/修理','玩法/模拟经营/建筑升级','玩法/小游戏/情景选择(废除)','元素/其他/人手','元素/其他/表情包','元素/场景/农场'],0.0,0.05,9,1,0);INSERT INTO creative_daily_summary (`date`,creative_id,tags,revenue,cost,impressions,clicks,installs) VALUES ('2022-01-08','1788',['代入/视角镜头/第一人称','剧情/情感共鸣/育儿','剧情/援助/解救','剧情/困境/自然灾害','玩法/模拟经营/摘果','玩法/模拟经营/引水','玩法/模拟经营/建筑升级','玩法/改变地形/挖河道'],0.0,130.64,31868,63,7);INSERT INTO creative_daily_summary (`date`,creative_id,tags,revenue,cost,impressions,clicks,installs) VALUES ('2022-01-09','1788',['代入/视角镜头/第一人称','剧情/情感共鸣/育儿','剧情/援助/解救','剧情/困境/自然灾害','玩法/模拟经营/摘果','玩法/模拟经营/引水','玩法/模拟经营/建筑升级','玩法/改变地形/挖河道'],0.0,0.04,76,0,0);INSERT INTO creative_daily_summary (`date`,creative_id,tags,revenue,cost,impressions,clicks,installs) VALUES ('2022-01-10','2124',['代入/代入角色/固定角色-女性','剧情/情感共鸣/购物焦虑','目的/目的/拉新','形式/技术形式/3D','元素/场景/城市街道','元素/场景/室内公共场所'],0.0,0.02,5,0,0); date creative_id tags revenue cost impressions clicks installs 2022-01-07 1721 [‘代入/代入角色/屏幕外角色’,’玩法/装修/修理’,’玩法/模拟经营/建筑升级’,’玩法/小游戏/情景选择(废除)’,’元素/其他/人手’,’元素/其他/表情包’,’元素/场景/农场’] 0.0 2.41 2204 0 0 2022-01-07 1721 [‘代入/代入角色/屏幕外角色’,’玩法/装修/修理’,’玩法/模拟经营/建筑升级’,’玩法/小游戏/情景选择(废除)’,’元素/其他/人手’,’元素/其他/表情包’,’元素/场景/农场’] 0.0 0.05 9 1 0 2022-01-08 1788 [‘代入/视角镜头/第一人称’,’剧情/情感共鸣/育儿’,’剧情/援助/解救’,’剧情/困境/自然灾害’,’玩法/模拟经营/摘果’,’玩法/模拟经营/引水’,’玩法/模拟经营/建筑升级’,’玩法/改变地形/挖河道’] 0.0 130.64 31868 63 7 2022-01-09 1788 [‘代入/视角镜头/第一人称’,’剧情/情感共鸣/育儿’,’剧情/援助/解救’,’剧情/困境/自然灾害’,’玩法/模拟经营/摘果’,’玩法/模拟经营/引水’,’玩法/模拟经营/建筑升级’,’玩法/改变地形/挖河道’] 0.0 0.04 76 0 0 2022-01-10 2124 [‘代入/代入角色/固定角色-女性’,’剧情/情感共鸣/购物焦虑’,’目的/目的/拉新’,’形式/技术形式/3D’,’元素/场景/城市街道’,’元素/场景/室内公共场所’] 0.0 0.02 5 0 0 检索效果产品期望实现的需求是统计“标签”的数据，但这个“标签”并不是完整的标签（代入/代入角色/屏幕外角色），而是标签的一部分（比如代入、代入角色、屏幕外角色等，拆分开来的），同时一个广告素材如果满足多个标签即拆分成多条数据。 思路1 利用子查询再分组，这样会有数据重复的问题，所以需要再套一层查询去重。12345678910111213141516171819202122232425262728293031323334select date, tag, sum(cost) as costfrom ( select date, tag, creative_id, sumDistinct(cost) as cost from ( select date, arrayJoin(tags) as t, concat(arrayElement(splitByChar('/',t),1),'/',arrayElement(splitByChar('/',t),2)) as tag, creative_id, sum(cost) as cost from creative_daily_summary where tag in ('代入/代入角色','玩法/模拟经营') group by t, creative_id, date) group by tag, creative_id, date)group by tag, date date tag cost 2022-01-08 玩法/模拟经营 130.64 2022-01-07 代入/代入角色 2.46 2022-01-10 代入/代入角色 0.02 2022-01-09 玩法/模拟经营 0.04 2022-01-07 玩法/模拟经营 2.46 思路2 因为深感第一种方式太挫，效率不高且毫无美感（最重要的是总感觉解决的方向不对）。1234567891011select date, arrayJoin(arrayDistinct(arrayMap(tag -&gt; concat(arrayElement(splitByChar('/',tag),1),'/',arrayElement(splitByChar('/',tag),2)),tags))) as tag, sum(cost) as costfrom creative_daily_summarywhere tag in('代入/代入角色','玩法/模拟经营')group by date, tag date tag cost 2022-01-10 代入/代入角色 0.02 2022-01-07 代入/代入角色 2.46 2022-01-08 玩法/模拟经营 130.64 2022-01-07 玩法/模拟经营 2.46 2022-01-09 玩法/模拟经营 0.04 总结接触新的事物还是要抱着谦逊的态度先系统的了解一下优势和缺点，不然解决问题时总会陷入定式思维的怪圈（在错误的方向浪费时间和精力）。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"clickhouse","slug":"clickhouse","permalink":"http://xupin.im/tags/clickhouse/"}]},{"title":"Go学习笔记 - 控制协程数量","date":"2021-12-02T16:00:00.000Z","path":"2021/12/03/go-coroutine-number/","text":"之前因为需要并发同步单张数据表的脚本，因为设置协程数量太多连接数过高导致服务宕机 … 得不偿失，所以想怎么控制一下协程数量 Channel利用channel阻塞，上代码： 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"sync\" \"time\")func main() &#123; // 允许最大协程数 ch := make(chan int, 5) wg := &amp;sync.WaitGroup&#123;&#125; for i := 0; i &lt; 10; i++ &#123; wg.Add(1) ch &lt;- i // ch长度等于5时，阻塞等待 go func(i int) &#123; defer wg.Done() fmt.Println(\"记录执行 \", i) time.Sleep(time.Second * 3) fmt.Println(\"执行完毕 \", &lt;-ch) &#125;(i) &#125; wg.Wait()&#125; 原理就是利用channel通道的特性，当通道满的时候继续向通道丢数据会阻塞代码执行。 封装一下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mainimport ( \"fmt\" \"sync\" \"time\")type Worker struct &#123; ch chan interface&#123;&#125; wg *sync.WaitGroup&#125;func main() &#123; // 允许最大协程数 worker := NewWorker(5) for i := 0; i &lt; 10; i++ &#123; worker.Run(i) go func(i int) &#123; fmt.Println(\"记录执行 \", i) time.Sleep(time.Second * 3) key := worker.Done() fmt.Println(\"执行完毕 \", key) &#125;(i) &#125; worker.Wait()&#125;func NewWorker(size int) *Worker &#123; return &amp;Worker&#123; ch: make(chan interface&#123;&#125;, size), wg: &amp;sync.WaitGroup&#123;&#125;, &#125;&#125;func (w *Worker) Run(key interface&#123;&#125;) &#123; w.ch &lt;- key w.wg.Add(1)&#125;func (w *Worker) Done() interface&#123;&#125; &#123; key := &lt;-w.ch w.wg.Done() return key&#125;func (w *Worker) Wait() &#123; w.wg.Wait()&#125; Goroutine Pool感觉Duck不必 … 日后再谈","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"Go学习笔记 - 并发查询单表","date":"2021-12-01T16:00:00.000Z","path":"2021/12/02/go-coroutine-query/","text":"最近接触一个项目，需要每天定时把PgSql一张数据量超大的数据表同步到ClickHouse 思考这种场景下怎么提高同步速度。 按主键分页思路 按照主键排序分区间查询 代码123456789101112131415161718192021222324252627282930313233package mainimport ( \"fmt\" \"math\" \"strconv\" \"sync\")func main() &#123; maxId := 203202 pageNum := 1500 num, _ := strconv.ParseFloat(fmt.Sprintf(\"%.2f\", float64(maxId)/float64(pageNum)), 64) maxPage := int(math.Ceil(num)) wg := &amp;sync.WaitGroup&#123;&#125; for i := 0; i &lt; maxPage; i++ &#123; wg.Add(1) go func(i int) &#123; defer wg.Done() startId := (i * pageNum) endId := startId + pageNum if endId &gt; maxId &#123; endId = maxId &#125; sql := fmt.Sprintf(\"select * from logs where id &gt;= %d and id &lt;= %d\", startId, endId) fmt.Printf(\"第 %d 页 SQL: %s\\n\", i+1, sql) &#125;(i) &#125; wg.Wait()&#125; 优点 命中索引查询 缺点 PgSql设置主键会影响插入和查询速度，这也是为什么PgSql数据表一般不设置主键的原因 游标思路 多个协程获取同一个游标的数据 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package mainimport ( \"fmt\" \"project/postgresql\" \"math\" \"strconv\" \"sync\")func main() &#123; // pgSql client cli := postgresql.GetClient() // trx tx, err := cli.Begin() if err != nil &#123; panic(err) &#125; // 计算多少个coroutine cnt := 0 cli.QueryRow(\"select count(*) from logs\").Scan(&amp;cnt) pageNum := 1500 num, _ := strconv.ParseFloat(fmt.Sprintf(\"%.2f\", float64(cnt)/float64(pageNum)), 64) maxPage := int(math.Ceil(num)) // cursor cursorName := \"cursor_name\" cursorQuery := \"select * from logs\" tx.Query(\"DECLARE \" + cursorName + \" CURSOR FOR \" + cursorQuery) wg := &amp;sync.WaitGroup&#123;&#125; lock := &amp;sync.Mutex&#123;&#125; for i := 1; i &lt;= maxPage; i++ &#123; wg.Add(1) go func(i int) &#123; defer wg.Done() // 互斥锁 lock.Lock() // 每次取&#123;pageNum&#125;条 rows, err := tx.Query(\"FETCH FORWARD \" + strconv.Itoa(pageNum) + \" FROM \" + cursorName) if err != nil &#123; panic(err) &#125; defer rows.Close() // 处理迭代逻辑 // for rows.Next() &#123; // &#125; fmt.Printf(\"第 %d 页 结果: %v\\n\", i, rows) // 解锁 // TODO 一定要迭代完成后再解锁 lock.Unlock() &#125;(i) &#125; wg.Wait() tx.Commit()&#125; 优点 能保证数据的完整，可靠性较高 缺点 必须等待结果集迭代完成后才能处理下一个 为啥不用 offset, limit因为offset, limit会扫全表 233","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"Python学习笔记 - yield","date":"2021-11-21T16:00:00.000Z","path":"2021/11/22/python-yield/","text":"在程序中除了迭代器还有生成器均是可迭代对象，迭代器是通过移动游标来遍历数据，那生成器呢 生成器生成器创建方式（python） (表达式)123456generator = (i for i in [1, 2, 3]) // 这里有区别于 [i for i in [1, 2, 3]]print(generator) # &lt;generator object &lt;genexpr&gt; at 0x10c312870&gt;print(next(generator)) # 1print(next(generator)) # 2print(next(generator)) # 3print(next(generator)) # raise StopIteration yield1234567891011def getGenerator(): for i in [1, 2, 3]: yield i print(\"done\")generator = getGenerator()print(generator) # &lt;generator object getGenerator at 0x10c3128c0&gt;print(next(generator)) # 1print(next(generator)) # 2print(next(generator)) # 3print(next(generator)) # done &amp; raise StopIteration 原理当yield出现在函数中（yield只能定义在函数中），那么调用时不会直接运行函数而是返回一个生成器对象，生成器也是一个特殊的迭代器（实现了迭代器协议的对象）。 第一次调用next()函数，遇见yield停下返回yield后面的内容 再次调用next()函数，从上次yield语句处恢复，如果还存在yield则正常执行，否则函数体执行完毕抛出StopIteration异常 通俗的来说：yield相当于return，但yield会记住这个返回位置，再次执行会从这个位置开始。 生成器中send()是和next()一样能让生成器恢复执行的函数，不同的是send(arg)函数可以传递参数12345678910111213def getGenerator(): for i in [1, 2, 3]: print(\"before: \", i) t = yield i print(\"after: \", i, t)generator = getGenerator()print(generator) # &lt;generator object getGenerator at 0x10c3128c0&gt;generator.send(None) # before: 1next(generator) # after: 1 None &amp; before: 2generator.send(22) # after: 2 22 &amp; before: 3generator.send(33) # after: 3 33 &amp; raise StopIteration 注意第一次调用生成器不可以直接传递非None参数，因为yield还未准备好（无返回值），可以generator.send(None)或next(generator)。具体可以参考python源码:12const char *msg = \"can't send non-None value to a \" \"just-started generator\"; https://github.com/python/cpython/blob/3.9/Objects/genobject.c#L181 迭代协议什么是可迭代协议？ 当处理迭代时首先调用可迭代对象.__iter__()，返回迭代器。然后通过迭代器.__next__()获取迭代器的元素，直至抛出StopIteration。 123456789101112131415161718192021222324252627class CustomGenerator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): print(\"look at me\") return self def __next__(self): if self.index &lt; len(self.data): val = self.data[self.index] self.index += 1 return val else: raise StopIterationgenerator = CustomGenerator([1, 2, 3])print(generator) # &lt;__main__.CustomGenerator object at 0x1047161f0&gt;for x in generator: print(x)# look at me# 1# 2# 3print(next(generator)) # raise StopIteration","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"python","slug":"python","permalink":"http://xupin.im/tags/python/"}]},{"title":"Mysql学习笔记 - cursor","date":"2021-11-17T16:00:00.000Z","path":"2021/11/18/mysql-cursor/","text":"游标（cursor）是可以从数据库检索结果集中每次提取一行记录的机制，游标的作用就是对结果集进行遍历，方便对记录进行操作。 特性三种特性： 只读 不能更新它 非滚动 根据SQL语句确定顺序，不能向后获取记录和跳过记录 敏感/不敏感 敏感游标指向的是数据表，不敏感游标指向的则是临时数据表。（Mysql使用的是敏感游标） 声明游标游标使用的变量必须在声明游标之前声明 1234567891011121314151617// Mysql// 不同于其他数据库，Mysql的游标必须声明在存储过程/函数中。create procedure cur_func()begin declare &#123;...VARs&#125; // e.g. declare email varchar(50) declare &#123;CUR_NAME&#125; cursor for &#123;STATEMENT&#125;; open &#123;CUR_NAME&#125;; fetch &#123;CUR_NAME&#125; into &#123;...VARs&#125; // e.g. fetch cur1 into email,name // 取出1行记录 &amp; 把值赋给变量email、name close &#123;CUR_NAME&#125;;end;// PgSqlbegin declare &#123;CUR_NAME&#125; cursor for &#123;STATEMENT&#125;; fetch &#123;STATEMENT&#125; from &#123;CUR_NAME&#125; // e.g. fetch forward 10 from cur1 取出10行记录 close &#123;CUR_NAME&#125;;end; 原理游标有着类似指针的作用，它是遍历容器的一套接口（迭代器），简单实现（golang）: 123456789101112131415161718192021222324252627282930313233343536373839404142package mainimport \"fmt\"type CustomArray []interface&#123;&#125;type Iterator struct &#123; data CustomArray index int&#125;func main() &#123; cusArr := CustomArray&#123;\"1_1\", 22, 3.3&#125; for iter := cusArr.iterator(); iter.hasNext(); &#123; fmt.Println(iter.key(), iter.next()) &#125;&#125;// 迭代器func (arr CustomArray) iterator() *Iterator &#123; return &amp;Iterator&#123; data: arr, index: 0, &#125;&#125;// 游标func (iter *Iterator) key() int &#123; return iter.index&#125;// 下一个元素是否存在func (iter *Iterator) hasNext() bool &#123; return iter.index &lt; len(iter.data)&#125;// 取出数据&amp;移动游标func (iter *Iterator) next() interface&#123;&#125; &#123; val := iter.data[iter.index] iter.index += 1 return val&#125; 简单的说：游标返回一个数据集合迭代器，当你取一行记录同时游标指向下一个元素。 举例使用游标的DB库，以下：https://github.com/PyMySQL/PyMySQL/blob/46d17402af/pymysql/cursors.py#L278https://github.com/golang/go/blob/master/src/database/sql/sql.go#L2983","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - InnoDB并发死锁问题","date":"2021-11-14T16:00:00.000Z","path":"2021/11/15/mysql-deadlock/","text":"项目中某个定时任务执行过程中出现死锁问题，具体错误如下:1Error 1213: Deadlock found when trying to get lock; try restarting transaction 分析检查一下表引擎 &amp; 事务隔离级别123456789101112131415161718192021mysql&gt; show variables like '%storage_engine%';+---------------------------------+-----------+| Variable_name | Value |+---------------------------------+-----------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || disabled_storage_engines | || internal_tmp_mem_storage_engine | TempTable |+---------------------------------+-----------+4 rows in set (0.00 sec)mysql&gt; select @@global.tx_isolation;ERROR 1193 (HY000): Unknown system variable 'tx_isolation' // Mysql8.0以后取消了该变量mysql&gt; select @@transaction_isolation;+-------------------------+| @@transaction_isolation |+-------------------------+| REPEATABLE-READ |+-------------------------+1 row in set (0.00 sec) 默认使用的引擎是InnoDB，Insert、Update、Delete操作会触发排它锁（X锁）即行锁，所以怀疑是不是触发了共享锁（S锁）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120mysql&gt; show engine innodb status;=====================================2021-11-15 02:42:28 0x2b1f5b160700 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 59 seconds...------------------------LATEST DETECTED DEADLOCK------------------------2021-11-12 14:21:02 0x2b204ca06700*** (1) TRANSACTION:TRANSACTION 3805455, ACTIVE 0 sec insertingmysql tables in use 1, locked 1LOCK WAIT 4 lock struct(s), heap size 1136, 3 row lock(s), undo log entries 1MySQL thread id 875327, OS thread handle 47411963238144, query id 20765167 10.85.62.212 dsc updateINSERT INTO `user_tag_distributes` (`created_at`,`updated_at`,`deleted_at`,`app_id`,`tag_name`,`tag_value`,`account_num`) VALUES ('2021-11-12 14:21:02.386','2021-11-12 14:21:02.386',NULL,'im','churn_model_v2','0.70709264',1)*** (1) HOLDS THE LOCK(S):RECORD LOCKS space id 334 page no 341 n bits 160 index PRIMARY of table `api`.`user_tag_distributes` trx id 3805455 lock_mode X locks rec but not gapRecord lock, heap no 89 PHYSICAL RECORD: n_fields 11; compact format; info bits 0 0: len 8; hex 000000000002515d; asc Q];; 1: len 6; hex 0000003a110f; asc : ;; 2: len 7; hex 81000001960110; asc ;; 3: len 7; hex 99ab18e5420f14; asc B ;; 4: len 7; hex 99ab18e5420f14; asc B ;; 5: SQL NULL; 6: SQL NULL; 7: len 2; hex 696d; asc im;; 8: len 14; hex 636875726e5f6d6f64656c5f7632; asc churn_model_v2;; 9: len 10; hex 302e3730373039323634; asc 0.70709264;; 10: len 3; hex 800001; asc ;;*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 334 page no 203 n bits 392 index tag of table `api`.`user_tag_distributes` trx id 3805455 lock mode S waitingRecord lock, heap no 88 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 14; hex 636875726e5f6d6f64656c5f7632; asc churn_model_v2;; 1: len 9; hex 302e37303730393735; asc 0.7070975;; 2: len 8; hex 0000000000022d5f; asc -_;;*** (2) TRANSACTION:TRANSACTION 3805441, ACTIVE 0 sec fetching rowsmysql tables in use 1, locked 1LOCK WAIT 89 lock struct(s), heap size 24784, 17767 row lock(s), undo log entries 10247MySQL thread id 875354, OS thread handle 47415383693056, query id 20765093 10.85.62.212 dsc updatingDELETE FROM `user_tag_distributes` WHERE app_id = 'im' and tag_name = 'churn_model_v2'*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 334 page no 203 n bits 392 index tag of table `api`.`user_tag_distributes` trx id 3805441 lock_mode X locks rec but not gapRecord lock, heap no 88 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 14; hex 636875726e5f6d6f64656c5f7632; asc churn_model_v2;; 1: len 9; hex 302e37303730393735; asc 0.7070975;; 2: len 8; hex 0000000000022d5f; asc -_;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 334 page no 341 n bits 160 index PRIMARY of table `api`.`user_tag_distributes` trx id 3805441 lock_mode X waitingRecord lock, heap no 89 PHYSICAL RECORD: n_fields 11; compact format; info bits 0 0: len 8; hex 000000000002515d; asc Q];; 1: len 6; hex 0000003a110f; asc : ;; 2: len 7; hex 81000001960110; asc ;; 3: len 7; hex 99ab18e5420f14; asc B ;; 4: len 7; hex 99ab18e5420f14; asc B ;; 5: SQL NULL; 6: SQL NULL; 7: len 2; hex 696d; asc im;; 8: len 14; hex 636875726e5f6d6f64656c5f7632; asc churn_model_v2;; 9: len 10; hex 302e3730373039323634; asc 0.70709264;; 10: len 3; hex 800001; asc ;;*** WE ROLL BACK TRANSACTION (1)------------TRANSACTIONS------------Trx id counter 3945122Purge done for trx's n:o &lt; 3945122 undo n:o &lt; 0 state: running but idleHistory list length 3LIST OF TRANSACTIONS FOR EACH SESSION:---TRANSACTION 328886640459544, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640456152, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640455304, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640457000, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640448520, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640447672, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640452760, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640458696, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640446824, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640453608, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640457848, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640451912, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640451064, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640454456, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640450216, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640449368, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640445976, not started0 lock struct(s), heap size 1136, 0 row lock(s)---TRANSACTION 328886640445128, not started0 lock struct(s), heap size 1136, 0 row lock(s)...----------------------------END OF INNODB MONITOR OUTPUT============================ 通过以上信息确认是因为[事务1]Insert &amp; [事务2]Delete互相等待对方事务释放锁的问题造成死锁，最后[事务1]进行回滚。 原因review代码梳理一下业务逻辑，场景应该如下： trx1 trx2 BEGIN BEGIN DELETE FROM user_tag_distributes WHERE app_id = ‘im’ and tag_name = ‘churn_model_v2’ DELETE FROM user_tag_distributes WHERE app_id = ‘im’ and tag_name = ‘churn_model_v2’ INSERT INTO user_tag_distributes (created_at,updated_at,deleted_at,app_id,tag_name,tag_value,account_num) VALUES (‘2021-11-12 14:21:02.386’,’2021-11-12 14:21:02.386’,NULL,’im’,’churn_model_v2’,’0.70709264’,1) ERROR 1213 (40001): Deadlock found when trying to get lock ROLLBACK … COMMIT 可以看出[事务1]进行Insert操作时发现[事务2]进行Delete操作且已申请X锁，[事务1]想要获取S锁则需要[事务2]提交，所以[事务1]、[事务2]在相互等待对方提交事务（释放锁）。 解决方案目前想到的几种措施： 尽量避免在并发程序中Delete &amp; Insert操作无缝执行 并发程序采用分布式锁控制 程序不允许并发执行 https://dev.mysql.com/doc/refman/8.0/en/mysql-nutshell.htmlhttps://dev.mysql.com/doc/refman/8.0/en/innodb-standard-monitor.html","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 数据迁移","date":"2021-11-11T16:00:00.000Z","path":"2021/11/12/mysql-migrate/","text":"之前项目数据表有分表设计，现在因为业务量增长数据表数据量膨胀数倍导致单表性能不理想 … 所以希望在不影响用户使用的情况下扩展数据表（迁移） 数据迁移方案基本分为热迁移和冷迁移两种方式 冷迁移 通俗的来说就是停机迁移，比如某些游戏合服维护等 优点: 操作可靠性高、数据一致性有保证、数据回滚方便 缺点: 用户体验差 热迁移 在不停机的情况下把数据表迁移，新旧表双写 优点: 用户无感知 缺点: 容易丢失数据、数据一致性不好保证 这种热迁移方式是不依赖第三方服务，纯靠DB完成迁移，极端场景下可靠性不确定。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Git小知识 - depth","date":"2021-10-31T16:00:00.000Z","path":"2021/11/01/git-depth/","text":"有时候git项目比较大，git clone会因为各种原因中断，但是Git并不会断点续“传”再次clone又是重新来过,类似这种问题…让人很是苦恼123$ error: RPC failed; HTTP 504 curl 22 The requested URL returned error: 504 Gateway Time-out$ ...$ error: RPC failed; curl 18 transfer closed with outstanding read data remaining 解决方式 git配置流 1234$ git config --global http.lowSpeedLimit 0 // 最小速度$ git config --global http.lowSpeedTime 999999 // 最大速度$ git config --global http.postBuffer 524288000 // 文件大小$ git config --global compression 0 // 关闭压缩 分块拉取 1$ git clone --depth=1 &#123;repo&#125; // 拉取最新的代码（最后一次commit） 但这样会带来其他的小问题，就是拉下来的代码默认分支既不是master也不是其他分支 … 需要拉取完整的项目123$ git fetch --unshallow // 拉取深层代码$ git config remote.origin.fetch \"+refs/heads/*:refs/remotes/origin/*\" // 修正remote关系（修改之前: \"fetch = +refs/heads/master:refs/remotes/origin/master\"）$ git fetch -pv // 拉取所有分支","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"git","slug":"git","permalink":"http://xupin.im/tags/git/"},{"name":"biscuits","slug":"biscuits","permalink":"http://xupin.im/tags/biscuits/"}]},{"title":"设计模式 - 控制反转/依赖注入","date":"2020-10-17T16:00:00.000Z","path":"2020/10/18/design-ioc-di/","text":"Laravel框架中控制反转和依赖注入功能是怎么实现的？其实控制反转和依赖注入是一种设计思想，它最早源于Java Spring框架设计中的机制，所以身边如果有做Java开发的小伙伴~简单聊聊就能明白许多。 1.依赖注入依赖注入（Dependency Injection），通俗的解释是当我们构建对象时需要的参数，只要不是手动创建而是以实例对象的形式注入都可以称为依赖注入。 举个例子，一个小游戏中玩家可以创建不同的职业。 1234567891011121314151617181920212223242526272829303132333435363738394041// 职业接口interface Role &#123; // 获取职业名 public function getName();&#125;class Dh implements Role &#123; // 职业名 protected $name = '猎魔人'; // 攻击成长 protected $attack; // 速度成长 protected $speed; // 气血成长 protected $hp; public function __construct($attack,$speed,$hp) &#123; $this-&gt;attack = $attack; $this-&gt;speed = $speed; $this-&gt;hp = $hp; &#125; public function getName() &#123; return $this-&gt;name; &#125;&#125;// 玩家类class Player &#123; // 职业 protected $role; public function __construct($role) &#123; $this-&gt;role = $role; &#125;&#125;// 猎魔人$dh = new Dh(100, 100, 60);// 玩家1-&gt;猎魔人$player1 = new Player($dh); 以上是创建玩家角色的逻辑，创建角色“player1”时选择角色“dh”，那么这种方式其实就是依赖注入。 2.控制反转（Ioc）控制反转（Inversion of Control），从字面意思来理解就是把控制权反转，那么究竟怎么反转呢？ 之前创建玩家角色的逻辑，通过程序可以看出所有依赖对象都是程序主动去创建（职业对象，玩家对象）。那么如果我们想把所有需要的参数都提前准备好，怎么优化？ 123456789101112131415161718192021222324252627282930313233343536373839class Container&#123; protected $binds; protected $instances; public function bind($abstract, $concrete) &#123; if ($concrete instanceof Closure) &#123; $this-&gt;binds[$abstract] = $concrete; &#125; else &#123; $this-&gt;instances[$abstract] = $concrete; &#125; &#125; public function make($abstract, $parameters = []) &#123; if (isset($this-&gt;instances[$abstract])) &#123; return $this-&gt;instances[$abstract]; &#125; array_unshift($parameters, $this); return call_user_func_array($this-&gt;binds[$abstract], $parameters); &#125;&#125;$container = new Container();$container-&gt;bind('player', function($container, $role) &#123; return new Player($container-&gt;make($role));&#125;);$container-&gt;bind('dh', function($container) &#123; return new Dh(100, 100, 60);&#125;);// 玩家1-&gt;猎魔人$player1 = $container-&gt;make('player', ['dh']); 加入了一个名叫“Container”的类，里面存放提前设计好的对象，当程序需要某些依赖对象时“Container”自动帮你去寻找执行。这其实就是Ioc设计思想中遵循“Don’t call us, we’ll call you”的原则，不需要程序去创建依赖，而是主动去帮程序寻找相应依赖。这也就是所说的控制反转，从狭义上讲依赖注入其实算是控制反转的一种实现。 最后可能很多小伙伴会觉得控制反转的思想和工厂模式很相像，它们是不是就是同一个东西呢？有兴趣的小伙伴可以去看看Dependency Injection vs Factory Pattern。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"design","slug":"design","permalink":"http://xupin.im/tags/design/"}]},{"title":"Mysql学习笔记 - 聚簇索引和回表查询的关系","date":"2020-08-11T16:00:00.000Z","path":"2020/08/12/mysql-clustered-index/","text":"之前简单了解过Mysql的索引，今天来学习一下Mysql（InnoDB）的聚簇索引以及SQL为什么会产生回表查询？ 1. 什么是回表查询？都知道Mysql存储的数据结构是B+Tree，所以当查询数据的时候能最快找到叶子节点的检索方式时是最快的。比如：主键直接定位行记录，而有些查询需要先检索索引树找到叶子节点的主键值，再通过主键值定位行记录这种扫描2次索引树的方式就叫做回表查询。 如何确定SQL语句会不会造成回表查询？如下表：12345678910CREATE TABLE `users` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user` varchar(125) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `name` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, `status` tinyint(4) NOT NULL, `created_at` datetime NOT NULL, PRIMARY KEY (`id`) USING BTREE, UNIQUE INDEX `user`(`user`) USING BTREE, INDEX `name`(`name`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Compact; 2种查询方式： 123456789101112131415# 未回表查询EXPLAIN SELECT id,name FROM users WHERE name = 'test1';+------+-------------+-------+------+---------------+------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+-------+------+---------------+------+---------+-------+------+--------------------------+| 1 | SIMPLE | users | ref | name | name | 202 | const | 1 | Using where; Using index |+------+-------------+-------+------+---------------+------+---------+-------+------+--------------------------+# 回表查询EXPLAIN SELECT id,user,name FROM users WHERE name = 'test1';+------+-------------+-------+------+---------------+------+---------+-------+------+-----------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+------+-------------+-------+------+---------------+------+---------+-------+------+-----------------------+| 1 | SIMPLE | users | ref | name | name | 202 | const | 1 | Using index condition |+------+-------------+-------+------+---------------+------+---------+-------+------+-----------------------+ 注意看Extra字段，当值为Using index condition时表示该SQL需要回表查询，那以上两条SQL到底有什么不同呢？ SQL1命中name索引并且在索引树的叶子节点找到主键id，满足了查询需求所以不需要回表查询其他字段。 SQL2同样是命中name索引并且在索引树叶子节点找到主键id，但还有一个user字段没有得到，所以需要拿着主键id去索引树查询user字段。 以上就是是否回表查询的区别，回表查询会额外产生一次查询的开销，故此效率较低。不过回表查询和聚簇索引又有什么关联呢？为什么回表查询需要遍历2次索引树呢？ 2. 聚簇索引InnoDB的索引类型之前有说过，多数使用B+Tree做索引但在实现上又区分为：聚簇索引和辅助索引。 聚簇索引（Clustered Index） 聚簇索引的叶子节点存储行记录，InnoDB有且只有一个聚簇索引。聚簇索引的每一个叶子节点都有一个指向相邻叶子节点的指针，所以面对Range查询聚簇索引效率很高。 1InnoDB默认主键是聚簇索引，如果没有定义主键则第一个Not Null &amp; Unique索引列为聚簇索引。如果以上条件都不满足，则会生成一个6字节的隐式自增长主键`row-id`。 （这也是为什么InnoDB引擎下要求数据表尽可能都要创建主键的原因。） 辅助索引（Secondary Index） 辅助索引的叶子节点存储主键值（聚簇索引）。 了解了索引树的结构，其实也就明白了为什么明明命中了索引却还会产生回表查询需要扫描2次索引树，即：先扫描辅助索引树拿到主键值，再扫描聚簇索引树获取行记录。 3. 如何避免回表查询？避免回表查询这里有个概念：覆盖索引（Covering index），Mysql官方虽然没有明确定义覆盖索引但是有同样的概念出现。 查询的字段尽可能在一棵索引树都能获取到，避免回表。 （概念出处：Using index） 具体怎么操作呢？直接上SQL。12ALTER TABLE `users` DROP INDEX `name`;ALTER TABLE `users` ADD INDEX `user_name` ( `user`, `name` ); 创建user、name复合索引，这样就能够覆盖索引不需要回表。 最后埋个坑，后面有时间会继续学习相比辅助索引聚簇索引平均会减少多少次IO操作。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Redis学习笔记 - Key事件通知","date":"2020-08-09T16:00:00.000Z","path":"2020/08/10/redis-keyspace-notify/","text":"网上购物看到自己喜欢的东西加入购物车然后付款，但是在付款的突然不想要了 … 往往这个订单会给一个30分钟的支付时间，时间一到就自动关闭了 所以这是怎么做到的呢？ 1.实现方式 轮询 1脚本每隔一定时间就去数据表检查一下状态，是否过期需要关闭。 定时器 1创建订单时开始计时，计时结束后直接处理关闭订单。 数据库事件+存储过程 1数据库建立检查事件，每隔一定时间去执行一次存储过程。 Key事件通知 1对于更改任何Redis Key的每个操作，都可以配置Redis将消息发布到Pub/Sub，然后订阅这些通知。 2.数据库事件+存储过程不多说，直接上SQL 订单表 1234567CREATE TABLE `orders` ( `id` int(11) NOT NULL AUTO_INCREMENT, `good_id` int(11) NOT NULL, `status` tinyint(1) NOT NULL COMMENT '0:待付款，1:已付款，-1:订单关闭', `created_at` datetime NOT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 1 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Compact; 创建存储过程，处理关闭订单。 1234CREATE PROCEDURE job_proce()BEGIN update orders set status = -1 where time_to_sec(timediff(now(), created_at)) &gt; 1800 and status = 0;END 创建事件，调用存储过程。 123456SET GLOBAL event_scheduler = 1; CREATE EVENT IF NOT EXISTS job_eventON SCHEDULE EVERY 1 SECOND # 1秒检查一次ON COMPLETION PRESERVE DO CALL job_proce(); 开启事件。 1ALTER EVENT job_event ON COMPLETION PRESERVE ENABLE; 这样job_event事件就会以1秒/次的频率去执行job_proce存储过程执行数据检查。 3. Key事件通知Key事件通知（Redis Keyspace Notifications）机制自2.8版本以后出现，该机制默认是关闭的。可通过配置redis.conf进行开启1234567891011121314notify-keyspace-events Ex # E表示Key事件通知，x代表Key过期行为。# notify-keyspace-events可配置参数# K 键空间事件，以__keyspace@&lt;db&gt;__前缀发布。# E 键事件事件，以__keyevent@&lt;db&gt;__前缀发布。# g 通用命令（非类型特定），如DEL，EXPIRE，RENAME等等# $ 字符串命令# l 列表命令# s 集合命令# h 哈希命令# z 有序集合命令# x 过期事件（每次键到期时生成的事件）# e 被驱逐的事件（当一个键由于达到最大内存而被驱逐时产生的事件）# A g$lshzxe的别名，因此字符串AKE表示所有的事件。 修改配置开启Key事件通知以后，当Redis在删除过期Key的时候会向指定channel（过期行为的channel：__keyevent@0__:expired） publish消息，该消息可以使用subscribe/psubscribe进行订阅。12345127.0.0.1:6379&gt; PSUBSCRIBE __keyevent@0__:expiredReading messages... (press Ctrl-C to quit)1) \"psubscribe\"2) \"__keyevent@0__:expired\"3) (integer) 1 所以我们只需要写个程序执行redis-&gt;psubscribe()进行监听即可。 最后emmmm…","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Git小知识 - Clone项目速度慢的小技巧","date":"2020-07-04T16:00:00.000Z","path":"2020/07/05/git-cloning-slow/","text":"碰上github网络抽风的时候，恰巧你又需要拉取github上托管的项目，这个时候看着2kb/s的下载速度是不是很抓狂？ 粗暴的解决方式打开ipaddress查询以下三个域名的ip，写到hosts文件中。 github.com github.global.ssl.fastly.net codeload.github.com 进阶的方式 挂代理，哦豁export ALL_PROXY=socks5://127.0.0.1:4000 究极方式 复制你要拉取的项目github地址，比如：https://github.com/v2ray/v2ray-core 打开码云，创建仓库-&gt;从 GitHub / GitLab 导入仓库 搞定！git clone https://gitee.com/xupinbest/v2ray-core","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"github","slug":"github","permalink":"http://xupin.im/tags/github/"},{"name":"biscuits","slug":"biscuits","permalink":"http://xupin.im/tags/biscuits/"}]},{"title":"Go学习笔记 - goroutine","date":"2020-06-11T16:00:00.000Z","path":"2020/06/12/goroutine/","text":"在现在大数据、高并发，到处都充斥着流量的互联网时代，能不能应对高并发俨然已经发展成一个衡量服务端架构是否合格的标准，作为程序媛我们思考如何利用语言在代码层面最优设计能应对并发的程序去并行处理任务。在程序中对于任务并行处理一般趋于使用：进程、线程，以及另外一种：协程。支持协程的语言有很多，比如：C/C++、Ruby、 Python（2.5+）、Golang等等，它们有些是本身语言支持协程，有些则是需要引入第三方包来使用。不过，我们主要来学习一下Golang这门语言（简称Go），它是如何理解以及实现协程的。 一、进程、线程和协程的前世今生都知道一台计算机的核心是CPU，它承担着所有的运算。而计算机承载的操作系统（内核）则是负责所有任务的处理和调度CPU以及资源的分配。如果用人类来比喻，大脑是CPU，思维则是操作系统（内核）。 进程最早的计算机每次只能运行一个程序，如果还有其他程序需要执行则要排队等待。后来CPU运算能力提高了，这种方式过于原始有些浪费性能，于是尝试让多个程序可以并行执行，但是这样面临一个新的问题：跑在同一个CPU中的程序都会使用计算机资源，那程序的运行状态和数据怎么保障？进程。1进程是内核资源管理分配的最小单位，每个进程都有独立的虚拟地址空间。内核中的每个程序都运行在独立进程的上下文中，上下文是由程序正常运行需要的一系列参数组成，参数包括存储器中的代码和数据，寄存器中的内容以及进程打开的文件描述符（文件句柄）等。可以把上下文通俗理解为：`环境`。 如果程序在运行过程中需要进行IO操作，IO操作阻塞了程序后面的计算，这时候CPU属于空闲状态，那内核会把CPU切换到其他进程去处理。不过当进程数量变高以后，计算机的大部分资源都被进程切换这个操作消耗掉了。为什么说进程切换操作消耗资源代价比较高？1所谓进程切换其实就是上下文切换，需要切换新的页表并加载新的虚拟地址空间、切换内核栈以及硬件上下文等。只要发生进程切换操作就得反复进入内核，加载切换一系列状态。 线程为了减少这种开销，线程应运而生。1线程是内核调度CPU执行的最小单位，线程是运行在进程上下文的逻辑流，线程是具体执行程序的单位。一个进程至少包含一个主线程（可以拥有多个子线程），但是一个线程只能存在于一个进程中。 线程切换相比进程切换开销就小了很多，线程切换只需要把寄存器刷新即可。 协程后面程序媛们发现线程这样还是有性能瓶颈（IO阻塞），无论是进程还是线程因为涉及到大量的计算机资源，所以都是由内核调度管理。能不能开发一种由代码控制的线程呢？这就是协程。 1协程是由用户控制的线程（用户态线程），协程在程序中实现自我调度，不需要像进程切换一样进入内核加载切换状态，提高了线程在IO上的性能问题（IO多路复用）。 但是协程也有个致命的问题，假如进程中的某一程序出现了阻塞操作同时被CPU中断处理（抢占式调度），那么该进程中的所有线程都会被阻塞。 后面会专门写一篇关于进程和线程以及协程的特性以及区别。（不够详细，埋坑Orz~） 二、什么是goroutine？上面简单学习了进程和线程以及协程的渊源，虽然不够详细但是我们大概知道其实在进程或者线程甚至于协程存在的性能瓶颈大部分是CPU调度问题。 1go func() // Go语言启动协程，只需要使用go关键字即可启动协程运行函数。使用go关键字创建的这个协程就叫做`goroutine` 之前说了goroutine是Go语言的协程，其实这么理解是可以的但goroutine比协程更强大。它们使用的线程模型有着本质的区别，如下： goroutine通过通道来通信，而协程通过让出执行和恢复操作来通信。 goroutine通过Go语言的调度器进行调度，而协程通过程序本身调度。 大部分语言或者第三方库提供的协程就是使用的用户态线程模型，但goroutine使用的不是传统的用户态线程模型。以下主流的线程模型： 内核级线程模型 内核级别的线程的状态切换需要内核直接处理，所以内核清楚的知道每一个KSE（Kernel Scheduling Entity）的存在（即内核线程和KSE是一对一关系），它们可以全系统内进行资源的竞争。 用户级线程模型（用户态线程，协程） 用户态级别的线程受用户控制，内核并不直接知道用户态线程的存在（为什么这么说？因为用户态线程和内核线程存在着多对一的关系，即多个用户态线程对应一个内核线程），一般用户态多线程属于同一个进程，所以它们只能在进程内进行资源竞争。 两级线程模型（混合型线程模型） 两级线程模型吸取了内核级和用户级线程的经验，两级线程模型下的线程和内核线程处于多对多的关系。一个进程内的多个线程可以分别绑定内核线程，既可以多个线程绑定多个内核线程也可以多个线程绑定一个内核线程，当某个线程内的程序产生阻塞其绑定的内核线程被CPU中断处理，进程内的其他线程可以重新与其他内核线程绑定。 goroutine使用的正是两级线程模型，但是这种多个线程跑在多个内核中，既不是用户级线程模型完全靠自身调度也不是内核级线程模型完全依赖内核调度，而是用户和内核协同调度。因为这种模型复杂性较高，所以Go语言开发了自己的runtime调度器。 三、Go runtime调度器Go runtime调度器的结构由三部分组成: G Goroutine，每个goroutine有对应的G结构体，G结构体储存goroutine的上下文信息。G并不能直接被调度，需要绑定对应的P才能被调度执行。 P Processor，为G和M进行调度的逻辑处理器，对于G来说P像是内核，而在M看来P相当于上下文。P的数量可以在程序中代码控制，如下: 1runtime.GOMAXPROCS(runtime.NumCPU()) // 该值最大为256。 M Machine，负责调度任务（可以理解为内核线程的抽象），代表着操作系统内核，是真正处理任务的服务。M的数量不是固定的，受Go runtime调度器控制。（不过该值最大为10000，可以参考：src/runtime/proc.go） 值得一提的是Go语言在最初的版本中Go runtime调度器的结构是GM模型（并非GMP模型），P服务是因为GM模型在并发上出现很大的性能损耗。有兴趣的小伙伴可以看一下Go runtime的核心开发者Dmitry Vyukov发现的问题Scalable Go Scheduler Design Doc。 简单说了一下Go runtime调度器中的GPM模型的概念，那么GPM究竟是怎么调度的呢？首先当通过go func()创建一个G对象的时候，G会被优先放入P的本地队列。为了执行G M需要绑定一个P，然后M启动内核线程并循环从P的本地队列取出G并执行。 当P发现当前绑定的M被阻塞时会转入绑定其他M（新的M可能是被创建或者从内核线程缓存中取出）。 如果M处理完了当前P的本地队列里的G后，P会尝试从全局队列里取G来执行（同样P也会周期性的检查全局队列是否有G可以执行）。如果全局队列没有可以执行的G，P会随机挑选另外一个P并从它的本地队列中取出一半G到自己的本地队列中执行。这个动作使用调度算法work-stealing（工作窃取算法）实现。 最后以上就是Go runtime调度器的运行原理（大概），后面有更深入的理解会补充进来。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"goroutine","slug":"goroutine","permalink":"http://xupin.im/tags/goroutine/"}]},{"title":"Mysql小知识 - 查找连续编号中的缺失编号","date":"2020-04-12T16:00:00.000Z","path":"2020/04/13/mysql-trick/","text":"在和小伙伴讨论问题的时候，小伙伴突然问了我这样一个小问题，数据库中如何查找连续编号中的缺失编号？ 1.描述场景大概是这样，有一份连续数据ID：1 … 27，其中ID：6，7，14的数据丢了。结构如下：1234567891011121314151617181920212223242526272829303132CREATE TABLE `letter` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(50) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 27 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci ROW_FORMAT = Compact;INSERT INTO `letter` VALUES (1, 'A');INSERT INTO `letter` VALUES (2, 'B');INSERT INTO `letter` VALUES (3, 'C');INSERT INTO `letter` VALUES (4, 'D');INSERT INTO `letter` VALUES (5, 'E');# INSERT INTO `letter` VALUES (6, 'F');# INSERT INTO `letter` VALUES (7, 'J');INSERT INTO `letter` VALUES (8, 'H');INSERT INTO `letter` VALUES (9, 'I');INSERT INTO `letter` VALUES (10, 'J');INSERT INTO `letter` VALUES (11, 'K');INSERT INTO `letter` VALUES (12, 'L');INSERT INTO `letter` VALUES (13, 'M');# INSERT INTO `letter` VALUES (14, 'N');INSERT INTO `letter` VALUES (15, 'O');INSERT INTO `letter` VALUES (16, 'P');INSERT INTO `letter` VALUES (17, 'Q');INSERT INTO `letter` VALUES (18, 'R');INSERT INTO `letter` VALUES (19, 'S');INSERT INTO `letter` VALUES (20, 'T');INSERT INTO `letter` VALUES (21, 'U');INSERT INTO `letter` VALUES (22, 'V');INSERT INTO `letter` VALUES (23, 'W');INSERT INTO `letter` VALUES (24, 'X');INSERT INTO `letter` VALUES (25, 'Y');INSERT INTO `letter` VALUES (26, 'Z'); 怎么把6，7，14这三条数据找出来？方法有很多种哈，今天我们说一下如何利用SQL快速查询出来。大概思路是把ID+1，然后查询ID+1这个值是否存在ID列表中，如果不存在那肯定就是缺失的。SQL如下：123456SELECT id + 1 AS id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) 但是这样会有一个小问题，就是MAX(id)+1（27）也会被查询出来，所以：123456789101112SELECT id + 1 AS id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) AND id &lt;( SELECT max( id ) FROM `letter` ) 2.优化1234567891011121314151617181920SELECT start_id, ( SELECT MIN(id)- 1 FROM `letter` WHERE id &gt; start_id ) AS end_id FROM ( SELECT id + 1 AS start_id FROM `letter` WHERE id + 1 NOT IN ( SELECT id FROM `letter` ) AND id &lt;( SELECT max(id) FROM `letter` ) ) AS max_id ORDER BY start_id 这种情况适用于查找整数类型的连续编号，那么如果编号是string类型的呢？后面有机会再补充进来=,=。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"},{"name":"biscuits","slug":"biscuits","permalink":"http://xupin.im/tags/biscuits/"}]},{"title":"Redis学习笔记 - 集群","date":"2020-03-26T16:00:00.000Z","path":"2020/03/27/redis-cluster/","text":"之前粗浅的学习了Redis三种集群策略的主从复制和哨兵策略，现在最后这篇来学习一下Redis Cluster也就是最后一个集群策略。 1.什么是集群？集群（Cluster），Redis2.6版本（正式版本是3.0）推出的分布式解决方案，有效解决了单Master节点写操作的压力并且分布式存储数据，大大提高了负载能力。 在Redis发布3.0正式版本前，一般使用代理中间件来实现分布式集群策略。这里不展开学习了，有兴趣的小伙伴自行研究。 特点 Cluster策略是分布式部署，节点间相互协调工作。 因为对主从复制和哨兵策略都称为集群策略，所以为了防止误解在下文中提及的集群（Cluster）策略，直接用Cluster称呼。 Cluster至少要3个Master节点，并且是无中心化设计。 客户端使用Cluster，不需要连接所有节点，只需要连接Cluster中任意一个可用节点即可。 数据的分布式存储不需要指定，Cluster会自动完成。 优点 Cluster策略拥有主从复制和哨兵策略的优点。 解决了单Master节点写操作的压力。 分布式存储数据，提高了负载能力。 支持线性扩容。 缺点 部分操作命令受限，比如mset，目前只能支持同一个插槽（slot）的key进行操作。 事务机制不支持多节点操作。 不支持多数据库，即只有db0。 2.如何配置（新旧方式） 旧方式，./redis-trib.rb create –replicas {SLAVE_NUM} {IP}:{PORT} … {IP}:{PORT} 新方式，./redis-cli –cluster create –cluster-replicas {SLAVE_NUM} {IP}:{PORT} … {IP}:{PORT} redis.conf示例12345678910111213# 是否以守护进程方式运行daemonize yes/no# 是否启用Clustercluster-enabled yes/no# 节点信息配置文件，自动生成。# FILE_NAME：配置文件名cluster-cluster-config-file &#123;FILE_NAME&#125;# 节点连接超时时间# MS：超时时间（单位：Millisecond）cluster-node-timeout &#123;MS&#125; 3.工作机制 Redis节点启动，节点根据配置cluster-enabled判断是否加入Cluster。 新节点通过cluster meet {IP} {PORT}命令和其他节点感知并建立连接，节点间会通过Gossip协议PING/PONG命令来检测状态和交换信息。 Cluster计算并且分配主节点插槽数量。这个地方注意，不是插槽数量，是每个节点的插槽数量。插槽数量是固定的：16384。 插槽分配成功之后，Cluster开始服务。 4.如何感知新节点？当给某一节点发送命令cluster meet {IP} {PORT}（新节点），该节点就会尝试与新节点建立连接，具体流程： 该节点向新节点发送MEET命令。 新节点接收到MEET命令后，回复PONG命令。 该节点接收到新节点返回的PONG命令，知道新节点成功接收了自己的MEET命令。 该节点向新节点发送PING命令。 新节点接收到该节点发送的PING命令，知道该节点已经成功接收到自己返回的PONG命令。 该节点和新节点握手完成，建立连接。 最后，该节点会将新节点的信息通过Gossip协议同步给Cluster中的其他节点，让其他节点也与新节点进行握手，建立连接。 可以通过cluster nodes命令查看集群中哪些节点已经建立连接。 5.数据插槽说到插槽（slot）不得不提一下，为了能够让数据平均分配到多个节点上而采用的数据分区算法。常见的数据分区算法：范围（Range）、哈希（Hash）、一致性哈希算法和虚拟哈希槽等。 Cluster采用的虚拟哈希槽数据分区算法，所有的key根据哈希函数映射到0 ~ 16383插槽内（公式：slot = crc16(key) &amp; 16383），之前也提到过插槽也是平均分配到每个Master节点的。 虚拟哈希槽的特点 降低了节点和数据之间的耦合性，方便线性扩容&amp;动态管理节点。 节点自己管理和插槽的对应关系。 支持查询节点、插槽和key的对应关系。 可以通俗理解为，插槽是Cluster管理数据的基本单位。 6.动态管理节点假如我们原有4个Master节点（M1 … M4），但是现在因为数据增量问题临时加一个Master节点（M5），我们需要怎么操作呢？ 启动M5节点，客户端发送MEET命令让M5节点加入到Cluster中，现在M5节点没有任何插槽所以不会接受任何读写操作。 在M5节点执行cluster setslot {SLOT} importing {SOURCE_NODE_ID}命令，让M5节点准备导入{SLOT}插槽。 在拥有这个{SLOT}插槽的源节点上面执行cluster setslot {SLOT} migrating {M5_NODE_ID}，让源节点准备好迁出插槽。 这时候如果客户端操作的key存在于{SLOT}插槽中，那么这个操作由源节点处理。如果key不存在于{SLOT}插槽中，这个操作将由M5节点操作。 现在源节点的{SLOT}插槽不会创建任何新的key，需要把源节点{SLOT}插槽中的key迁移到M5节点。执行cluster getkeysinslot {SLOT} {COUNT}命令获取{SLOT}插槽中指定{COUNT}数量的key列表。 在源节点对每个key执行migrate命令，把key迁移到M5节点。 在源节点和M5节点执行cluster setslot {SLOT} NODE {M5_NODE_ID}，完成迁移。 这就是动态增加节点的流程了，可能在新的Redis版本中增加了节点迁移工具，但是核心流程应该还是这样。 7.Hash Tag学习了数据插槽，我们知道Redis在key分配到插槽的这一操作完全是自动化的，不过当我们有需求对不同的key需要放到同一插槽中的时候，这个时候要怎么操作呢？我们只需要在key中加入{}符号即可，比如： {UID_1001}:following {UID_1001}:followers 这两个key会被分配到同一插槽，原理就是当key中存在{}符号，哈希算法只会针对{}符号内的字符串。 8.插槽为什么是16384？这个值能修改吗？首先crc16算法算出的值有16bit，2^16即65536。也就是说该算法的值在0 ~ 65535之间，那么为什么作者还是选择了16384即0 ~ 16383。 很开心，对于这个疑问Redis作者给了明确的回答。前面我们知道每个节点之间会以每秒1次的频率互相发送心跳包（PING）&amp;交换信息（信息分为消息头和消息体），之前也提了交换的信息体里面主要包含节点的信息等，那么消息头的内容呢？如下：123456789101112131415161718192021222324typedef struct &#123; char sig[4]; /* Siganture \"RCmb\" (Redis Cluster message bus). */ uint32_t totlen; /* Total length of this message */ uint16_t ver; /* Protocol version, currently set to 1. */ uint16_t port; /* TCP base port number. */ uint16_t type; /* Message type */ uint16_t count; /* Only used for some kind of messages. */ uint64_t currentEpoch; /* The epoch accordingly to the sending node. */ uint64_t configEpoch; /* The config epoch if it's a master, or the last epoch advertised by its master if it is a slave. */ uint64_t offset; /* Master replication offset if node is a master or processed replication offset if node is a slave. */ char sender[CLUSTER_NAMELEN]; /* Name of the sender node */ unsigned char myslots[CLUSTER_SLOTS/8]; char slaveof[CLUSTER_NAMELEN]; char myip[NET_IP_STR_LEN]; /* Sender IP, if not all zeroed. */ char notused1[34]; /* 34 bytes reserved for future usage. */ uint16_t cport; /* Sender TCP cluster bus port */ uint16_t flags; /* Sender node flags */ unsigned char state; /* Cluster state from the POV of the sender */ unsigned char mflags[3]; /* Message flags: CLUSTERMSG_FLAG[012]_... */ union clusterMsgData data; /* message body*/&#125; clusterMsg; 其中有个myslots字段要注意，该字段使用位图，即1bit代表1slot，如果该bit为1即说明该插槽属于这个节点。那么该字段的大小为：16384 / 8bit / 1024b = 2kb。也就是消息头不考虑其他信息的情况，单是myslots就已经有2kb大小。 那么消息体呢？之前已经提到了消息体中会包含节点信息。具体是什么样的呢？消息体每次携带最少3个节点的信息，数量约为总节点数的1/10。如果节点数量越多，消息体越大。 如果插槽数量是65535，那么该字段的大小放大为：65535 / 8bit / 1024b = 8kb。这对于每秒1次频率的心跳包来讲，带宽开销是极大的。 上面说了节点越多，消息体也就越大，如果节点超过1000个也会导致网络拥堵，因为Redis作者不建议Cluster节点的数量超过1000，那么对于1000个以下的节点来说16384个插槽也就够用了。 第三个考虑是关于位图的压缩问题，我还没有搞明白~~所以这里不展开说了，先埋个坑。 这个值能修改吗？16384插槽数量是写死在Redis源代码中的，所以是不可以更改的。 附上关于作者的回答 https://github.com/antirez/redis/issues/2576 最后Cluster在故障恢复主从切换的机制（包括：主观宕机、客观宕机、投票选举、主从切换）和哨兵策略基本一致，所以在这里就不学习Cluster关于故障恢复主从切换的相关知识了。 前面提到过Gossip协议，它主要职责就是各节点间的信息交换，常用的Gossip消息可分为： ping pong meet fail 后面会专门来学习Gossip协议的知识（-,-再次埋个坑）。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Redis学习笔记 - 哨兵","date":"2020-03-25T16:00:00.000Z","path":"2020/03/26/redis-sentinel/","text":"现在这篇来学习一下Redis Sentinel即哨兵策略的相关知识。 1.什么是哨兵？哨兵（Sentinel），Redis2.6版本（正式版本是2.8，现2.6版本已被废弃）开始提供的一种集群策略，核心思想是解决了主从复制（Replication）在Master节点故障，无法自动切换Slave节点为新Master节点的问题。 特点 哨兵策略是分布式部署，节点间相互协调工作。 哨兵集群至少要3个节点。 可以把哨兵看作是一种特殊的Redis服务。 优点 哨兵策略拥有主从复制的优点。 哨兵策略解决了Master节点故障，无法自动切换Slave节点为新Master节点的问题。 缺点 Master节点写操作的压力没有得到解决。 数据存储能力还是受到单节点限制。 2.如何配置（多种方式） 启动Redis哨兵，./redis-sentinel redis-sentinel.conf 启动Redis服务时并且启动哨兵，./redis-server redis-sentinel.conf –sentinel redis-sentinel.conf示例1234567891011121314151617181920212223242526# 哨兵监控的节点。# MASTER_NAME：自定义的Master节点名称。# IP：Master节点的地址。# PORT：Master节点的端口。# QUORUM：当Master节点故障，确认Master节点odown最少的哨兵数量。sentinel monitor &#123;MASTER_NAME&#125; &#123;IP&#125; &#123;PORT&#125; &#123;QUORUM&#125;# 哨兵验证# PASS：Master节点的auth，需要注意的时哨兵不能同时为Master节点和Slave节点设置密码，所以auth需要保持一致。sentinel auth-pass &#123;MASTER_NAME&#125; &#123;PASS&#125;# 哨兵心跳Ping最大时间，如果哨兵向Master节点发送Ping超过这个时间或者回复err，那么哨兵会主观（sdown）认为该Master节点已经处于不可用状态。# MS：心跳Ping等待响应的最大时间（单位：Millisecond）。sentinel down-after-milliseconds &#123;MASTER_NAME&#125; &#123;MS&#125;# 同时同步数据的Slave节点数量# SLAVE_NUM：当Master节点故障，Slave节点通过竞选当选新Master节点，最多允许几个Slave节点开始同步新Master的数据。sentinel parallel-syncs &#123;MASTER_NAME&#125; &#123;SLAVE_NUM&#125;# 主从节点切换所需要的最大时间。# MS：主从节点切换所需要的最大时间（单位：Millisecond）。sentinel failover-timeout &#123;MASTER_NAME&#125; &#123;MS&#125;# Master节点故障调用的脚本# SCRIPT_PATH：脚本路径sentinel notification-script &#123;MASTER_NAME&#125; &#123;SCRIPT_PATH&#125; 3.工作机制 哨兵向已知节点和哨兵发送心跳包检测状态。 Master节点无效回复，哨兵判断Master节点状态：主观宕机，客观宕机。 Master节点被确定客观宕机，进行领头（Leader）哨兵选举。 准备进行主从切换的领头哨兵获取其他哨兵的授权。 授权成功，从Slave节点中选举新Master节点。 领头哨兵把新Master节点信息同步给其他哨兵，其他哨兵把新Master节点信息同步对应Slave节点。 4.心跳包每个哨兵以每秒钟1次的频率向它已知的Master节点、Slave节点和其他哨兵发送PING命令，希望得到的有效回复如下：123PING replied with +PONG.PING replied with -LOADING error.PING replied with -MASTERDOWN error. 其它任何回复或者无回复都是无效回复。 5.哨兵和节点之间的自动发现机制通过Redis的pub/sub系统实现，每个哨兵都会向自己监控的节点对应的channel：sentinel:hello发送一条消息（消息体包含自己的{IP}、{PORT}和{RUNID}以及Master节点的完整配置），每个订阅该channel的哨兵都可以消费这条消息并且能发现到其他哨兵的存在，如果某个哨兵发现自己维护的节点配置低于新接收的节点配置，则会用新的节点配置进行覆盖。 6.主观/客观宕机如果一个Master节点在收到PING命令后没有在有效时间内（down-after-milliseconds）进行有效回复，则会被标记为主观宕机（sdown，Subjectively Down）。 哨兵会获取其他哨兵检测该节点的状态，命令：12345# IP：主观宕机的Master节点地址# PORT：主观宕机的Master节点端口# CURRENT_EPOCH：哨兵的配置纪元，用于领头哨兵选举。# RUNID：可以是*和哨兵的RunID，当值是 * 代表检测节点是否主观宕机，如果是RunID则用于领头哨兵选举。SENTINEL is-master-down-byaddr &#123;IP&#125; &#123;PORT&#125; &#123;CURRENT_EPOCH&#125; &#123;RUNID&#125; 当有足够数量（{QUORUM}）的哨兵都认为该Master节点处于主观宕机状态。则该Master节点会被标记为客观宕机（odown，Objectively Down），若没有足够数量的哨兵都认为该Master节点处于主观宕机状态，则不会被标记为客观宕机，同时如果该Master节点重新返回哨兵有效回复，该Master节点主观宕机状态会被移除。 7.领头哨兵选举因为只需要一个哨兵完成主从切换，所以需要选举一个领头哨兵。 每个哨兵都会发送SENTINEL is-master-down-byaddr命令希望成为领头哨兵。收到该命令的哨兵如果没有同意过其他哨兵的同样命令，那么同意该请求，否则拒绝。 如果某一哨兵发现同意自己请求的哨兵数量并且数量大于等于{QUORUM}，那么它将成为领头哨兵。 如果选举过程中有多个哨兵当选领头哨兵，等待一段时间后选举会重新进行。 该方法基于raft算法领头选举方法实现。 8.主从切换授权当选举出领头哨兵之后并未马上进行主从切换，领头哨兵还需要获取{MAJORITY}数量的哨兵授权。1# MAJORITY：该值不可配置，Redis自行计算。公式：majority = voters / 2 + 1 如果{QUORUM} &lt; {MAJORITY}，领头哨兵需要{MAJORITY}数量的哨兵进行授权。 如果{QUORUM} &gt;= {MAJORITY}，那么领头哨兵需要{QUORUM}数量的哨兵授权才可以。 当领头哨兵获得授权之后，正式开始主从切换流程。 9.主从切换开始主从切换（failover），首先领头哨兵会选举出一个Slave节点出来作为新Master节点，该Slave节点选举参考参数： 该节点和Master节点断开的时长，如果一个Slave节点与Master节点断开连接时间已经超过down-after-milliseconds参数的10倍，再加上Master宕机的时长，该Slave节点就会被认为不适合选举为新的Master节点。 1(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state Slave节点的优先级（slave-priority），slave-priority越低优先级越高。 复制数据偏移量（复制数据最完整） RunID（最小的） 10.配置同步在领头哨兵完成主从切换之后，会在本地生成最新的Master配置然后通过pub/sub消息机制同步给其他哨兵，其他哨兵则更新对应的Master节点配置。 那么其他哨兵怎么知道这份配置是最新的呢？ 领头哨兵准备执行主从切换前，会从要切换成新Master节点的Slave节点取得一个configuration epoch，可以理解为配置版本号。如果领头哨兵主从切换失败了，那么其他哨兵会等待failover-timeout时间然后接替继续执行切换，每次接替都会重新获取一个configuration epoch，作为新的配置版本号。如果领头哨兵切换成功，那么其他哨兵会根据自己的配置版本号来更新对应Slave节点的Master节点配置。 最后为什么说哨兵最少要3个节点，举个例子： 如果是2个节点，{QUORUM}值为1，此时其中一台服务器出现客观宕机。领头哨兵需要进行主从切换，在进行主从切换前需要获取{MAJORITY}数量的哨兵同意，该{MAJORITY}参数最小的值是：2，此时领头哨兵无法进行主从切换。 https://redis.io/topics/sentinel","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Redis学习笔记 - 主从复制","date":"2020-03-24T16:00:00.000Z","path":"2020/03/25/redis-replication/","text":"Redis单点配置的情况下如果服务出现故障宕机，那么服务也就处于不可用状态，假如是生产环境那么带来的后果会有很严重，所以出现了高可用方案：集群策略。 1.三种集群策略Redis提供了三种集群策略，它们分别是： 主从复制（Replication） 哨兵（Sentinel） 集群（Cluster） 这三种策略会逐一学习，本篇主要学习策略之一主从复制（Replication）。 2.主从复制的概念在主从复制（Replication）策略中，服务分为两类：Master节点、Slave节点。 特点 Master节点可以拥有多个Slave节点，但Slave节点只能服务一个Master节点。 数据复制方向只能是Master节点-&gt;Slave节点。 优点 实现了多机热数据备份，提高了面对宕机数据恢复的灾备能力。 在主从复制的基础上实现读写分离提高服务吞吐量，即：Master节点提供写服务，Slave节点提供读服务。 如果Master节点宕机可以快速切换使用Slave节点提供服务。 缺点 Master节点故障，无法自动切换Slave节点为新Master节点的问题。 Master节点写操作的压力没有得到解决。 数据存储能力还是受到单节点限制。 3.如何配置（多种方式） 在Slave节点服务器redis.conf增加配置行，slaveof {MASTER_IP} {MASTER_PORT}。 启动redis-server时，./redis-server slaveof {MASTER_IP} {MASTER_PORT}。 在redis-cli命令行界面输入，slaveof {MASTER_IP} {MASTER_PORT}。 4.工作机制 Slave节点执行slaveof命令，保存Master节点信息。 节点内部的定时任务发现主节点信息，开始尝试Socket连接主节点。 连接建立成功，Slave节点发送ping命令，期望得到pong命令响应，否则发起重连。 如果主节点有设置auth，那么进行auth验证，成功继续，失败终止。（非必须，根据Master节点是否配置auth决定） Slave节点同步Master节点全量数据集。（该操作是Master节点向Slave节点发送数据哟） Master节点持续把写命令同步Slave节点。 5.同步命令Redis主从复制数据有两个命令，sync和psync，sync是Redis2.8版本之前的同步方法，psync是Redis2.8版本以后优化sync新设计同步方法。在这会着重学习psync，也会捎带说一下为什么sync会被优化。 首先，psync需要3个参数支持： Master节点和Slave节点复制数据的偏移量。 1Master节点和Slave节点复制数据的偏移量，主要作用是通过对比复制偏移量，来判断Master节点和Slave节点数据是否一致。 Master节点复制积压缓冲区。 1psync的特性之一，用于增量数据复制和补救丢失的复制数据。 Master节点的RunID（Replication ID）。 1Redis服务启动的时，都会生成一个40位的唯一RunID。 Master节点和Slave节点复制数据的偏移量：每个参与复制数据的节点都会维护一份复制偏移量，Master节点在处理完写命令后，会把命令的字节长度进行累加，Slave节点每秒钟会向Master节点上报自己的复制偏移量，因此Master节点也会记录Slave节点的偏移量。Master节点持续把写命令同步Slave节点，Slave节点成功接收到之后也会累加自身的偏移量。查看偏移量：123&gt;info replicationmaster_repl_offset:&#123;NUM&#125; # Master节点偏移量slave_repl_offset:&#123;NUM&#125; # Slave节点偏移量 复制积压缓冲区：复制积压缓冲区是一个保存在Master节点拥有固定长度的队列，该队列先进先出，大小受repl-backlog-size参数控制（默认：1MB），查看缓冲区：12&gt;info replicationrepl_backlog_size:&#123;NUM&#125;（byte） Master节点的RunID（Replication ID）：该ID主要是用来识别Redis服务节点，因为如果使用IP+PORT方式，假如Master节点重启之后修改了RDB/AOF备份文件，此时Slave节点再基于原来的复制偏移量进行复制数据是不可靠的。查看RunID：12&gt;info serverrun_id:8d252f66c3ef89bd60a060cf8dc5cfe3d511c5e4 psync命令使用方式1psync &#123;RUNID&#125; &#123;OFFSET&#125; 6.增量/全量复制知道了命令如何使用，那么当Slave节点发送psync命令给Master节点之后会发生什么？流程分为全量复制和增量复制两种。 全量复制，如果Slave节点发送的命令是：psync ? -1 Master节点知道Slave节点要全量复制数据，返回命令则是：+fullresync {RUNID} {OFFSET}，同时Master节点会执行RDB备份并且使用复制积压缓冲区来记录此后所有的写命令。Master节点 RDB备份完成之后向Slave节点发送备份文件，同时继续缓冲写命令，在备份文件发送完毕后Master节点会向Slave节点发送缓冲区的写命令。Slave节点在收到Master节点发送的备份文件之后，会丢弃所有的旧数据，开始载入备份文件并且开始执行Master节点发送缓冲区的写命令。 值得一提的是，在Slave节点加载备份文件的时候数据处于不可靠阶段，此时可以通过参数slave-server-stale-data（yes、no）配置是否响应请求，yes响应，no则抛出“SYNC with master in progress”。 如果备份从创建到传输完毕消耗时间大于repl-timeout参数的值，Slave节点将会放弃接收备份文件并且清理已经下载的临时文件。 增量复制，Master节点会根据{RUNID}和{OFFSET}决定返回结果。 Master节点首先会检查{RUNID}是否与自身一致，如果不一致将会执行全量数据复制。如果一致会根据{OFFSET}参数在缓冲区查找，如果数据偏移量之后的数据存在缓冲区，返回命令：+continue，表示可以增量复制数据。如果返回命令+err，表示Master节点版本过低不支持psync命令，将会使用sync进行全量复制数据。 最后最后学习一下为什么sync会被优化？ 使用sync命令，在网络或者其他不可抗力因素导致Master节点和Slave节点断开连接，需要重新进行一次全量数据复制，Slave节点数据恢复成本极高。 https://redis.io/topics/replication http://try.redis.io","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"Mysql学习笔记 - 分片和分区","date":"2020-03-22T16:00:00.000Z","path":"2020/03/23/mysql-sharding-partition/","text":"之前在复习Mysql主从知识的同时，小伙伴考了我一个问题，分片和分区有什么差别 … 乍一听可能只是名词上的不同，但其实它们俩的确不是同一个东西。 1.什么是分片（Sharding）？我们举个例子，一张文章表。结构如下：123456789CREATE TABLE `mark`.`article` ( `id` int NOT NULL AUTO_INCREMENT, `type` varchar(20) NOT NULL COMMENT '类型', `title` varchar(255) NOT NULL COMMENT '标题', `content` longtext NOT NULL COMMENT '内容', `author` varchar(20) NOT NULL COMMENT '作者', `date` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`)); 需求如下： 用户打开app会直接展示文章列表（类型，标题，作者，日期） 点击文章查看详情 那么这张表在数据量比较小的初期应对访问是没什么问题的。但是随着数据量日益膨胀，查询效率会越来越低，因为里面content字段非常巨大。 这时候要怎么优化呢？把content字段拆出去，因为访问最多的请求并不需要content，怎么拆呢？结构如下： 1234567891011121314151617# 文章表CREATE TABLE `mark`.`article` ( `id` int NOT NULL AUTO_INCREMENT, `type` varchar(20) NOT NULL COMMENT '类型', `title` varchar(255) NOT NULL COMMENT '标题', `c_id` int NOT NULL COMMENT '文章ID', `author` varchar(20) NOT NULL COMMENT '作者', `created_at` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`));# 文章内容表CREATE TABLE `mark`.`article_content` ( `id` int NOT NULL AUTO_INCREMENT, `content` longtext NOT NULL COMMENT '内容', `created_at` datetime NOT NULL COMMENT '日期', PRIMARY KEY (`id`)); 2.垂直（纵向）切分上述这种情况就是分片的一种，垂直切分也被称为纵向切分，这种分片的方式不仅可以跨表也可以跨库。 优点 拆分规则简单。 数据模块清晰。 容易维护，容易定位问题。 缺点 如果分片表跨库，那么在SQL层无法进行连表查询，只能在程序层处理。 事务处理复杂度变高。 后期表结构的扩展性受限。 3.水平（横向）切分说到了垂直切分，那就不得不提一下水平切分也被称之为横向切分。同样举个例子，一张用户日志表。结构如下：12345CREATE TABLE `mark`.`user_log` ( `u_id` int NOT NULL COMMENT '用户ID', `content` longtext NOT NULL COMMENT '日志内容', `created_at` datetime NOT NULL COMMENT '创建日期'); 需求如下： 根据用户ID快速检索日志信息 情况一样，在数据量比较小的时候功能响应速度应该还可以，但是随着数据量增长的比较厉害。查询效率会呈断崖式下降。 那么这时候要怎么优化呢？我们按照用户ID去拆分表，即每个用户存储的日志固定在一张表。结构如下：12345CREATE TABLE `mark`.`user_log_&#123;NUM&#125;` ( `u_id` int NOT NULL COMMENT '用户ID', `content` longtext NOT NULL COMMENT '日志内容', `created_at` datetime NOT NULL COMMENT '创建日期'); 为了省事，后面{NUM}是个数字哈，每个用户进来我们会利用摘要算法（比如：crc32）取个固定数值，然后固定把对应的数据存储到相应的表内。 优点 不会出现跨库无法连表查询的情况。 事务处理相对简单。 很难出现扩展性受限的问题。 缺点 数据分布不平均，可能一张表10W行，另外一张100W行。 难维护，定位问题需要前置算法查询。 后期数据迁移比较麻烦。 4.什么是分区（Partition）？分片简单说了说，那么接下来要学习一下分区。分区和分片比较明显的一点是：分片多是利用程序配合来实现，分区则是数据库（不仅仅是Mysql，其它数据库也有）提供的机制。 首先，分区分为4种模式： Range List Hash Key 5.RangeRange（范围）大概的思想是将数据进行分段，比如按照日期将内购表进行拆分，分为不同的年份。 1234567891011CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY RANGE( YEAR(created_at) ) ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (1995), PARTITION p2 VALUES LESS THAN (2000), PARTITION p3 VALUES LESS THAN (2005), PARTITION p4 VALUES LESS THAN (2010), PARTITION p5 VALUES LESS THAN (2015));# 也可以按照金额来进行分区。# PARTITION BY RANGE( money ) (); 写入以下数据1234567891011INSERT INTO purchase(app_id,name,money,currency,created_at) VALUES(600001,'desk organiser',6.0000,'CNY','2003-10-15'),(600001,'alarm clock',12.0000,'CNY','1997-11-05'),(600002,'chair',30.0000,'CNY','2009-03-10'),(600002,'bookcase',68.0000,'CNY','1989-01-10'),(600002,'exercise bike',128.0000,'CNY','2014-05-09'),(600003,'sofa',258.0000,'CNY','1987-06-05'),(600003,'espresso maker',648.0000,'CNY','2011-11-22'),(600004,'aquarium',99.0000,'USD','1992-08-04'),(600005,'study desk',129.0000,'USD','2006-09-16'),(600006,'lava lamp',299.0000,'USD','1998-12-25'); 然后我们观察一下，可能通过肉眼看没什么变化，执行这条查看分区状态的SQL1234567891011121314151617SELECT partition_name part, partition_expression expr, table_rows FROM information_schema.PARTITIONS WHERE table_schema = SCHEMA () AND table_name = '&#123;TABLE&#125;';# 结果如下part | expr | descr | table_rowsp0 | YEAR(created_at) | 1990 | 2p1 | YEAR(created_at) | 1995 | 1p2 | YEAR(created_at) | 2000 | 2p3 | YEAR(created_at) | 2005 | 1p4 | YEAR(created_at) | 2010 | 2p5 | YEAR(created_at) | 2015 | 2 很明显，数据分别按照规则写进了分区表p0 ~ p5。 6.ListList（列表）模式，多用于对于指定字段的数值进行明确的数据拆分。 123456789CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY LIST( app_id ) ( PARTITION p0 VALUES IN (600001), PARTITION p1 VALUES IN (600002), PARTITION p2 VALUES IN (600003), PARTITION p3 VALUES IN (600004), PARTITION p4 VALUES IN (600005), PARTITION p5 VALUES IN (600006)); 写入同样的数据，之后执行查看分区状态的SQL。1234567part | expr | descr | table_rowsp0 | app_id | 600001 | 2p1 | app_id | 600002 | 3p2 | app_id | 600003 | 2p3 | app_id | 600004 | 1p4 | app_id | 600005 | 1p5 | app_id | 600006 | 1 7.HashHash（哈希）模式通过对一个或多个字段进行Hash计算，通过这个Hash值来进行分区（是不是感觉很像分片，其实分片的思想就是根据分区衍生而来）。 123CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY HASH( YEAR(created_at) )PARTITIONS 4; # 为了区别，设4个分区。 然后写入同样的数据，之后执行查看分区状态的SQL。12345part| expr | descr | table_rowsp0 | YEAR(created_at) | NULL | 1p1 | YEAR(created_at) | NULL | 3p2 | YEAR(created_at) | NULL | 3p3 | YEAR(created_at) | NULL | 3 8.KeyKey（键）模式和Hash模式极其相似，可能有一点区别就是，Hash模式下是用户自定义规则进行Hash计算，而Key模式是Mysql使用自己的函数进行Hash计算。 123CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY KEY( created_at )PARTITIONS 3; # 为了区别，设3个分区。 重复步骤，写入同样的数据，之后执行查看分区状态的SQL。1234part| expr | descr | table_rowsp0 | `created_at` | NULL | 2p1 | `created_at` | NULL | 3p2 | `created_at` | NULL | 5 Key模式和Hash模式在expr上面有了直观的不同表现，Hash模式是：YEAR(created_at)，而Key模式是：created_at。这就是我们前面提到的Hash计算方式的区别。 9.最后其实Mysql分区还有第5种模式，叫做Composite（复合）模式，比如：Range - Key123456789101112CREATE TABLE purchase (id INT, app_id INT, name VARCHAR(50), money NUMERIC(20,4),currency VARCHAR(10),created_at DATE)PARTITION BY RANGE( YEAR( created_at ) ) SUBPARTITION BY KEY( created_at )SUBPARTITIONS 3( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (1995), PARTITION p2 VALUES LESS THAN (2000), PARTITION p3 VALUES LESS THAN (2005), PARTITION p4 VALUES LESS THAN (2010), PARTITION p5 VALUES LESS THAN (2015)); 这里就不展开Composite（复合）模式的学习了，最后通过以上的例子，可以看出分区也是有垂直分区和水平分区的说法的。 https://dev.mysql.com/doc/mysql-partitioning-excerpt/5.7/en/partitioning-management.html","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 刷脏机制","date":"2020-03-20T16:00:00.000Z","path":"2020/03/21/mysql-checkpoint/","text":"之前学习了LRU算法和Mysql缓冲池使用的LRU变体算法，其中有个共同点就是当LRU链表写满以后如果再有新数据进来会淘汰尾部的数据，那么Mysql淘汰这些尾部数据的时候是否会进行什么操作呢？这就是我们在最后提到了一个参数Modified db pages，即脏页。 1.什么是脏页？ 脏页，这个名词很抽象从字面意思去看可能很不解，脏页是当内存中的数据页和磁盘中的数据页内容不一致时，这个数据页称之为脏页。因为从操作系统的角度来讲，自己读入的数据被外部所修改等于被污染，所以叫脏页。 当内存中的数据页和磁盘中的数据页数据一致，叫干净页。 2.脏页什么时候会刷新？ 缓冲池（buffer pool）空间不足，也就是LRU链表写满时，新数据进来时淘汰掉的尾部数据脏页。 redo log不可用时，需要强制将脏页列表中的一些数据页刷入磁盘。 Mysql在服务器负载较小时，会主动进行刷脏操作。 Mysql服务正常关闭，会刷新所有脏页。 3.什么是redo log？InnoDB中两块非常重要的日志，一个是undo log，另外一个就是我们接下来要学习的redo log。前者用来保证事务的原子性以及InnoDB的MVCC（Mutil-Version Concurrency Control），后者用来保证事务的持久性。 那么什么时候写redo log呢？当数据库对数据做修改的时候，需要把数据页从磁盘读到缓冲池中，然后在缓冲池中进行修改。但是InnoDB采用的是WAL（Write Ahead Log）策略来防止数据丢失，也就是事务提交时，先写redo log才会再去修改内存数据页。 redo log的文件名默认以ib_logfile{NUM}，存储在my.cnf中datadir目录下面，受以下两个参数控制 innodb_log_file_size，日志大小 innodb_log_files_in_group，日志个数，默认是2个。 所以redo log的大小等于innodb_log_files_in_group*innodb_log_file_size。 既然redo log会产生，那么什么时候会被覆盖呢？redo log被设计成可循环使用，当日志文件写满时那些已经被刷入磁盘中的数据就可以被覆盖啦。 WAL（Write Ahead Log），是关系数据库系统中用于提供原子性和持久性（ACID属性中的两个）的一系列技术，ARIES是WAL系列技术常用的算法，在文件系统中WAL通常称为journaling。WAL的主要思想是将元数据的实时变更操作写入日志文件中，然后在系统负载较小时再把日志刷入磁盘。主要是为了减少磁盘的IO操作，此处就不展开学习了。先Mark一下 4.CheckpointCheckpoint（检查点），在数据库中一般是用来把redo log脏页刷入磁盘的一个操作，通过LSN保存记录，作用是当发生宕机等crash情况时，再次启动时会查询Checkpoint，在该Checkpoint之后发生的事务修改恢复到磁盘。通俗来解释，就像我们玩一些游戏每过不久就会存一次档，然后如果游戏客户端不幸crash重新进入最近的一次存档即可，同理。 Checkpoint存在的目的： 缩短恢复数据时间。 缓冲池写满时，淘汰脏页刷入磁盘。 redo log写满时，进行刷脏操作。 那么怎么查看我们的检查点呢？可以使用命令show engine innodb status来查看：12345678910111213&gt;show engine innodb status\\G;---LOG---Log sequence number 1597945Log flushed up to 1597945Last Checkpoint at 1597945Max Checkpoint age 7782360Checkpoint age target 7539162Modified age 0Checkpoint age 00 pending log writes, 0 pending chkp writes8 log i/o's done, 0.42 log i/o's/second 12345Log sequence number | 当前系统LSN最大值，新的日志LSN将在此基础上生成（LSN+新日志的大小）。Log flushed up to | 当前已经写入日志文件的LSN。Last Checkpoint at | 当前已经写入Checkpoint的LSN。Max Checkpoint age | Percona的XtraDB参数，此处不过多解释。Checkpoint age target | 同上。 LSN（Log Sequence Number），LSN是日志空间中每条日志的结束点，用字节偏移量来表示。每个数据页有LSN，redo log也有LSN，Checkpoint亦有LSN。该LSN记录当前数据页最后一次修改的LSN号，用于在恢复数据时对比重做日志LSN号决定是否对该数据页进行恢复数据。可以通俗理解为，存档编号。 5.Checkpoint什么时候会触发？InnoDB存储引擎有两种Checkpoint，分别是： Sharp Checkpoint Fuzzy Checkpoint Sharp Checkpoint发生在数据库服务关闭时，将所有脏页刷入磁盘，此时innodb_fast_shutdown参数的值为1（innodb_fast_shutdown参数的值：0、1、2），这是默认机制。但是考虑到如果数据库在使用时也执行这种机制，数据库的性能会受到影响，所以Fuzzy Checkpoint刷新部分脏页的这种机制产生了。 Fuzzy Checkpoint刷新部分脏页，也分为以下几种方式： Master Thread Checkpoint FLUSH_LRU_LIST Checkpoint Dirty Page too much Checkpoint Async/Sync Flush Checkpoint Master Thread Checkpoint 主线程以每秒或者每十秒从缓冲池的脏页列表（Flush List）刷新一定比例的数据页回磁盘，这个操作过程是异步的不会阻塞线程。 FLUSH_LRU_LIST Checkpoint InnoDB需要保证LRU链表中有足够空闲页可以使用，在InnoDB1.1.x版本前，如果LRU链表写满有新的数据进来如果淘汰尾部脏页，会触发Checkpoint机制强制进行刷脏操作。该操作是阻塞线程的，所以在InnoDB1.2.x版本开始，这个操作放到Page Cleaner Thread来处理，每次刷新LRU链表脏页的数量受innodb_lru_scan_depth参数控制（默认：1024）。 Dirty Page too much Checkpoint 当LRU链表中脏页数量过多时（比例），InnoDB为了保证缓冲池中有足够多的空闲页可以使用，会强制触发Checkpoint机制进行刷脏操作。此值受innodb_max_dirty_pages_pct参数控制（默认：75%）。 Async/Sync Flush Checkpoint 为了保证redo log循环使用的可重用性，在redo log不可用时会强制触发Checkpoint刷脏操作。在InnoDB1.2.x版本以前，Async Flush Checkpoint会阻塞当前查询线程，Sync Flush Checkpoint会阻塞所有查询线程。InnoDB1.2.X之后放到单独的Page Cleaner Thread来处理。 6.最后关于Async/Sync Flush Checkpoint刷脏方式的原理有些复杂，这里先Mark一下，暂时不展开学习了。 前面说到的Percona XtraDB https://www.percona.com/doc/percona-server/8.0/scalability/innodb_io.html","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"PHP学习笔记 - 文件分片上传","date":"2020-03-18T16:00:00.000Z","path":"2020/03/19/laravel-chunk-upload/","text":"1.什么是分片上传？ 如果我们上传的文件是一个很大的文件，那么上传的时间应该会比较久，再加上网络不稳定各种因素的影响，很容易导致传输中断。 服务端一般都会设置固定大小的接收BUFF，往往上传文件的体积是该值的几何倍数。 分片上传的原理大致可以描述为，把一个较大文件分成若干份的分片一个一个传输，服务端在接收到最后一个分片后进行合并资源。 2.前端前端使用Dropzone控件，DropzoneJS是一个开源库，提供带有图像预览的拖放文件上传并且有分割文件分片上传的机制，当然该控件不仅仅可以上传图片，经测试视频，Word等文件一样可以。 Dropzone官网提供了一个simple.html的上传demo（js、css文件路径有改动过），内容如下：1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;meta charset=\"utf-8\"&gt;&lt;title&gt;Dropzone simple example&lt;/title&gt;&lt;!-- DO NOT SIMPLY COPY THOSE LINES. Download the JS and CSS files from the latest release (https://github.com/enyo/dropzone/releases/latest), and host them yourself!--&gt;&lt;script src=\"./dropzone.js\"&gt;&lt;/script&gt;&lt;link rel=\"stylesheet\" href=\"./dropzone.css\"&gt;&lt;p&gt; This is the most minimal example of Dropzone. The upload in this example doesn't work, because there is no actual server to handle the file upload.&lt;/p&gt;&lt;!-- Change /upload-target to your upload address --&gt;&lt;form action=\"api/upload\" class=\"dropzone\"&gt;&lt;/form&gt; Dropzone官方提供的demo默认是没有开启分片上传的，需要修改dropzone.js123456789101112/** * Whether you want files to be uploaded in chunks to your server. This can't be * used in combination with `uploadMultiple`. * * See [chunksUploaded](#config-chunksUploaded) for the callback to finalise an upload. */chunking: false, // 修改为true,即开启分片/** * If `chunking` is `true`, then this defines the chunk size in bytes. */chunkSize: 2000000, // 默认分片大小，单位：Byte 3.后端 框架：Laravel 6.8扩展：laravel-chunk-upload代码如下：1234567891011121314151617181920212223242526 // create the file receiver$receiver = new FileReceiver(\"file\", $request, HandlerFactory::classFromRequest($request));// check if the upload is success, throw exception or return response you needif ($receiver-&gt;isUploaded() === false) &#123; throw new UploadMissingFileException();&#125;// receive the file$save = $receiver-&gt;receive();// check if the upload has finished (in chunk mode it will send smaller files)if ($save-&gt;isFinished()) &#123; // save the file and return any response you need, current example uses `move` function. If you are // not using move, you need to manually delete the file by unlink($save-&gt;getFile()-&gt;getPathname()) return $this-&gt;saveFile($save-&gt;getFile());&#125;// we are in chunk mode, lets send the current progress/** @var AbstractHandler $handler */$handler = $save-&gt;handler();return response()-&gt;json([ \"done\" =&gt; $handler-&gt;getPercentageDone(), 'status' =&gt; true]); 4.Demo https://github.com/xupin/chunk-upload-example","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"laravel","slug":"laravel","permalink":"http://xupin.im/tags/laravel/"},{"name":"dropzone","slug":"dropzone","permalink":"http://xupin.im/tags/dropzone/"}]},{"title":"Mysql学习笔记 - LRU算法","date":"2020-03-17T16:00:00.000Z","path":"2020/03/18/mysql-lru/","text":"1.什么是LRU？LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“最近使用的页面数据会在未来一段时期内仍然被使用,已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用”。 2.LRU的实现LRU算法最常见的实现是使用链表来保存数据，该链表是双向链表，然后利用先进先出的特性，最新写入的数据会最快被获取。 访问不存在的数据时，缓存数据则会写入链表的头部，链表写满时会淘汰掉尾部的缓存数据。 当缓存数据被访问时，则将该缓存数据向链表头部移动。 优点：面对频繁访问的热点数据，查询效率高 缺点：如果一次查询扫描全表，那么LRU列表会被污染 3.Mysql LRU算法有何不同？Mysql（InnoDB）的缓冲池（buffer pool）使用了LRU算法的变体，将链表分为三个部分：young、midpoint、old。链表比例(5、3)分配给young、old。其中young占据5/8，old占据3/8，此比例受参数innodb_old_blocks_pct控制。 young是链表中最近访问过的新子表。 old是链表中最近访问的旧子表。 midpoint是介于新子表 &amp; 旧子表的边界中间位置（也可以理解为旧子表的头部）。 数据库刚启动LRU链表为空时，此时会检查Free List中是否有空闲的数据页，如果有则从Free List中删除并且在LRU链表中写入相同的数据页。 访问不存在的数据页时，数据页不会直接写入链表的头部，而是写入中间位置，如果链表满了则会从旧子表尾部淘汰数据页。 当数据页被访问时，则判断访问数据页的时间是否大于设定innodb_old_blocks_time（默认：1000ms），如果大于则向链表头部移动，如果小于其位置不变。 改进优点：能够防止单次大量的全表扫描污染整个LRU链表。 4.Mysql LRU相关查询命令123456789101112131415161718192021222324252627282930&gt;show engine innodb status\\G;----------------------BUFFER POOL AND MEMORY----------------------Total memory allocated 137756672; in additional pool allocated 0Total memory allocated by read views 88Internal hash tables (constant factor + variable factor) Adaptive hash index 2217584 (2213368 + 4216) Page hash 139112 (buffer pool 0 only) Dictionary cache 593780 (554768 + 39012) File system 83536 (82672 + 864) Lock system 333248 (332872 + 376) Recovery system 0 (0 + 0)Dictionary memory allocated 39012Buffer pool size 8191Buffer pool size, bytes 134201344Free buffers 8048Database pages 143Old database pages 0Modified db pages 0Pending reads 0Pending writes: LRU 0, flush list 0, single page 0Pages made young 0, not young 00.00 youngs/s, 0.00 non-youngs/sPages read 143, created 0, written 00.00 reads/s, 0.00 creates/s, 0.00 writes/sNo buffer pool page gets since the last printoutPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/sLRU len: 143, unzip_LRU len: 0I/O sum[0]:cur[0], unzip sum[0]:cur[0] 123456Buffer pool size | innodb_buffer_pool的大小Free buffers | 当前Free List中数据页数量Database pages | LRU链表中数据页数量Old database pages | LRU链表中旧子表数据页数量Modified db pages | LRU链表中脏页数量Pages made young 0, not young 0 | 数据页从old移至young的行为：page made young,数据页因为innodb_old_blocks_time导致没有从old移至young的行为：page not young。后面的数值是该行为发生的次数。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - Int长度的问题","date":"2020-03-15T11:30:00.000Z","path":"2020/03/15/mysql-int-length/","text":"创建数据表的时候我们总是要考虑存储数据的字段该用哪种类型、多少长度比较合适，不过在Mysql中如果字段类型是Int，此时长度是不生效的。 1.Int类型的长度问题12345CREATE TABLE `test` ( `id` int(3) NOT NULL, `name` varchar(20) COLLATE utf8mb4_unicode_ci NOT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci 有这样一张表，id是Int类型长度3的字段，那么理论上是不能存储长度超过3的值，比如：1INSERT INTO `test`(`id`, `name`) VALUES (1000000, &apos;name&apos;); 不过结果是意外的，这条数据可以正常写入。 2.Int类型的长度为什么不生效翻了一下Mysql的数据类型文档，发现这么一句话： 对于整数类型，M表示最大显示宽度。 对于浮点和定点类型，M是可以存储的总位数（精度）。 对于字符串类型，M是最大长度。 M的最大允许值取决于数据类型。 所以这个Int类型的“长度”其实叫宽度，也可以理解为显示长度。 3.如果存储的数据长度低于显示宽度会怎样？1INSERT INTO `test`(`id`, `name`) VALUES (1, &apos;name&apos;); 当然是可以写入的，但是好像并没有什么特别的不一样，不过我们再执行一条SQL1ALTER TABLE `test` MODIFY COLUMN `id` int(3) UNSIGNED ZEROFILL; 这次数据产生变化了，变成了这样123id | name001 | name1000000 | name 4.为什么要设计显示宽度？我也没有得出明确的答案，和小伙伴讨论这个问题~ta觉得是为了方便排序，你觉得呢？","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql int length","slug":"mysql-int-length","permalink":"http://xupin.im/tags/mysql-int-length/"}]},{"title":"Go学习笔记 - 定时任务调度","date":"2020-01-18T16:00:00.000Z","path":"2020/01/19/go-console/","text":"每一个任务都需要编写一个Crontab命令，这是件很麻烦且很不友好的事情。 任务调度器允许你以代码的形式定义调度命令，并且服务器上只需要一个Crontab命令即可, 任务调度又是我们俗称的 “计划任务” 1.工欲善其事，必先利其器Github一番搜索，发现Golang有个cron包（robfig/cron）大概满足需求，于是学习一下。 2.介绍看了一遍文档，cron包支持的已经很全面了~~~不用自己造轮子了。 表达式 兼容Linux的crontab表达式（支持分钟级别）。 日志 可以很详细的记录调度的任务状态 时区 支持任务级别的时区配置 预定义计划 支持在未来指定的时间去运行 线程安全问题 该cron lib管理任务队列的slice没有做并发安全考虑，可能会出现任务竟抢行为。 3.实例1234567891011121314151617181920212223242526package mainimport ( \"fmt\" \"time\" \"github.com/robfig/cron\")func main() &#123; c := cron.New() c.AddFunc(\"* * * * * *\", test) c.Start() timer := time.NewTimer(time.Second * 10) for &#123; select &#123; case &lt;-timer.C: timer.Reset(time.Second * 10) &#125; &#125;&#125;func test() &#123; fmt.Println(\"I'm a test script!\")&#125; 12345$ go run .I'm a test script!!!I'm a test script!!!I'm a test script!!!... 4.简单封装 main.go 123456789101112131415package mainimport \"cron-example/console\"func main() &#123; quitChan := make(chan bool, 1) // console go func() &#123; console.Default() &#125;() &lt;-quitChan&#125; console.go 12345678910111213141516171819202122232425262728293031package consoleimport ( \"log\" \"time\" \"cron-example/console/commands\" \"github.com/robfig/cron\")func schedule(c *cron.Cron) &#123; c.AddFunc(\"* * * * * *\", commands.Test)&#125;func Default() &#123; log.Println(\"Starting...\") c := cron.New() c.Start() schedule(c) timer := time.NewTimer(time.Second * 10) for &#123; select &#123; case &lt;-timer.C: timer.Reset(time.Second * 10) &#125; &#125;&#125;","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"PHP小知识 - aliyundb SQLSetStatement is NOT supported","date":"2020-01-06T16:00:00.000Z","path":"2020/01/07/php-aliyundb/","text":"PHP项目在数据库迁移使用ADB（aliyundb）时发现这个问题，具体错误描述是：1SQLSTATE[HY000]: General error: 1815 [15022, 2020010711034801025210201503151413416] statement type: class com.alibaba.fastsql.sql.ast.statement.SQLSetStatement is NOT supported! (SQL: select * from `users`) 在排查该问题的过程中，尝试了各种方式，降版本、使用原生语句等，甚至查找ADB手册和Issue还是无法解决。 1.原因因为ADB兼容Mysql但是有些机制还是略有不同，阅读源码后发现比如：ADB就不能很好的支持本地预处理语句，这也是该异常的原因。 2.解决方案PHP PDO链接的属性值PDO::ATTR_EMULATE_PREPARES需要设置为true1PDO::ATTR_EMULATE_PREPARES =&gt; true","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"php","slug":"php","permalink":"http://xupin.im/tags/php/"},{"name":"biscuits","slug":"biscuits","permalink":"http://xupin.im/tags/biscuits/"}]},{"title":"Go学习笔记 - WEB框架Gin","date":"2019-12-30T16:00:00.000Z","path":"2019/12/31/go-gin/","text":"Gin 是一个用 Go (Golang) 编写的 HTTP web 框架。 它是一个类似于 martini 但拥有更好性能的 API 框架, 由于 httprouter，速度提高了近 40 倍。 1.有哪些优点 较高的性能（Golang WEB框架比对数据） 简单易用的中间件 使用了性能高可扩展的HTTP路由httprouter 社区长期有着很高的活跃度 Github星星多（Orz） …等 2.安装/更新安装1$ go get github.com/gin-gonic/gin 更新1$ go get -u github.com/gin-gonic/gin 3.Hello World1234567891011121314151617181920package mainimport ( \"github.com/gin-gonic/gin\")func main() &#123; // 框架 g := gin.Default() // 路由 g.GET(\"/t\", test) // 服务端口 g.Run(\":4000\")&#125;func test(c *gin.Context) &#123; c.String(200, \"Hallo!\")&#125; 运行1$ go run main.go 访问 http://127.0.0.1:4000/t ，页面输出Hallo!。 4.路由Gin支持的路由方式和大部分主流框架基本一致。12345678910111213func main() &#123; g := gin.Default() g.GET(\"/someGet\", getting) g.POST(\"/somePost\", posting) g.PUT(\"/somePut\", putting) g.DELETE(\"/someDelete\", deleting) g.PATCH(\"/somePatch\", patching) g.HEAD(\"/someHead\", head) g.OPTIONS(\"/someOptions\", options) g.Run(\":4000\")&#125; Gin同样也支持路由参数，不过不支持路由正则表达式。1234567891011121314151617181920func main() &#123; g := gin.Default() // 此 handler 只能匹配 /user/&#123;PARAM&#125; g.GET(\"/user/:name\", func(c *gin.Context) &#123; name := c.Param(\"name\") c.String(http.StatusOK, \"Hello %s\", name) &#125;) // 此 handler 会匹配 /user/&#123;PARAM&#125;/ 和 /user/&#123;PARAM&#125;/&#123;ACTION_2&#125; // 如果访问 /user/&#123;PARAM&#125;，在没有 \"/user/:name\" 路由的情况下，会重定向至 /user/&#123;PARAM&#125;/ 匹配当前路由。 g.GET(\"/user/:name/*action\", func(c *gin.Context) &#123; name := c.Param(\"name\") action := c.Param(\"action\") message := name + \" is \" + action c.String(http.StatusOK, message) &#125;) g.Run(\":4000\")&#125; 5.路由组Gin在路由分组上的做法和其他框架也都是一致的，再次体现了Gin简单易用极容易上手的优点。1234567891011121314151617func main() &#123; g := gin.Default() userGroup := g.Group(\"/user\") &#123; userGroup.POST(\"/login\", ctrls.Login) userGroup.POST(\"/logout\", ctrls.Logout) &#125; reportGroup := g.Group(\"/report\") &#123; reportGroup.POST(\"/revenue\", ctrls.RevenueReport) reportGroup.POST(\"/cost\", ctrls.CostReport) &#125; g.Run(\":4000\")&#125; 6.中间件Gin的中间件分为：全局，路由组，路由级别。123456789101112131415func main() &#123; g := gin.Default() // 全局中间件 g.Use(middleware) // 路由组中间件 routerGroup := g.Group(\"/user\", middleware) &#123; // 路由中间件 routerGroup.GET(\"/login\", middleware, ctrls.Login) &#125; g.Run(\":4000\")&#125; 7.参数Gin如何获取请求参数？下面的简单例子：123456789101112131415161718192021222324func main() &#123; g := gin.Default() // URL参数 // URL：/user?email=mark@im.com g.GET(\"/user\", func(c *gin.Context) &#123; // 使用 DefaultQuery 或者 Query email := c.DefaultQuery(\"email\", \"default value\") email = c.Query(\"email\") c.String(http.StatusOK, \"Hallo, %s\", email) &#125;) // POST参数 g.POST(\"/user/login\", func(c *gin.Context) &#123; // 使用 DefaultPostForm 或者 PostForm email := c.PostForm(\"email\") email = c.DefaultPostForm(\"email\", \"default value\") c.String(http.StatusOK, \"Hallo, %s\", email) &#125;) g.Run(\":4000\")&#125; 最后Gin支持的操作还有很多，比如：数据绑定、数据验证、上传文件、静态资源嵌入等。 Gin没有提供的组件也有很多，比如：ORM、Console、日志滚动分割等。 仁者见仁智者见智，Gin专注做HTTP WEB框架的核心，更多扩展需要开发人员自己去选择，组件化的设计无疑是好的。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"golang","slug":"golang","permalink":"http://xupin.im/tags/golang/"}]},{"title":"ES学习笔记 - 实现列分组统计","date":"2019-12-16T16:00:00.000Z","path":"2019/12/17/elasticsearch-group/","text":"Elasticsearch是Elastic Stack核心的分布式搜索和分析引擎。 1.背景因为业务调整采用了ES作为数据库，所以需要了解ES对于这一块的设计如何实现类似Mysql中Group By的查询的效果。 2.实现方式ES实现Group By有两种方式：TermsAgg、CompositeAgg，它们也具有不同程度的优缺点。 TermsAgg的使用方式非常粗暴，直接进行桶嵌套即可，如下: TermsAgg请求DSL语句12345678910111213141516171819&#123; \"aggregations\": &#123; \"group_app\": &#123; \"terms\": &#123; \"field\": \"app\", \"size\": 1000000 &#125;, \"aggregations\": &#123; \"group_campaign_id\": &#123; \"terms\": &#123; \"field\": \"campaign_id\", \"size\": 1000000 &#125; &#125; &#125; &#125; &#125;, \"size\": 0&#125; TermsAgg执行结果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; \"took\": 142, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 175033, \"max_score\": 0.0, \"hits\": [] &#125;, \"aggregations\": &#123; \"group_app\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"com.aa.bb.cc\", \"doc_count\": 59929, \"group_campaign_id\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"Campaign_118\", \"doc_count\": 56466 &#125;, &#123; \"key\": \"Campaign_119\", \"doc_count\": 1937 &#125; ] &#125; &#125;, &#123; \"key\": \"com.dd.ee.ff\", \"doc_count\": 23231, \"group_campaign_id\": &#123; \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ &#123; \"key\": \"Campaign_120\", \"doc_count\": 16692 &#125;, &#123; \"key\": \"Campaign_121\", \"doc_count\": 5336 &#125; ] &#125; &#125; ] &#125; &#125;&#125; CompositeAgg的使用方式略微有一些不同，下面： CompositeAgg请求DSL语句1234567891011121314151617181920212223242526&#123; \"aggregations\": &#123; \"group_by\": &#123; \"composite\": &#123; \"sources\": [ &#123; \"app\": &#123; \"terms\": &#123; \"field\": \"app\" &#125; &#125; &#125;, &#123; \"campaign_id\": &#123; \"terms\": &#123; \"field\": \"campaign_id\" &#125; &#125; &#125; ], \"size\": 1000 &#125; &#125; &#125;, \"size\": 0&#125; CompositeAgg执行结果123456789101112131415161718192021222324252627282930313233343536373839&#123; \"took\": 14, \"timed_out\": false, \"_shards\": &#123; \"total\": 5, \"successful\": 5, \"skipped\": 0, \"failed\": 0 &#125;, \"hits\": &#123; \"total\": 175033, \"max_score\": 0.0, \"hits\": [] &#125;, \"aggregations\": &#123; \"group_by\": &#123; \"after_key\": &#123; \"app\": \"com.dd.ee.ff\", \"campaign_id\": \"Campaign_120\" &#125;, \"buckets\": [ &#123; \"key\": &#123; \"app\": \"com.aa.bb.cc\", \"campaign_id\": \"Campaign_119\" &#125;, \"doc_count\": 182 &#125;, &#123; \"key\": &#123; \"app\": \"com.dd.ee.ff\", \"campaign_id\": \"Campaign_120\" &#125;, \"doc_count\": 40 &#125; ] &#125; &#125;&#125; 3.优缺点TermsAgg 优点：使用简单、没有数据量限制。 缺点：数据结构层次深、不支持分页。 CompositeAgg 优点：使用简单、数据结构清晰可读，支持分页。 缺点：有数据量限制。 最后至于ES这两种分组方式的性能方面还没有研究，后续学习笔记会更新性能上面的差异。 https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-composite-aggregation.html // 附上ES官方文档","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://xupin.im/tags/elasticsearch/"}]},{"title":"Nginx - 正向/反向代理","date":"2019-12-16T16:00:00.000Z","path":"2019/12/17/nginx-proxy/","text":"代理通常用于在几台服务器之间分配负载，无缝显示来自不同网站的内容或通过除HTTP之外的协议将处理请求传递给应用服务器。 1.代理的常用场景及优点正向代理 gfw科学上网。 客户端访问鉴权。 反向代理 负载均衡。 保证内网的安全，阻止web攻击。 2.正向代理和反向代理有什么不同比较通俗的来解释：正向代理代理的是客户端，服务端不知道实际发起请求的客户端、反向代理代理的是服务端，客户端不知道实际接收请求的服务端。 一张图表示（来源：知乎wplulala）","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"nginx","slug":"nginx","permalink":"http://xupin.im/tags/nginx/"}]},{"title":"Mysql学习笔记 - 索引","date":"2019-12-16T11:30:00.000Z","path":"2019/12/16/mysql-index/","text":"索引用于快速查找具有特定列值的行。没有索引，MySQL必须从第一行开始，然后通读整个表以找到相关的行。如果表中有相关​​列的索引，MySQL可以快速确定要在数据文件中间查找的位置，而不必查看所有数据。这比顺序读取每一行要快得多。InnoDB和MyIsam只支持Btree，因此默认均是Btree，Memory和Heap支持Hash和Btree，如无明确声明，则默认索引均是Hash（包括主键）。 1.Mysql有哪些常用的索引 主键索引：数据列不允许重复，不允许为null，一个表只能有一个主键。 唯一索引：数据列不允许重复，允许为null，一个表允许多个列创建唯一索引。 普通索引：基本的索引类型，没有唯一性的限制，允许为null。 全文索引：全文索引是目前实现大数据搜索的关键技术。 2.Mysql索引的建立原则Mysql的索引遵循最左原则，在创建多列索引时，要根据业务需求，where条件中使用最频繁的一列放在最左边。 12345678910111213index(a,b,c)where a = v // 使用awhere a = v and b = v // 使用a,bwhere a = v and b = v and c = v // 使用a,b,cwhere b = v // 未使用索引where a = v and c = v // 使用awhere b = v and c = v // 未使用索引where a &gt; v // 是否使用索引，取决于查询结果集，如果全表扫描速度比索引速度快，那么不使用索引。where a like \"v%\" // 使用awhere a like \"%v%\" // 不适用索引 3.如何分析SQL语句执行性能以下是两种分析SQL性能的常用方式，explain、show profiles/show profile。 explain + SQL语句，获取SQL分析数据 select_type：对应SQL语句的查询复杂度。 table：正在访问的表。 partitions：数据所在的分区。 type：表示是否用上索引，以及索引是如何使用的，此字段决定索引的性能。ALL&lt;TYPE&lt;RANGE&lt;REF&lt;CONST possible_keys：查询条件存在的索引。 key：触发的索引。 key_len：索引字段的长度。 ref：索引访问，返回所有匹配某个单值的行。 rows：执行查询必须检查的行数，在InnoDB中此值不精确。 filtered：条件过滤出的行数的百分比。 extra：查询分析结果的额外信息，很重要 e.g. Using index、Using where … show profiles获得当前会话中执行的SQL语句，字段为：Query_ID, Duration, Query，show profile all for query {Query_ID} 4.Mysql中索引建立常见问题 为经常需要排序、分组和联合操作的字段建立索引 为常作为查询条件的字段建立索引 索引列值保证唯一性 索引建立的数量不要过多 索引列不要使用函数或者表达式 行锁依赖索引的建立 普通索引的数据重复率过高会导致索引失效 最左原则","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 事务隔离级别","date":"2019-12-05T04:30:00.000Z","path":"2019/12/05/mysql-transaction/","text":"什么是事务？事务（Transaction）一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)，具有4个特性：原子性、一致性、隔离性、持久性，简称ACID。 那么Mysql的事务隔离级别又是什么呢？Mysql的事务隔离级别分别是4种：未提交读（Read Uncommitted）、已提交读（Read Committed）、Repeatable Read（可重复读）、Serializable（可串行化）。可以简单理解为Mysql事务的4种执行标准。 一、ACID特性 原子性（Atomicity）：一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。 一致性（Consistency）：事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 隔离性（Isolation）：一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durability）：指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 二、Mysql事务隔离级别 未提交读（Read Uncommitted），指当前事务可以读取到其他事务还未提交的数据变化，最容易带来的问题是脏读（Dirty Read），很少应用到生产环境。 已提交读（Read Committed），大部分数据库默认隔离级别但不包括Mysql，指当前事务可以读取其他事务已提交的数据变化，具有隔离性的基本标准，但是在出现交叉事务的并发操作场景中会发生两次读取的数据结果集不一致的问题，即不可重复读（Nonrepeatable Read）。 Repeatable Read（可重复读），Mysql数据库的默认隔离级别，这里顺便解释一下为什么Mysql的隔离级别不是Read Committed，因为在Mysql5.0之前日志格式只有statement这一种，这种格式导致主从复制一致性很难得到保证。这种隔离级别可以解决Nonrepeatable Read的问题，但如果也出现在交叉事务的并发操作场景会出现幻读（Phantom Read）的现象，这一行为状态是当前事务不能及时有效的读取其他事务的数据变化。 Serializable（可串行化），Mysql事务隔离最高级别，望文生义大概意思是事务的执行是有序（串行化顺序）的，不存在事务交叉执行的场景从而解决了Phantom Read的问题，但是在高并发的业务场景下请求会出现阻塞、超时、锁竟抢的问题，系统的可用性也随之降低。 收尾 https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html // Mysql事务官方文档","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Mysql学习笔记 - 悲观锁/乐观锁","date":"2019-12-04T11:30:00.000Z","path":"2019/12/04/pessimistic-optimistic/","text":"什么是锁？在多进程（线程）编程中为了数据的一致性、有效性，如果一资源被某进程（线程）上锁，那么在释放锁之前其他进程（线程）无法进行操作或者等待获取（上）锁。 一、悲观锁悲观锁（Pessimistic locking），顾名思义对待任何事务都持悲观态度，做任何事情都会认为会有竞抢行为~所以在进行任何操作之前都要做好万全准备（上锁）才会继续执行后续的操作。通常是利用系统提供的锁机制来实现。比如：12Mysql中的上锁命令：for update、lock in share mode等Redis中的一些操作命令：setnx、getset等（原子操作） 二、乐观锁乐观锁（Optimistic locking），和悲观锁相反对待任何事务都持乐观态度，只会在最后即将执行操作的时刻前才会进行验证。通常是通过程序配合来实现。比如：1update `orders` set hash = md5(now()) where uid = 1 and hash = &#123;lastHash&#125;; 三、优缺点悲观锁优点很鲜明，因为每次执行的操作都是独占的，数据的一致性、有效性、安全性较高。但缺点同样突出，每次操作都会产生上锁的开销，在并发请求比较密集的情况下容易阻塞或者驳回请求，甚至是造成死锁，也大大降低了系统的性能。 乐观锁优点是省去了锁的开销，能较高的提高系统吞吐量，缺点是如果出现在并发高且竟抢（冲突）行为比较多的场景下数据的一致性很难保证。","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"mysql","slug":"mysql","permalink":"http://xupin.im/tags/mysql/"}]},{"title":"Redis学习笔记 - 持久化方式","date":"2019-12-03T07:30:00.000Z","path":"2019/12/03/redis-aof-rdb/","text":"Redis是一个开放源代码（BSD许可）的内存中数据结构存储，用作数据库，缓存和消息代理。 一、Redis数据存储的方式有两种12单纯的缓存模式, 整个数据的生命周期随着Redis Server的停止而消失。persistence模式, 数据会持续备份到磁盘文件。 二、Redis如何实现持久化存储？Redis提供了两种方式。12RDB(Redis DataBase)AOF(Append-only file) 三、RDBRDB的工作原理有点像运维脚本自动化定时备份数据一样，当内存中的数据到达配置的阈值就会执行DUMP操作备份数据到临时文件,备份成功结束后重命名为dump.rdb文件。123优点：fork子进程来进行备份,父进程不会进行io操作。恢复数据时的速度很快。缺点：这种方式是每隔一次才会进行备份,假如下一次备份前down机会丢失部分数据。而且如果备份的数据比较大，fork的子进程将会比较耗时，这段时间内会导致父进程阻塞。补充：因为这种每隔一段时间去备份一次的方式，类似快照。所以又叫 Snapshot。 四、AOFAOF的工作原理是讲写操作（数据）格式化追加到日志尾部，该日志文件保存了所有的历史操作数据，这一点非常类似Mysql中的bin.log。12优点：AOF这种方式可以保证较高的数据完整性，可以设置不同的策略，比如不保存，每一秒钟保存一次，或者每执行一个命令保存一次。AOF的默认策略为每一秒钟保存一次, 就算出现down机最多也就丢失1秒钟的数据, AOF备份数据因为是在后台线程执行fork子进程, 所以不会阻塞主线程。缺点：对于同样规模的数据备份的文件体积AOF要大于RDB。在备份速度上面也会慢于RDB，恢复速度同样也没有RDB快。 五、AOF备份触发机制1231. Redis服务端接收到客户端bgrewriteaof指令请求，如果当前没有在进行备份那么立即进行备份，否则等待备份完毕之后再执行备份。2. Redis conf配置了auto-aof-rewrite-percentage和auto-aof-rewrite-min-size参数，并且当前AOF文件大小server.aof_current_size大于auto-aof-rewrite-min-size，且AOF文件大小的增长率大于auto-aof-rewrite-percentage时触发备份。3. 使用config set appendonly yes命令时，调用startAppendOnly()函数触发备份。 六、那究竟使用哪种方式？12RDB 会有丢失数据风险，备份文件体积小，数据备份/恢复速度快。AOF 数据完整性更加安全，但是频繁备份需要过多的io操作性能会受到影响，备份文件体积较大，数据恢复速度慢。 首先，我们明白AOF, RDB的优缺点之后可以概括出: AOF适合热备，RDB适合冷备，Redis4.0以后允许使用aof-use-rdb-preamble配置项打开RDB-AOF混合持久化。 七、RDB/AOF混合使用的之后备份文件的变化当RDB和AOF同时开启之后121. Redis默认会优先加载AOF的配置文件。2. AOF备份文件的内容格式发生改变，备份文件前半段是RDB格式的全量数据后半段是Redis命令格式的增量数据。 八、AOF文件内容格式AOF文件内容格式是Redis通讯协议RESP（REdis Serialization Protocol）格式的命令文本存储，在此不展开学习后续会专门学习Redis的RESP协议。 最后附上Redis对于持久化方式说明的文档 https://redis.io/topics/persistence","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"redis","slug":"redis","permalink":"http://xupin.im/tags/redis/"}]},{"title":"设备通知推送 - Google FCM","date":"2017-02-27T16:00:00.000Z","path":"2017/02/28/google-msg-push/","text":"Firebase 是一个移动平台，可以帮助您快速开发高品质应用，扩大用户群，并赚取更多收益。Firebase 由多种互补功能组成，您可以自行组合和匹配这些功能以满足自己的需求。 1.获取Google推送的KEY(Google应用后台) AIzaSyCm5Vufc1FAtWE1kupGhVDRb0ThkCoWC7d // 这个是我的 2.PHP推送代码12345678910111213141516171819202122232425262728293031// 初始化环境$url = 'https://fcm.googleapis.com/fcm/send';$ch = curl_init($url);curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false);curl_setopt($ch, CURLOPT_RETURNTRANSFER,1);curl_setopt($ch, CURLOPT_POST, 1);$header = array( 'Content-Type: application/json', 'Authorization: key=AIzaSyCm5Vufc1FAtWE1kupGhVDRb0ThkCoWC7d' // 这里是刚才的KEY);curl_setopt($ch, CURLOPT_HTTPHEADER, $header);// 构建消息体,json编码$content = json_encode(['notification'=&gt;[ 'title' =&gt; '这里是标题', 'body' =&gt; '这里是内容', 'icon' =&gt; '这里是后台设置的iconName'],'to'=&gt;'设备标识(Google SDK可以获取)','registration_ids'=&gt;['设备标识','设备标识'] // 如果要推送多台设备,可以使用此字段,最多支持1000台设备]);curl_setopt($ch, CURLOPT_POSTFIELDS, $content);$ret = json_decode(curl_exec($ch), true);$ret['success']; // 推送成功数$ret['failure']; // 推送失败数 3.返回值示例1234567891011121314151617&#123; \"multicast_id\": 7669649331143639654, \"success\": 3, \"failure\": 0, \"canonical_ids\": 0, \"results\": [ &#123; \"message_id\": \"0:1488360060090224%6490a2d16490a2d1\" &#125;, &#123; \"message_id\": \"0:1488360060095871%6490a2d16490a2d1\" &#125;, &#123; \"message_id\": \"0:1488360060090765%6490a2d16490a2d1\" &#125; ]&#125; 4.Google(FCM) 文档 https://firebase.google.com/docs","tags":[{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"},{"name":"google fcm","slug":"google-fcm","permalink":"http://xupin.im/tags/google-fcm/"}]},{"title":"设备通知推送 - Apple APNS","date":"2017-02-21T16:00:00.000Z","path":"2017/02/22/apple-msg-push/","text":"1.合成证书(需要两个文件.cer,.p12,苹果开发者后台获取) ios.cer,ios.p12 // 我这里的文件名 123456$ openssl x509 -in ios.cer -inform der -out ios_cer.pem$ openssl pkcs12 -nocerts -out ios_p12.pem -in ios.p12Enter Import Password: // 这里输入Apple后台文件导出时的密码MAC verified OKEnter PEM pass phrase: // 这里输入新的pem密码(用于代码中),我输入的是123456789$ cat ios_cer.pem ios_p12.pem &gt; key.pem 2.PHP推送代码12345678910111213141516171819202122232425// 初始化环境$ctx = stream_context_create();stream_context_set_option($ctx, 'ssl', 'local_cert', 'assets/key/key.pem'); // key文件的路径stream_context_set_option($ctx, 'ssl', 'passphrase', '123456789'); // 生成pem输入的密码// 建立APNS连接$fp = stream_socket_client( 'ssl://gateway.push.apple.com:2195', $err, $errstr, 60, STREAM_CLIENT_CONNECT|STREAM_CLIENT_PERSISTENT, $ctx);// 构建消息体,json编码$payload = json_encode( array( 'aps'=&gt;array('alert' =&gt; '这里是内容', 'sound' =&gt; '这里是提示声音')));// 转换二进制$msg = chr(0) . pack('n', 32) . pack('H*', '设备标识(Apple SDK可以获取)') . pack('n', strlen($payload)) . $payload;// 发送消息$result = fwrite($fp, $msg, strlen($msg));if(!$result)&#123; // 推送失败&#125;else&#123; // 推送成功&#125;fclose($fp); 3.APNS文档 https://developer.apple.com/notifications/","tags":[{"name":"apns","slug":"apns","permalink":"http://xupin.im/tags/apns/"},{"name":"develop","slug":"develop","permalink":"http://xupin.im/tags/develop/"}]}]